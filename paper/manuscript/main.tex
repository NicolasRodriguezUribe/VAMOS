% VAMOS Paper: IEEE Transactions on Evolutionary Computation (IEEE TEVC)
% =============================================================================
% IEEE Editorial Style Manual (updated 29 July 2024) quick checks:
%   - Title capitalization: capitalize nouns/pronouns/adjectives/verbs/adverbs;
%     lowercase articles, coordinating conjunctions, and short prepositions
%     (unless first/last).
%   - Abstract: one paragraph; 150--250 words; no numbered equations, numbered
%     reference citations, or footnotes.
%   - Index Terms: alphabetical order; capitalize first word; define acronyms
%     at first use.
%   - Figures/Tables in text: cite as ``Fig.'' and ``Table''; first citations
%     in numerical order.
%   - Table captions: no period at the end of the caption text.
%   - Acronyms: define at first use in the abstract and body; use American
%     spelling.
%   - References: one source per reference number; avoid ``in Smith [1]''
%     constructions unless the name is essential.
% ======================================================================
\documentclass[journal]{IEEEtran}

\usepackage{cite}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\pmark}{\ding{109}}
\newcommand{\xmark}{\ding{55}}
\usepackage[hidelinks]{hyperref}
\hypersetup{hypertexnames=false}
\makeatletter
\g@addto@macro\UrlBreaks{\do\.\do\_\do\-\do\/}
\makeatother

% Code listing style
\lstset{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red!70!black},
  showstringspaces=false,
  frame=single,
  breaklines=true,
  numbers=left,
  numberstyle=\tiny\color{gray}
}

\newcommand{\VAMOS}{VAMOS}
\newcommand{\HV}{\mathrm{HV}}
\newcommand{\Intdomain}[1]{[#1]_{\mathbb{Z}}}
\newcommand{\Realdomain}[1]{[#1]_{\mathbb{R}}}

% Original title (includes the framework name VAMOS):
% \title{VAMOS: A Vectorized Architecture for Multiobjective Optimization Studies: A High-Performance Python Framework}
%\title{Vectorized Architecture for Multiobjective Optimization Studies}
\title{A High-Performance Vectorized Framework for Multiobjective Evolutionary Optimization in Python}

% --------------------------------------------------------------------------
%  Author block.
% --------------------------------------------------------------------------
\author{Nicol\'{a}s~R.~Uribe,
        Alberto~Herr\'{a}n,
        Antonio~J.~Nebro,
        Javier~Del~Ser,
        and~J.~Manuel~Colmenar%
\thanks{N. R. Uribe, A. Herr\'{a}n, and J. M. Colmenar are with the
  Department of Computer Sciences, Universidad Rey Juan Carlos, M\'{o}stoles,
  28933 Madrid, Spain (e-mail: nicolas.rodriguez@urjc.es; alberto.herran@urjc.es;
  josemanuel.colmenar@urjc.es).}%
\thanks{A. J. Nebro is with the Department of Lenguajes y Ciencias de la
  Computaci\'{o}n, ITIS Software, University of M\'{a}laga, 29071
  M\'{a}laga, Spain (e-mail: ajnebro@uma.es).}%
\thanks{J. Del Ser is with TECNALIA, Basque Research \& Technology
  Alliance (BRTA), Derio, 48160, Spain, and also with the University
  of the Basque Country (UPV/EHU), Leioa, 48940, Spain (e-mail: javier.delser@tecnalia.com).}%
\thanks{Corresponding author: Alberto Herr\'{a}n (alberto.herran@urjc.es).}%
}

\markboth{IEEE Transactions on Evolutionary Computation}%
{A Vectorized Framework for Multiobjective Evolutionary Optimization}

\begin{document}
\maketitle

% --------------------------------------------------------------------------
%  ABSTRACT â€” IEEE Editorial Style Manual:
%    - one paragraph, 150--250 words;
%    - no numbered equations, numbered reference citations, or footnotes.
% --------------------------------------------------------------------------
\begin{abstract}
Python is a widely adopted platform for empirical studies in
multi-objective optimization, yet its interpreted nature makes
metaheuristics implemented in pure Python inherently slower than those of frameworks
written in compiled languages.
Most Python libraries further aggravate this limitation by managing
populations as collections of individual solution objects, which
prevents effective vectorization and keeps hot-path computations in
the interpreter.
We present VAMOS (Vectorized Architecture for Multiobjective
Optimization Studies), a Python framework that stores population state
in dense numerical arrays and dispatches hot-path computations to
pluggable backends (NumPy, Numba, MooCore/C), keeping algorithmic logic
independent of the numerical substrate.
VAMOS implements nine multi-objective evolutionary algorithms
spanning dominance-based, decomposition-based, and indicator-based paradigms, with richer component-level
configurability than typical frameworks, including interchangeable
variation operators, configurable steady-state modes, and
optional bounded or unbounded external archives.
A two-line entry point, automatic algorithm selection, and a
browser-based graphical interface lower the entry barrier for domain experts
with limited Python experience, while explicit component-level configuration
supports metaheuristics practitioners who require fine-grained control.
On the ZDT, DTLZ, and WFG benchmark suites with two and three objectives, VAMOS with its Numba backend reduces wall-clock runtime by $4$--$12\times$ relative to object-centric libraries and by $1.1$--$1.2\times$ relative to the closest array-aware competitor, with the advantage widening as population size grows. Paired Wilcoxon signed-rank tests with Holm correction confirm statistically equivalent solution quality across frameworks.
VAMOS is an open-source project available at \url{https://github.com/vamos-framework/vamos}.
\end{abstract}

% Index Terms: alphabetical order; capitalize first word.
\begin{IEEEkeywords}
Benchmarking, Evolutionary algorithms, Multiobjective optimization,
Software framework, Vectorization
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

%==============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

\IEEEPARstart{M}{ulti-objective} optimization problems (MOPs) arise whenever trade-offs exist between conflicting objectives, requiring the computation of an accurate approximation, in terms of convergence and diversity, of the Pareto-optimal set rather than a single optimum~\cite{Deb01}. Such problems are pervasive in engineering design, machine learning, scheduling, and resource allocation, among other domains. In this field, multi-objective evolutionary algorithms (MOEAs) remain a dominant approach due to their robustness, population-based anytime behavior, and ability to return a set of non-dominated solutions in a single run~\cite{coello2007evolutionary}.

The widespread adoption of MOEAs has been fueled in large part by the availability of software frameworks that lower the implementation barrier and facilitate reproducible experimentation. Early efforts such as PISA~\cite{bleuler2003pisa} established the concept of modular, platform-independent interfaces for multi-objective search. The jMetal framework~\cite{durillo2011jmetal} later consolidated a comprehensive Java-based toolkit that became a reference for algorithm development and research studies. More recently, PlatEMO~\cite{tian2017platemo} has provided a MATLAB platform with a large catalog of algorithms and test problems. In parallel, the Python ecosystem has seen the emergence of dedicated libraries, such as pymoo~\cite{blank2020pymoo}, jMetalPy~\cite{benitez2019jmetalpy}, Platypus~\cite{hadka2015platypus}, and DEAP~\cite{fortin2012deap}, that leverage Python's accessibility and rich scientific stack. 

As Python has become a widely adopted platform for MOEA research and experimentation, three practical limitations hinder its broader adoption. First, Python's interpreted nature makes the inner loops of pure-Python implementations inherently slower than those of frameworks written in compiled languages such as C++ or Java, and many Python MOEA libraries further aggravate this overhead by managing populations as collections of per-solution objects, preventing effective vectorization and keeping hot-path computations, namely, selection, variation, and survival, inside the interpreter~\cite{benitez2019jmetalpy,fortin2012deap,hadka2015platypus}.

Second, existing MOEA frameworks often provide limited configurability at the component level. If we focus on NSGA-II~\cite{deb2002fast}, for instance, most tools expose only commonly used parameters, such as the population size, the number of generations, or the crossover and mutation probabilities and distribution indexes of the SBX and polynomial mutation operators. However, features such as setting the offspring population size (e.g., to configure a steady-state variant if that size is 1) or attaching external archives, which have been shown to be useful to allow NSGA-II to handle problems with more than two objectives~\cite{IshibuchiPS20}, are not included in most tools, with the exception of jMetal~\cite{durillo2011jmetal}.

Third, adoption is hindered by usability gaps affecting two complementary user profiles. On the one hand, domain experts who face multi-objective trade-offs in fields such as biology, physics, or engineering may have limited experience with Python or optimization frameworks; in this case, the need to import and wire together multiple modules (algorithm, problem, operators, termination), understand framework-specific abstractions, and write 10--15 lines of boilerplate code for a standard run becomes a practical barrier. On the other hand, users who are expert in metaheuristics benefit from frameworks that expose explicit control over algorithm components and hyperparameters so that they can exploit the full potential of MOEAs without being constrained by rigid presets.

We introduce \VAMOS{}, a Python-based package designed to address these limitations, i.e., runtime overhead, configurability, and accessibility, with a single guiding principle: \emph{keep algorithm state in dense arrays and push hot loops into vectorized kernels}. \VAMOS{} stores population state in dense numerical arrays and dispatches hot-path computations to pluggable compute backends (NumPy, Numba, MooCore/C), keeping algorithmic logic independent of the numerical substrate.

To support both novice and expert workflows, \VAMOS{} provides a two-line entry point (\texttt{optimize("zdt1", algorithm="nsgaii")}) for complete optimization runs, a \texttt{make\_problem} factory that turns any Python function into a problem without subclassing, automatic algorithm selection from problem traits, and a browser-based graphical interface (\emph{VAMOS Studio}) with domain-specific templates that allows users to define and solve problems without writing any code. The contributions of this work are:
\begin{enumerate}
  \item \textbf{High-performance vectorized core}: a unified, array-based internal representation with pluggable compute kernels (NumPy/Numba/MooCore) that reduces inner-loop overhead by $4$--$12\times$ relative to object-centric libraries.
  \item \textbf{Rich component-level configurability}: nine MOEAs implemented on top of shared components, with interchangeable variation operators, configurable offspring batch size (enabling steady-state variants), optional bounded or unbounded external archives, multiple constraint-handling modes.
  \item \textbf{Accessible API and tooling for novice-to-expert workflows}: a two-line entry point for complete optimization runs, a \texttt{make\_problem} factory that turns any Python callable into a problem without subclassing, automatic algorithm selection from problem traits, a CLI wizard for guided setup, and a Web-based interactive dashboard (\emph{\VAMOS{} Studio}) with domain-specific templates for both novice and expert users.
\end{enumerate}

The rest of the paper is organized as follows. Section~\ref{sec:related} reviews existing Python MOEA frameworks and benchmarking practices. Section~\ref{sec:framework} presents the \VAMOS{} framework, including its design goals, architecture, compute kernels, and algorithm suite. Section~\ref{sec:experiments} describes the experimental setup, reports backend-level and cross-framework runtime comparisons including a scalability study of the vectorized architecture, and provides solution-quality summaries with statistical analyses. Finally, Section~\ref{sec:conclusions} draws conclusions and outlines directions for future work.

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

We review the most relevant Python MOEA frameworks below.

\subsection{Python MOEA frameworks}
\label{sec:related_frameworks}

Several mature Python libraries exist for MOEAs. We describe next the most relevant ones, including pymoo, jMetalPy, DEAP, Platypus, and Pygmo:

\textbf{pymoo}~\cite{blank2020pymoo}  is arguably the most comprehensive Python framework currently available, offering a wide range of multi-objective algorithms (NSGA-II, NSGA-III, MOEA/D, among others), alongside utilities for performance indicator computation, visualization, and constraint handling. Its object-oriented architecture encourages algorithm customization through subclassing, and its documentation and community support are well-established. However, pymoo's design tightly couples algorithmic logic with its internal data structures, making it difficult to swap computation backends or to systematically push hot-path computations into compiled kernels. Component-level configurability, such as interchangeable variation operators or configurable archiving strategies, requires non-trivial subclassing efforts.

\textbf{jMetalPy}~\cite{benitez2019jmetalpy} is the Python port of the widely used jMetal Java framework~\cite{durillo2011jmetal} and follows a similar object-oriented, component-based design philosophy. It supports both standard and preference-based multi-objective algorithms and includes observer and observable patterns for runtime monitoring. Despite these strengths, jMetalPy's architecture prioritizes design clarity and Java-style extensibility over computational performance, resulting in slower runtimes compared to array-oriented frameworks. Its population representation relies on lists of solution objects rather than dense numerical arrays, which limits the extent to which hot-path computations can be vectorized or accelerated by alternative computation backends.

\textbf{DEAP} (Distributed Evolutionary Algorithms in Python)~\cite{fortin2012deap} takes a highly generic approach, providing a toolkit of evolutionary operators that can be assembled into custom algorithms through a combination of functional programming patterns and a declarative toolbox mechanism. This makes DEAP extremely flexible for prototyping novel algorithms. However, this generality comes at the cost of abstraction: multi-objective optimization is supported through NSGA-II and SPEA2, but building more sophisticated pipelines requires substantial user effort. DEAP does not provide integrated support for performance indicators, external archives, or decomposition-based paradigms, and its per-individual object model shares the same vectorization limitations as jMetalPy.

\textbf{Platypus}~\cite{hadka2015platypus} is a lightweight framework specifically designed for multi-objective optimization, implementing several classical algorithms including NSGA-II, NSGA-III, MOEA/D, and epsilon-MOEA. Its API is clean and beginner-friendly, and it supports parallelism via Python's multiprocessing module. Platypus, however, has seen limited maintenance in recent years and lacks support for more recent indicator-based algorithms and advanced archiving strategies. Like jMetalPy and DEAP, it relies on object-based population representations that preclude backend-level vectorization.

\textbf{pygmo}~\cite{Biscani2020} is a Python library wrapping the C++ optimization framework pagmo2, designed with performance and parallelism as primary concerns. By exposing its core algorithms through compiled C++ extensions, pygmo achieves runtimes that are substantially faster than pure-Python frameworks, and its island model provides a principled abstraction for distributed and parallel optimization. pygmo supports several multi-objective algorithms, including NSGA-II and MOEA/D, and allows users to define custom problems and algorithms through a duck-typing interface. However, pygmo's architecture is primarily oriented toward single-objective and continuous optimization, and its multi-objective capabilities are comparatively limited in scope. Furthermore, pygmo does not expose component-level configurability in variation operators and selection mechanisms, and archiving strategies are bundled within each algorithm and cannot be recombined independently, which limits its utility for practitioners interested in algorithm design rather than algorithm application.

%We build upon this methodological focus on rigorous experimental studies and complement it with an array-based execution model and a fairness-oriented benchmarking protocol that aligns problem definitions and operator semantics across frameworks.

Taken together, these frameworks occupy a spectrum between usability and performance. To the best of our knowledge, they do not explicitly separate algorithmic logic from the numerical substrate in a way that enables transparent substitution of computation backends, nor do they combine a low-barrier entry point for domain experts with fine-grained component configurability for practitioners, which motivates the focus of \VAMOS{}.

In contrast to these Python libraries, EvoX~\cite{huang2025evox} targets automated, distributed, and heterogeneous execution of general evolutionary computation workloads. It provides a functional programming model for declaring the logical flow of evolutionary algorithms and a hierarchical state-management strategy that maps the resulting workflow onto heterogeneous resources, enabling GPU-accelerated and multi-node execution. In contrast, \VAMOS{} targets a complementary, algorithm-level gap: a vectorized, backend-agnostic architecture optimized for multi-objective optimization studies, with emphasis on efficient inner-loop execution in Python and reproducible cross-framework benchmarking.

 Beyond general-purpose MOEA libraries and infrastructure-oriented frameworks like EvoX, recent research has introduced specialized frameworks and workflows. Examples include a coevolutionary approach for constrained multiobjective optimization using helper problems and information sharing~\cite{tian2021coevolutionary}; an evolutionary multitask framework for multimodal optimization with explicit/implicit bi-knowledge transfer between species~\cite{zhao2026multitask}; Light-EvoOPT, a four-stage pipeline for ultralarge-scale mixed-integer linear programs combining problem division, model-based initialization, reduction, and evolutionary improvement~\cite{ye2026lightevoopt}; and LLaMoCo, which instruction-tunes large language models to generate optimization code~\cite{ma2026llamoco}. These contributions focus on specific problem settings or automation, whereas \VAMOS{} targets a vectorized, backend-agnostic architecture for multi-objective optimization studies and reproducible cross-framework benchmarking.

\textcolor{red}{Table~\ref{tab:framework-comparison} summarizes the main differences between Python multi-objective optimization frameworks relevant to this work, including algorithmic capabilities, architectural features, configurability, accessibility (minimum lines of code for a standard run), and tooling. \VAMOS{} is the only framework that combines a dense array population model, pluggable computation backends, rich component-level configurability, a minimal two-line entry point (Listing~\ref{lst:quickstart}), and a graphical user interface.}

\begin{table*}[t]
\centering
\caption{Comparison of Python multi-objective optimization frameworks.
\cmark~indicates full support, \pmark~indicates partial or limited support,
and \xmark~indicates absence of the feature.}
\label{tab:framework-comparison}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lcccccc}
\toprule
\textbf{Feature} & \textbf{pymoo} & \textbf{jMetalPy} & \textbf{DEAP} & \textbf{Platypus} & \textbf{pygmo} & \textbf{VAMOS} \\
\midrule
Dominance-based algorithms          & \cmark & \cmark & \pmark & \cmark & \pmark & \cmark \\
Decomposition-based algorithms      & \cmark & \pmark & \xmark & \cmark & \pmark & \cmark \\
Indicator-based algorithms          & \pmark & \pmark & \xmark & \xmark & \xmark & \cmark \\
Pluggable computation backends      & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark \\
Dense array population model        & \pmark & \xmark & \xmark & \xmark & \xmark & \cmark \\
Interchangeable variation operators & \pmark & \pmark & \cmark & \xmark & \xmark & \cmark \\
Configurable steady-state mode      & \xmark & \pmark & \pmark & \xmark & \xmark & \cmark \\
External archive support            & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark \\
Performance indicators              & \cmark & \pmark & \xmark & \pmark & \xmark & \cmark \\
Parallelism                         & \pmark & \pmark & \cmark & \cmark & \cmark & \pmark \\
\textcolor{red}{Graphical user interface}            & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \textcolor{red}{\cmark} \\
Automatic algorithm selection       & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark \\
\textcolor{red}{Min.\ lines of code\textsuperscript{$\dagger$}} & \textcolor{red}{$\sim$10} & \textcolor{red}{$\sim$15} & \textcolor{red}{$\sim$20} & \textcolor{red}{$\sim$8} & \textcolor{red}{--} & \textcolor{red}{2} \\
Active maintenance                  & \cmark & \pmark & \cmark & \xmark & \cmark & \cmark \\
\bottomrule
\multicolumn{7}{l}{\textcolor{red}{\footnotesize\textsuperscript{$\dagger$}Lines of code for a minimal NSGA-II run on a built-in benchmark.}}
\end{tabular}
\end{table*}

Fig.~\ref{fig:representation} provides an intuitive view of how these frameworks represent and manipulate a population. In \VAMOS{}, the population state is stored in dense arrays ($X \in \mathbb{R}^{N\times n}$ and $F \in \mathbb{R}^{N\times m}$), and selection/variation/survival primarily operate by computing index arrays and applying array kernels. In contrast, pymoo and jMetalPy typically manage the population as a collection of solution objects; array computations may still be used internally, but more of the algorithmic control flow involves object/container manipulation.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/Representation.png}
  \caption{Comparing population representation across Python MOEA frameworks. \VAMOS{} (left) stores the full population in dense arrays ($X$, $F$) and applies operators via array-oriented kernels (NumPy/Numba/MooCore). pymoo (center) uses a population container of individual objects backed by NumPy arrays, mixing array computation with object overhead. jMetalPy (right) manages a list of solution objects with per-solution method calls. All three implement the same selection--variation--survival loop, but differ in internal data model and hot-loop granularity.}
  \label{fig:representation}
\end{figure*}

%==============================================================================
\section{\VAMOS{} Framework}
\label{sec:framework}
%==============================================================================

This section presents the \VAMOS{} framework. We first state the design goals that guided its development (Section~\ref{sec:design_goals}), then describe the architecture and data model (Section~\ref{sec:architecture}), compute kernels, algorithm suite, execution model, variation pipeline, archives and metrics, and the benchmark runner.

\subsection{Design goals}
\label{sec:design_goals}

The design of \VAMOS{} is motivated by three practical gaps identified in existing Python MOEA libraries, corresponding to the three contributions of this work: performance, configurability, and accessibility.

\paragraph{Performance: array-native execution with pluggable backends.}
The population state remains in dense arrays throughout the optimization loop, and hot-path computations are dispatched to vectorized or compiled kernels. Switching the compute backend is a single-parameter change (\texttt{engine="numba"}, \texttt{"numpy"}, etc.) that is transparent to algorithm code. No other Python MOEA framework allows swapping the entire computational substrate---from array operations to compiled kernels---without modifying the algorithm implementation. Similarly, evaluation strategies (serial, multiprocessing, or Dask-based distributed evaluation) are selected via configuration rather than code changes, and multi-seed studies are first-class: passing a list of seeds (\texttt{seed=[0,1,2]}) triggers automatic multi-run execution and returns a list of results. Section~\ref{sec:experiments} demonstrates that this design yields substantial runtime reductions across multiple MOEA variants and benchmark families without degrading solution quality.

\paragraph{Configurability: fine-grained control over algorithmic components.}
Most frameworks provide a small set of high-level parameters (population size, crossover/mutation probabilities), but do not expose fine-grained control over offspring batch size, archive attachment, or operator adaptation. \VAMOS{} exposes all of these as first-class configuration options through a builder API (Listing~\ref{lst:builder} in Appendix~\ref{app:code}): users can set the offspring population size (e.g., to configure a steady-state variant), attach bounded or unbounded external archives, and select among multiple constraint-handling modes (feasibility rules, penalty functions, $\varepsilon$-relaxation). Algorithm performance is also sensitive to hyperparameters; \VAMOS{} therefore includes a tuning module (\texttt{vamos.engine.tuning}) exposed both programmatically and via the CLI (\texttt{vamos tune}). It supports random search and an irace-inspired racing procedure~\cite{lopez2016irace} with multi-fidelity evaluation (successive halving), warm-starting, and parallel execution. The same interface can also delegate to model-based backends such as Optuna~\cite{akiba2019optuna}, SMAC3~\cite{lindauer2022smac3}, and BOHB~\cite{falkner2018bohb}, which are provided as optional dependencies.

\paragraph{Accessibility: lowering the entry barrier for domain experts.}
Most frameworks require importing separate modules for the algorithm, problem, operators, and termination, then wiring them together (typically 10--15 lines for a standard Nondominated Sorting Genetic Algorithm~II (NSGA-II) run on a benchmark problem). This level of boilerplate assumes familiarity with MOEA terminology and framework internals, which is a barrier for domain experts in fields such as biology, physics, or engineering who need multi-objective optimization but lack a background in metaheuristics. \VAMOS{} addresses this gap at multiple levels. First, a single entry point (\texttt{optimize}) handles defaults, operator configuration, and termination in a single call (Listing~\ref{lst:quickstart} in Appendix~\ref{app:code}): two lines of code are sufficient to run a complete optimization with sensible defaults. For comparison, the equivalent run in pymoo requires $\sim$10 lines; jMetalPy requires $\sim$15 lines. Second, a \texttt{make\_problem} factory turns any Python callable into a problem object without subclassing (Listing~\ref{lst:make_problem}), and an interactive CLI wizard (\texttt{vamos create-problem}) scaffolds a ready-to-run script from a guided questionnaire. Third, \VAMOS{} provides automatic algorithm selection (\texttt{algorithm="auto"}) that maps problem traits (number of objectives, encoding type) to a suitable default algorithm. When finer control is needed, the builder API ensures that the framework scales from novice to expert use without a different API.

\subsection{Architecture and data model}
\label{sec:architecture}

\VAMOS{} is organized into four layers:
\begin{enumerate}
  \item \textbf{Foundation}: problem definitions (ZDT, DTLZ, WFG, ZCAT, CEC2009, LZ09, LSMOP, constrained families such as C-DTLZ and MW, and real-world suites), kernels, metrics, and archives;
  \item \textbf{Engine}: algorithm implementations and shared components;
  \item \textbf{Experiment}: CLI, benchmarking utilities, visualization, and reporting;
  \item \textbf{UX}: interactive dashboard (\emph{\VAMOS{} Studio}) with a visual problem builder and results explorer.
\end{enumerate}

Fig.~\ref{fig:architecture} illustrates the core layered organization: the Foundation layer provides problem definitions, compute kernels, metrics, and archives; the Engine layer implements nine MOEAs on top of shared components; the Experiment layer exposes CLI tools, benchmarking utilities, and reporting; and the UX layer provides \VAMOS{} Studio, a Streamlit-based interactive environment for visual problem definition, live Pareto-front preview, and post-run analysis.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/architecture.png}
\caption{\VAMOS{} four-layer architecture. The Foundation layer provides problem definitions, compute kernels (NumPy, Numba, MooCore/C), metrics, and archives. The Engine layer implements nine MOEAs using shared selection, variation, and survival components. The Experiment layer exposes CLI tools for problem creation, hyperparameter tuning, benchmarking runners, table/plot generation, and results analysis. The UX layer provides \VAMOS{} Studio, an interactive dashboard for visual problem building, onboarding, and results exploration.}
\label{fig:architecture}
\end{figure*}

The user-facing API and configuration model are described in Section~\ref{sec:design_goals}. The core design choice is to represent a population as a pair of dense arrays: decision variables $X \in \mathbb{R}^{N \times n}$ and objective values $F \in \mathbb{R}^{N \times m}$ for population size $N$, $n$ decision variables, and $m$ objectives. Variation and survival operate on these arrays (or views thereof), enabling vectorized operations (NumPy) and compilation of hot loops (Numba) without per-individual Python objects.

\subsection{Compute kernels}

Algorithmic logic (e.g., \emph{NSGA-II selects survivors by non-dominated sorting and crowding distance}) is separated from numerical kernels that implement the corresponding computations. \VAMOS{} provides multiple backends:
\begin{itemize}
  \item \textbf{NumPy}: baseline backend using vectorized array operations~\cite{harris2020numpy}.
  \item \textbf{Numba}: just-in-time (JIT) compilation of hot operators and utilities, reducing Python overhead~\cite{lam2015numba}.
  \item \textbf{MooCore}: optional C-backed kernels for Pareto ranking and hypervolume-based utilities~\cite{moocore}.
\end{itemize}

Backends are selected through configuration (e.g., \texttt{engine="numba"}) and are transparent to algorithm code.

In addition to accelerating arithmetic kernels, \VAMOS{} reduces overhead by minimizing Python-side allocations: variation operators can reuse pre-allocated workspaces (e.g., scratch buffers for simulated binary crossover/polynomial mutation) and algorithms maintain state as contiguous arrays. This design is particularly beneficial in inner-loop routines such as tournament selection, non-dominated sorting, crowding-distance computation, and archive updates.

\subsection{Algorithm suite}

\VAMOS{} implements nine multi-objective algorithms covering dominance-based, decomposition-based, indicator-based, and reference-vector paradigms. Table~\ref{tab:algorithms} summarizes the suite.

\begin{table}[htbp]
\centering
\caption{Multi-objective algorithms implemented in \VAMOS{}}
\label{tab:algorithms}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{llc}
\toprule
\textbf{Algorithm} & \textbf{Category} & \textbf{Ref.} \\
\midrule
Nondominated Sorting Genetic Alg.~II (NSGA-II) & Dominance & \cite{deb2002fast} \\
Nondominated Sorting Genetic Alg.~III (NSGA-III) & Dominance & \cite{deb2014evolutionary} \\
MOEA Based on Decomposition (MOEA/D) & Decomposition & \cite{zhang2007moea} \\
S-Metric Selection EMOA (SMS-EMOA) & Indicator & \cite{beume2007sms} \\
Strength Pareto EA~2 (SPEA2) & Dominance & \cite{zitzler2001spea2} \\
Indicator-Based EA (IBEA) & Indicator & \cite{zitzler2004indicator} \\
Speed-Constrained PSO (SMPSO) & Swarm & \cite{nebro2009smpso} \\
Adaptive Geometry Estimation MOEA (AGE-MOEA) & Dominance & \cite{panichella2019adaptive} \\
Reference Vector Guided EA (RVEA) & Decomposition & \cite{cheng2016reference} \\
\bottomrule
\end{tabular}
\end{table}

All nine algorithms are built on top of the shared selection, variation, and survival components described in Sections~\ref{sec:architecture} and below, ensuring consistent operator semantics and enabling component-level substitution (e.g., replacing SBX crossover and polynomial mutation with any of the others available variation operators and using bounded and unbounded external archives) without algorithm-specific code changes. The suite spans the three major MOEA paradigms: dominance ranking (NSGA-II, NSGA-III, SPEA2, AGE-MOEA), decomposition (MOEA/D, RVEA), and indicator contribution (SMS-EMOA, IBEA), plus a particle swarm optimization algorithm (SMPSO), so that practitioners can compare paradigms under identical infrastructure and configuration conventions. \textcolor{red}{Table~\ref{tab:algo_overview} in Appendix~\ref{app:param_space} summarizes the configurable parameter space for each algorithm, ranging from 8 parameters (SMPSO) to 37 (NSGA-II).}

%AGE-MOEA follows the adaptive geometry estimation survival scheme described in~\cite{panichella2019adaptive}: after non-dominated sorting, the first front is normalized by extreme points, a curvature-dependent $p$-norm is estimated, and survival uses geometry-aware crowding based on Minkowski distances. RVEA follows~\cite{cheng2016reference} with angle-penalized distance survival and periodic reference-vector adaptation using ideal/nadir scaling; we therefore enforce \texttt{pop\_size} to match the number of simplex-lattice reference directions induced by \texttt{n\_partitions}.

\subsection{Execution model and evaluation backends}

Algorithms in \VAMOS{} expose a uniform \texttt{run(problem, termination, seed, eval\_backend, live\_viz)} interface\textcolor{red}{, which the high-level \texttt{optimize()} entry point (Section~\ref{sec:design_goals}) delegates to after resolving defaults and configuring operators}. \textcolor{red}{Several algorithms} also support an \emph{ask--tell} loop for streaming or interactive use. Objective evaluations are dispatched through a small evaluation-backend protocol, enabling serial evaluation for controlled benchmarking and parallel evaluation when objective functions are expensive. Separating evaluation from algorithmic state keeps the algorithm core deterministic under a fixed random seed and reduces the risk of framework-specific parallelism becoming a confounder in empirical comparisons.

\subsection{Variation pipeline and encodings}

\VAMOS{} supports real-valued, binary, integer, permutation, and mixed-variable encodings through a variation pipeline that composes selection, crossover, mutation, and optional repair operators. For continuous domains, the default configuration uses simulated binary crossover (SBX) and polynomial mutation (PM), with mutation probability typically set to $1/n$ where $n$ is the number of decision variables. Binary and integer encodings use standard crossover (one-point, two-point, or uniform) and bit-flip or bounded mutation, while permutation encodings provide order and cycle crossover with insertion mutation. Mixed-variable problems combine encoding-specific operators transparently. Encodings are normalized internally to ensure consistent operator resolution and avoid silent configuration mismatches. \textcolor{red}{The experimental evaluation in Section~\ref{sec:experiments} focuses on continuous encodings; benchmarking of discrete and mixed-variable operators is left to future work.}

\subsection{Archives, metrics, and analysis tooling}

The framework includes optional external archives---both bounded (with crowding-distance or hypervolume-based pruning) and unbounded---that accumulate non-dominated solutions independently of the evolving population. This decouples the population-based search from the elite set, which is particularly useful for problems with more than two objectives~\cite{IshibuchiPS20}. Common quality indicators are built in, including hypervolume, generational distance (GD), inverted generational distance (IGD), and spread. Hypervolume computation can use optimized backends (MooCore) when installed, with fallback implementations for low-dimensional cases.

\subsection{Benchmark runner and reporting}

The experiment layer includes a benchmark runner that executes standardized studies under a shared protocol (fixed problem definitions, evaluation budgets, and seeds) and emits machine-readable artifacts (per-seed results and summary CSVs). Table generation scripts produce LaTeX-ready tables directly from these artifacts, reducing transcription errors and enabling end-to-end reproducibility.

\subsection{Interactive dashboard and problem-definition tooling}

The UX layer provides \emph{\VAMOS{} Studio}, a Streamlit-based dashboard designed to make multi-objective optimization accessible to users with no programming experience. It offers three main views: (i)~a \textbf{Welcome} tab with a first-launch onboarding wizard and quick-reference tables for the CLI and Python API; (ii)~a \textbf{Problem Builder} that lets users write objective and constraint functions in the browser, adjust bounds and algorithm settings, and see the Pareto front update live after each preview run; and (iii)~an \textbf{Explore Results} tab for loading completed studies, comparing algorithms, and ranking solutions with multi-criteria decision-making (MCDM) methods. The Problem Builder ships with domain-specific templates (engineering design, machine learning, and scheduling) so that domain experts---e.g., a biologist optimizing a drug-candidate pipeline or a physicist fitting a multi-response model---can start from a realistic example rather than a blank editor. Generated problems can be exported as standalone Python scripts or run directly through the \texttt{make\_problem} convenience API (Listing~\ref{lst:make_problem}).

%==============================================================================
\section{Experimental Evaluation}
\label{sec:experiments}
%==============================================================================

We evaluate \VAMOS{} along two dimensions: runtime efficiency across backends and frameworks, and solution quality measured by normalized hypervolume under a fixed-reference-front protocol. We first describe the benchmark suite and the semantic-alignment procedure that ensures cross-framework fairness, then report backend-level runtime comparisons and a scalability study of the vectorized architecture, followed by cross-framework runtime comparisons, solution-quality summaries, and statistical analyses.

\subsection{Benchmark suite and configuration alignment}

We evaluate runtime and solution quality on widely used synthetic benchmarks: ZDT1--4 and ZDT6~\cite{zitzler2000zdt} (two objectives; ZDT5 is excluded because it uses a binary encoding), DTLZ1--7~\cite{deb2002dtlz} (three objectives), and WFG1--9~\cite{huband2006wfg} (two objectives). Problem dimensionalities are fixed to standard definitions: for example, DTLZ2/3/4 use $n = m + k - 1$ with $m=3$ and $k=10$ (thus $n=12$). Non-standard DTLZ dimensionalities are permitted but explicitly flagged to avoid accidental comparisons against canonical settings.
Unless otherwise stated, each (problem, framework) configuration is executed for 50{,}000 objective evaluations with 30 independent seeds.

Fig.~\ref{fig:protocol} summarizes the benchmark protocol and semantic-alignment workflow used to ensure cross-framework fairness and to support the ``same quality, faster'' claim.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/PROTOCOL.png}
  \caption{Benchmark protocol and semantic alignment: map equivalent NSGA-II settings across frameworks, validate objective definitions by sampling decision vectors within bounds, run independent seeds under a fixed evaluation budget, and compute normalized hypervolume using fixed reference fronts.}
  \label{fig:protocol}
\end{figure*}

Table~\ref{tab:alignment_checklist} summarizes the key semantic-alignment constraints enforced in our benchmarking pipeline.

\begin{table}[htbp]
\centering
\caption{Semantic alignment checklist for cross-framework benchmarking (NSGA-II baseline)}
\label{tab:alignment_checklist}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{p{0.22\columnwidth}p{0.72\columnwidth}}
\toprule
\textbf{Item} & \textbf{Enforced alignment} \\
\midrule
Benchmarks & Shared bounds/parameterizations; validated via random decision-vector sampling. \\
Budget & Fixed 50{,}000 evaluations per run, same across frameworks. \\
Operators & SBX ($p_c{=}1.0$, $\eta_c{=}20$) + PM ($\eta_m{=}20$, $p_m{=}1/n$), mapped explicitly. \\
Survival & Tournament selection; rank--crowding survival. \\
Timing & Wall-clock median over seeds; Numba warm timings exclude JIT. \\
Quality & Normalized hypervolume on final non-dominated set with fixed reference fronts. \\
\bottomrule
\end{tabular}
\end{table}

Although \VAMOS{} implements nine MOEAs, our cross-framework comparison focuses on algorithms with the closest semantic matches across libraries: NSGA-II (baseline plus steady-state and archive variants), SMS-EMOA, and MOEA/D. This scope avoids confounding effects from missing implementations or materially different operator and termination semantics in specific Frameworks.

To ensure comparability across frameworks, we standardize the algorithmic settings of NSGA-II: population size $N=100$, SBX crossover probability $p_c=1.0$ and distribution index $\eta_c=20$, polynomial mutation distribution index $\eta_m=20$ and mutation probability $p_m=1/n$, tournament selection, and rank--crowding survival. For each framework we map these settings to the closest available implementation.

For SMS-EMOA we follow the same protocol and align what we can: population size $N=100$, SBX ($p_c=1.0$, $\eta_c=20$), polynomial mutation ($p_m=1/n$, $\eta_m=20$), random parent selection, and a steady-state $(\mu+1)$ loop (one offspring per iteration). However, the hypervolume-contribution reference point is not defined consistently across Frameworks: \VAMOS{} and jMetalPy compute contributions in raw objective space with an adaptive reference point $r = \max(F) + 1$, while pymoo normalizes objectives and uses a fixed shifted reference point $r = \mathbf{1} + \epsilon$ (we set $\epsilon=1$). We therefore report the cross-framework SMS-EMOA comparison with this caveat.

For MOEA/D we align population size $N=100$, weight vectors (uniform for two objectives; a shared $W3D\_100$ set for three objectives), neighborhood size $T=20$, neighborhood selection probability $\delta=0.9$, and replacement limit $\eta=20$ (effectively unrestricted, matching pymoo's update rule). We use penalty-based boundary intersection (PBI) scalarization with $\theta=5$ and a differential evolution (DE)/current/1/bin-style crossover ($CR=1.0$, $F=0.5$) followed by polynomial mutation ($p_m=1/n$, $\eta_m=20$), mapping these settings to the closest available implementation in each toolkit.

This workflow follows standard practice in comparative MOEA studies (independent runs, indicator computation, and structured reporting) and is consistent with the experimentation methodology described in jMetalPy~\cite{benitez2019jmetalpy}. Our main extension is to make the \emph{inputs} to this workflow comparable across Frameworks by enforcing semantic alignment of both problem definitions and operator probabilities.

Crucially, we align \emph{semantics} (not only parameter names) and enforce consistent benchmark definitions. Two examples illustrate why this matters. First, in \textbf{pymoo} the polynomial mutation operator exposes both an individual-level probability (\texttt{prob}) and a per-variable probability (\texttt{prob\_var}); to match the standard ``$p_m=1/n$ per decision variable'' setting used by jMetalPy and DEAP, we set \texttt{prob}=1 and \texttt{prob\_var}=1/n. Second, some libraries ship benchmark problems under canonical names but with library-specific domains; for instance, Platypus' built-in ZDT base class uses $[0,1]^n$, whereas ZDT4 is defined with $x_1\in[0,1]$ and $x_{2..n}\in[-5,5]$.

Therefore, for Frameworks or APIs where problem definitions differ (notably DEAP and Platypus for ZDT4), we evaluate individuals using a shared reference implementation of the benchmark function and bounds (validated against pymoo) while still using each framework's NSGA-II implementation and operators. For WFG we adopt pymoo's canonical parameterization for two objectives ($k=4$, $l=20$ for $n=24$) and pass parameters explicitly when a toolkit exposes different defaults; in Platypus we wrap pymoo's WFG definitions to avoid definition drift. While full equivalence cannot be guaranteed due to framework-specific details (tie-breaking, boundary handling, duplicate elimination, etc.), these choices aim to make the comparison as fair as possible by removing confounders unrelated to algorithmic overhead and numerical kernels.

As an additional validity check, we sample 64 random decision vectors per problem uniformly within the specified bounds and verify that objective values match across implementations within a numerical tolerance ($\mathrm{rtol}=10^{-6}$, $\mathrm{atol}=10^{-8}$). This guards against silent benchmark-definition drift (e.g., differing domains or parameterizations) that could invalidate hypervolume comparisons.

\subsection{Runtime measurement}

All runtimes are measured wall-clock using high-resolution timers and include algorithm overhead and objective evaluations. Unless otherwise stated, we use a fixed budget of 50{,}000 objective evaluations per run and report medians across 30 independent seeds, with per-problem and per-family summaries.

For Numba-accelerated backends, we distinguish two timing policies. \textbf{Warm} timings exclude one-time JIT compilation by performing a short warmup run in the same process before starting the timer (2{,}000 warmup evaluations; not counted in the reported time). \textbf{Cold} timings measure the first run from a fresh process and therefore include JIT compilation overhead. Unless otherwise stated, we report warm timings to reflect steady-state throughput; cold-start costs can be reproduced with the provided scripts.

Experiments were run on a workstation with an Intel(R) Core(TM) Ultra 9 185H central processing unit (CPU) (16 cores, 22 logical processors) and 32\,GB random-access memory (RAM), running Microsoft Windows 11 Home (build 26200). We used Python 3.12.3 and evaluated \VAMOS{} 0.1.0 (NumPy 2.3.5 with OpenBLAS 0.3.30; Numba 0.63.1) against pymoo 0.6.1.6, jMetalPy (local source checkout at commit \texttt{fecb85c}, accessed January 2026), DEAP 1.4.3, and Platypus (local source checkout at commit \texttt{fe7aeff}, accessed January 2026).

\subsection{Normalized hypervolume protocol}
\label{sec:hv}

Hypervolume (HV), denoted $\HV(A,r)$, measures the Lebesgue measure of the region dominated by an approximation set $A$ with respect to a reference point $r$ (for minimization)~\cite{zitzler2003performance}.
Because HV is scale-dependent and sensitive to the choice of $r$, naive implementations can produce values that are not comparable across runs (e.g., if $r$ is expanded per run) and can be misleading when dominated solutions are included. To ensure a stable and comparable quality metric, we adopt the following protocol:
\begin{enumerate}
  \item \textbf{Non-dominated filtering}: for every framework we compute HV on the final non-dominated set only.
  \item \textbf{Fixed reference Pareto fronts}: for each benchmark problem we store a dense reference Pareto front $P_{\text{ref}}$ in \texttt{data/reference\_fronts/}. For ZDT and DTLZ (except DTLZ7) these fronts are generated analytically; for DTLZ7 and WFG we generate $P_{\text{ref}}$ by dense sampling followed by non-dominated filtering.
  \item \textbf{Fixed reference point}: we set $r = \max(P_{\text{ref}}) + \epsilon$ (component-wise) with a small $\epsilon$, and we disallow run-dependent expansion.
  \item \textbf{Normalization}: we report $\HV(A,r) / \HV(P_{\text{ref}},r)$, yielding a dimensionless score typically in $[0,1]$ and reducing sensitivity to objective scaling.
\end{enumerate}
For WFG problems, the reference front is necessarily an approximation; we use large Pareto-set samples and non-dominated filtering, and we increase density for particularly sensitive cases (e.g., WFG2) to avoid underestimating $\HV(P_{\text{ref}}, r)$ and obtaining normalized HV slightly above~1.

\textcolor{red}{As a complementary quality indicator, we also report the Inverted Generational Distance Plus (IGD+)~\cite{ishibuchi2015igd}, which measures the average minimum distance from each reference-front point to the closest solution in the approximation set (lower is better). Unlike HV, IGD+ captures both proximity and diversity with respect to the entire reference front. We report IGD+ summaries alongside normalized HV in Section~\ref{sec:quality}.}

\subsection{\VAMOS{} backend comparison}

Table~\ref{tab:backends} reports median runtime for \VAMOS{} backends aggregated by problem family.

\begin{table}[htbp]
\centering
\caption{\VAMOS{} backend comparison: median runtime (seconds) by problem family\protect\footnotemark}
\label{tab:backends}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Backend} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Average\textsuperscript{*}} \\
\midrule
Numba & \textbf{2.62} & \textbf{3.46} & \textbf{4.03} & \textbf{3.37} \\
MooCore & 2.69 & 3.52 & 4.16 & 3.45 \\
NumPy & 5.18 & 6.82 & 7.37 & 6.46 \\
\bottomrule
\end{tabular}
\end{table}
\footnotetext{Average\textsuperscript{*} in summary tables denotes the mean of family-level medians (3 values). Per-problem averages over all 21 problems are reported in the appendix (Tables~\ref{tab:detailed_backends} and~\ref{tab:detailed_comparison}).}

\subsection{Scalability of vectorized acceleration}
\label{sec:scaling}

To quantify how the array-native architecture scales, we measure wall-clock runtime of the Numba and NumPy backends as a function of population size on three representative problems (ZDT4, DTLZ2, WFG2) using NSGA-II with 50{,}000 evaluations and 5 independent seeds per configuration. Population sizes range from 50 to 800; all other settings match the main benchmark protocol.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/scaling.png}
  \caption{Runtime scaling of \VAMOS{} backends (Numba vs.\ NumPy) as a function of population size on three representative problems. Shaded regions indicate the interquartile range across 5 seeds. NumPy runtime grows steeply with population size due to increasing Python-level overhead in per-generation operations, while Numba runtime remains nearly flat, demonstrating that JIT-compiled kernels amortize loop overhead effectively on larger arrays.}
  \label{fig:scaling}
\end{figure*}

Fig.~\ref{fig:scaling} shows that the runtime gap between NumPy and Numba widens substantially as the population grows. At $N=50$ the two backends perform similarly (and NumPy is even marginally faster on ZDT4), but by $N=800$ the NumPy backend requires 30\,s on all three problems while Numba stays below 12\,s---a $3\text{--}5\times$ speedup. The effect is consistent across problem families despite their different dimensionalities ($n_{\text{var}}=10$, 12, and 24 for ZDT4, DTLZ2, and WFG2 respectively). Crucially, the Numba curve remains nearly flat across population sizes, confirming that the JIT-compiled kernels scale sub-linearly with array size in the per-evaluation cost, whereas NumPy's interpreted dispatch overhead accumulates with each generation cycle. These results reinforce the practical relevance of the vectorized design for large-scale benchmarking studies that require large populations or many independent seeds under a fixed compute budget.

\textcolor{red}{\subsection{Convergence analysis}}

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/convergence.png}
  \caption{\textcolor{red}{Convergence of normalized hypervolume over 50{,}000 evaluations for \VAMOS{} (Numba) and pymoo on DTLZ2 (3 objectives, 12 variables) and ZDT1 (2 objectives, 30 variables). Solid/dashed lines show median HV across 30 seeds; shaded regions indicate the interquartile range. Both frameworks follow nearly identical convergence trajectories, confirming that the semantic alignment protocol produces equivalent algorithmic behavior.}}
  \label{fig:convergence}
\end{figure*}

\textcolor{red}{To verify that runtime speedups do not come at the expense of convergence rate, we track normalized hypervolume at 1{,}000-evaluation intervals over the full 50{,}000-evaluation budget. Fig.~\ref{fig:convergence} compares \VAMOS{} and pymoo on two representative problems: DTLZ2 (3 objectives, 12 variables) and ZDT1 (2 objectives, 30 variables). The convergence curves are nearly indistinguishable, with overlapping interquartile ranges across all 30 seeds. Both frameworks reach the same final quality at the same evaluation counts, confirming that the performance gains observed in runtime measurements are not accompanied by degraded convergence behavior.}

\textcolor{red}{\subsection{Pareto front visualization}}

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/pareto_fronts.png}
  \caption{\textcolor{red}{Final non-dominated sets from \VAMOS{}, pymoo, and jMetalPy on ZDT1 (left, 2-objective) and DTLZ2 (right, 3-objective), overlaid with the reference Pareto front. All three frameworks produce well-distributed approximation sets that closely track the reference front, confirming solution-quality equivalence under the fixed-reference-front protocol.}}
  \label{fig:pareto_fronts}
\end{figure*}

\textcolor{red}{Fig.~\ref{fig:pareto_fronts} provides a visual comparison of the final non-dominated sets produced by \VAMOS{}, pymoo, and jMetalPy on ZDT1 (2D Pareto front) and DTLZ2 (3D Pareto front). In both cases, the solutions from all three frameworks overlap closely with the reference front and with each other, providing qualitative confirmation of the quantitative equivalence reported in the hypervolume and IGD+ summaries. The 3D DTLZ2 plot illustrates that \VAMOS{}'s approximation set covers the spherical Pareto surface with comparable uniformity to the established frameworks.}

\subsection{Cross-framework comparison}

Table~\ref{tab:frameworks_perf} compares \VAMOS{} against other Python frameworks on runtime. \VAMOS{} is evaluated with its accelerated backend (Numba) for the headline comparison, while additional backends are reported in the appendix.
We repeat the same runtime protocol for NSGA-II variants (steady-state and external archive), SMS-EMOA (steady-state, one offspring per iteration), and MOEA/D (decomposition with differential-evolution variation), reported in Tables~\ref{tab:frameworks_perf_nsgaii_ss} and~\ref{tab:frameworks_perf_nsgaii_archive} and Tables~\ref{tab:frameworks_perf_smsemoa} and~\ref{tab:frameworks_perf_moead}.
These supplementary runtime tables are placed in Appendix~\ref{app:variants} to keep the main results focused.

\begin{table}[htbp]
\centering
\caption{NSGA-II (generational). Median runtime (seconds) by problem family across all frameworks}
\label{tab:frameworks_perf}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Framework} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Average\textsuperscript{*}} \\
\midrule
VAMOS & \textbf{2.62} & \textbf{3.46} & \textbf{4.03} & \textbf{3.37} \\
pymoo & 3.22 & 3.86 & 4.56 & 3.88 \\
jMetalPy & 11.55 & 14.91 & 43.94 & 23.47 \\
DEAP & 14.99 & 21.42 & 43.72 & 26.71 \\
Platypus & 20.28 & 29.01 & 50.16 & 33.15 \\
\bottomrule
\end{tabular}
\end{table}

Fig.~\ref{fig:runtime_comparison} visualizes these relative runtimes. Each bar represents the ratio of a competitor's median family-level runtime to \VAMOS{} (Numba); the dashed line at $2^0=1$ marks parity. Platypus, DEAP, and jMetalPy are consistently $4\text{--}12\times$ slower, with the largest gaps on WFG problems where per-solution overhead is amplified by higher dimensionality ($n=24$). pymoo is the closest competitor, staying within $1.1\text{--}1.2\times$ of \VAMOS{} on ZDT and DTLZ, though the gap widens on WFG. Error bars indicate variability across per-problem medians within each family.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/time_comparison.png}
  \caption{Runtime of each framework relative to \VAMOS{} (Numba) for generational NSGA-II, grouped by problem family ($\log_2$ scale). Bars above the dashed parity line ($2^0=1$) indicate that the competitor is slower. Error bars show variability across per-problem medians within each family. All runs use 50{,}000 evaluations and 30 seeds.}
  \label{fig:runtime_comparison}
\end{figure*}

\paragraph{Steady-state NSGA-II.}
Some Frameworks expose a steady-state (incremental-replacement) variant of NSGA-II, typically implemented as a $(\mu+1)$ loop with one offspring per iteration and one survivor replaced.
We benchmark this mode by aligning NSGA-II settings across frameworks and setting the offspring batch size to~1.
In \VAMOS{}, we configure steady-state mode with \texttt{offspring\_size}=1 and \texttt{replacement\_size}=1; in pymoo we set \texttt{n\_offsprings}=1; in jMetalPy we set \texttt{offspring\_population\_size}=1; and for DEAP we implement the corresponding one-offspring loop around \texttt{selNSGA2}.
Platypus does not provide an equivalent steady-state NSGA-II API, so we omit it from this comparison (see Appendix~\ref{app:variants}).

\paragraph{NSGA-II with external archive.}
To assess the overhead of maintaining an external elite set, we also benchmark NSGA-II with an \emph{unbounded} external archive that accumulates non-dominated solutions throughout the run.
For this variant, we compute normalized hypervolume on the archive contents (rather than the final population) to reflect the larger retained approximation set.
In \VAMOS{}, we enable an external archive with \texttt{unbounded=True} and size initialized to the population size; in pymoo we maintain a separate unbounded archive updated from the evolving population/offspring; in jMetalPy we use an NSGA-II implementation with an archive hook; for DEAP we maintain an equivalent separate unbounded archive; and in Platypus we pass an \texttt{Archive()} object (see Appendix~\ref{app:variants}).

\textcolor{red}{\paragraph{Memory footprint.}
To complement the runtime comparison, we measure peak resident-set-size (RSS) overhead during NSGA-II runs on three representative problems (ZDT1, DTLZ2, WFG4) using 50{,}000 evaluations and 5 seeds per framework. \VAMOS{} achieves a median peak memory of 0.28\,MB, compared to 0.84\,MB for pymoo ($3\times$ higher) and 0.50\,MB for jMetalPy ($1.8\times$ higher). The lower memory footprint reflects the array-native representation, which stores the entire population in contiguous NumPy arrays rather than materializing individual solution objects with per-instance attribute dictionaries.}

\subsection{Solution quality summary}
\label{sec:quality}

Table~\ref{tab:frameworks_hv} summarizes normalized hypervolume for the NSGA-II baseline across frameworks, aggregated by benchmark family. We report per-problem medians across seeds, and summarize these per-problem medians using median and interquartile range (IQR) within each family, yielding a descriptive view of solution quality under the fixed normalization protocol (Section~\ref{sec:hv}).

\begin{table*}[htbp]
\centering
\caption{NSGA-II. Normalized hypervolume summary (median (IQR)) by problem family across frameworks}
\label{tab:frameworks_hv}
\begin{tabular}{lcccc}
\toprule
\textbf{Framework} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Overall\textsuperscript{*}} \\
\midrule
VAMOS & \textbf{0.990 (0.008)} & 0.799 (0.153) & \textbf{0.934 (0.140)} & \textbf{0.934 (0.181)} \\
pymoo & \textbf{0.990 (0.008)} & 0.792 (0.180) & 0.846 (0.222) & 0.914 (0.192) \\
jMetalPy & \textbf{0.990 (0.009)} & 0.797 (0.155) & 0.846 (0.224) & 0.915 (0.192) \\
DEAP & \textbf{0.990 (0.008)} & \textbf{0.802 (0.170)} & 0.839 (0.226) & 0.917 (0.186) \\
Platypus & \textbf{0.990 (0.008)} & 0.786 (0.209) & 0.920 (0.139) & 0.920 (0.196) \\
\bottomrule
\end{tabular}
\end{table*}

Across ZDT, all frameworks attain nearly identical normalized HV, suggesting that benchmark definitions and operator semantics are effectively aligned. For DTLZ, medians are tightly clustered across frameworks, while WFG exhibits the largest dispersion; \VAMOS{} achieves the highest median on WFG and overall. Together with the runtime results, these summaries support improved throughput without degraded final quality under the fixed-reference-front protocol.

\textcolor{red}{Tables~\ref{tab:frameworks_hv_nsgaii_ss} and~\ref{tab:frameworks_hv_nsgaii_archive} extend this analysis to the NSGA-II steady-state and external-archive variants. For the steady-state variant, all three frameworks achieve nearly identical overall HV. For the archive variant, pymoo and jMetalPy reach near-perfect normalized HV (1.000 overall), while \VAMOS{} attains 0.944---a gap attributable to differences in how the unbounded archive interacts with the survival operator across frameworks.}

\input{frameworks_hv_nsgaii_ss.tex}
\input{frameworks_hv_nsgaii_archive.tex}

\textcolor{red}{As a complementary quality indicator, Tables~\ref{tab:frameworks_igd_nsgaii}--\ref{tab:frameworks_igd_moead} report IGD+ summaries across frameworks for NSGA-II, SMS-EMOA, and MOEA/D. For NSGA-II, IGD+ values are tightly clustered across frameworks (overall median 0.0108 for \VAMOS{} and pymoo, 0.0105 for jMetalPy), mirroring the HV equivalence. SMS-EMOA achieves the lowest IGD+ overall (0.0078 for all three frameworks), reflecting the steady-state indicator-based selection's effectiveness at maintaining proximity and diversity. For MOEA/D, \VAMOS{} (0.0102) and jMetalPy (0.0102) outperform pymoo (0.0160), consistent with the known difficulty of MOEA/D under certain weight-vector configurations.}

\input{frameworks_igd_nsgaii.tex}
\input{frameworks_igd_smsemoa.tex}
\input{frameworks_igd_moead.tex}

\subsection{Statistical analysis}
\label{sec:stats}

To move beyond descriptive summaries, we apply Wilcoxon signed-rank tests (paired by seed) comparing VAMOS against each competitor on every problem, with Holm step-down correction for familywise error control ($\alpha=0.05$). We report the Vargha--Delaney $\hat{A}_{12}$ effect size~\cite{vargha2000critique} alongside each test ($\hat{A}_{12}>0.5$ favors VAMOS; thresholds: negligible${}<0.56$, small${}<0.64$, medium${}<0.71$, large${\ge}0.71$). We also report paired-bootstrap 90\% confidence intervals for the relative normalized-HV difference $\Delta = (\HV_{\text{VAMOS}} - \HV_{\text{fw}})/\HV_{\text{fw}}$, and we declare \emph{equivalence} when the entire CI falls within $\pm 1\%$.

Table~\ref{tab:stats_hypervolume_nsgaii} presents the per-problem Wilcoxon results for the NSGA-II baseline. Of 21 problems, only one (ZDT6) shows a statistically significant difference in normalized HV after Holm correction, and the magnitude is small ($\Delta \approx 0.3\%$). Table~\ref{tab:hv_equivalence_summary_nsgaii} summarizes the equivalence analysis across all frameworks: VAMOS is equivalent to pymoo on 14/21 problems and non-inferior on 16/21, with a median $\Delta$HV of $-0.03\%$. Similar patterns hold for jMetalPy, DEAP, and Platypus. Tables~\ref{tab:hv_robustness_summary_nsgaii} and~\ref{tab:hv_equivalence_ci_nsgaii} detail robustness and per-problem CIs. Corresponding analyses for SMS-EMOA and MOEA/D are provided in Appendix~\ref{app:stats}.
Detailed NSGA-II statistical tables are provided in Appendix~\ref{app:stats_nsgaii}.

\textcolor{red}{\subsection{Extended algorithm comparisons}}

\textcolor{red}{To assess the generality of \VAMOS{} beyond the core NSGA-II/SMS-EMOA/MOEA/D comparison, we conduct preliminary evaluations on additional algorithms and problem classes. These comparisons are necessarily less comprehensive (fewer frameworks per algorithm, smaller problem sets) but provide evidence that \VAMOS{}'s design extends to a broader algorithmic scope.}

\textcolor{red}{\paragraph{NSGA-III.}
We compare \VAMOS{} and pymoo on NSGA-III across 12 problems (ZDT1--4, ZDT6, DTLZ1--7) using 50{,}000 evaluations and 30 seeds. \VAMOS{} achieves a substantially higher median normalized HV (0.876 vs.\ 0.785 for pymoo). The gap is most pronounced on DTLZ3 (0.842 vs.\ 0.000, where pymoo fails to converge), DTLZ5 (0.907 vs.\ 0.785), and DTLZ6 (0.471 vs.\ 0.238). Both frameworks use structured reference directions, but the number of retained solutions differs (91 for \VAMOS{} vs.\ 7--91 for pymoo on two-objective problems, reflecting different reference-direction generation strategies). Runtime is not directly comparable, as pymoo's NSGA-III implementation uses a different internal architecture; however, the quality advantage suggests that \VAMOS{}'s reference-direction handling and survival operator produce more effective population distributions under the fixed evaluation budget.}

\textcolor{red}{\paragraph{SPEA2.}
We compare \VAMOS{} and jMetalPy on SPEA2 across 11 problems (ZDT and DTLZ families). Solution quality is comparable (median HV 0.966 for \VAMOS{} vs.\ 0.967 for jMetalPy). However, \VAMOS{} is slower on this algorithm (median 46.7\,s vs.\ 14.6\,s). This reversal reflects the nature of SPEA2's steady-state loop: with one offspring per iteration, the algorithm performs 50{,}000 selection--variation--survival cycles, and the overhead of dispatching array operations for single-individual batches outweighs the benefit of vectorization. This limitation is consistent with the pattern observed for steady-state variants in Section~\ref{sec:experiments} and motivates future work on adaptive batch sizing.}

\textcolor{red}{\paragraph{Constrained problems.}
We evaluate constrained multi-objective optimization on five problems---C1-DTLZ1, C2-DTLZ2, MW1, MW3, and MW6~\cite{ma2019mw}---comparing \VAMOS{} against pymoo using NSGA-II with 60{,}000 evaluations and 30 seeds. \VAMOS{} finds feasible solutions on all runs across all problems, whereas pymoo returns zero feasible solutions on 28 out of 30 seeds for C1-DTLZ1. On MW1, \VAMOS{} achieves substantially higher median HV (0.993 vs.\ 0.910), while C2-DTLZ2 and MW3 show similar performance. These results suggest that \VAMOS{}'s feasibility-rule constraint handling is effective at maintaining feasible populations on problems where alternative constraint-handling approaches may struggle.}

\textcolor{red}{\paragraph{Three-objective WFG.}
As a preliminary many-objective evaluation, we compare \VAMOS{} and pymoo on three representative WFG problems (WFG1, WFG4, WFG9) scaled to three objectives with 50{,}000 evaluations and 30 seeds. pymoo achieves slightly higher median HV (0.781 vs.\ 0.740), indicating room for improvement in \VAMOS{}'s handling of higher-dimensional objective spaces. This comparison validates that \VAMOS{} supports many-objective problems; a broader evaluation with additional problems and objectives is left to future work.}

\subsection{Discussion}

The observed speedups are primarily due to (i) the array-based representation that removes per-solution Python overhead, and (ii) JIT compilation of hot operators and utilities in the Numba backend. The scalability study (Fig.~\ref{fig:scaling}) confirms this mechanism: the NumPy backend's runtime grows steeply with population size, reflecting per-generation interpreted overhead, while the Numba backend remains nearly flat. Table~\ref{tab:frameworks_hv} summarizes that these performance gains are not obtained at the expense of final solution quality under the fixed-reference-front normalized-HV protocol, avoiding artifacts caused by run-dependent reference points.

As Fig.~\ref{fig:runtime_comparison} illustrates, the cross-framework gaps are problem-family dependent: WFG problems exhibit the largest relative slowdowns for object-centric libraries, consistent with their higher dimensionality amplifying per-solution overhead. The magnitude of the speedup also varies across algorithm variants in a pattern consistent with this overhead model. Steady-state variants (Tables~\ref{tab:frameworks_perf_nsgaii_ss} and~\ref{tab:frameworks_perf_smsemoa}) show the largest relative speedups because they perform one selection--variation--survival cycle per offspring: frameworks with high per-iteration Python overhead incur that cost 50{,}000 times (one per evaluation) rather than 500 times (one per generation of 100 offspring). \VAMOS{}'s array-kernel dispatch amortizes this overhead more effectively, widening the gap.

Not all comparisons favor \VAMOS{}. For MOEA/D on WFG problems (Table~\ref{tab:frameworks_perf_moead}), jMetalPy achieves lower median runtime (8.34\,s vs.\ 11.12\,s). This is attributable to differences in the decomposition inner loop: jMetalPy's MOEA/D processes one subproblem at a time with lightweight scalar operations, a pattern where Python-level overhead is modest and \VAMOS{}'s array-batching strategy does not yield the same advantage as in generational algorithms with population-wide survival.

Beyond runtime, \VAMOS{} offers a level of component-level configurability that distinguishes it from the closest competitor. pymoo provides a modular API for standard algorithm configurations, but does not natively support configuring the offspring batch size to obtain steady-state variants, attaching bounded or unbounded external archives to arbitrary algorithms, switching between multiple constraint-handling modes (feasibility rules, penalty functions, or $\varepsilon$-relaxation). In contrast, \VAMOS{} exposes all of these as first-class configuration options---as demonstrated experimentally by the steady-state NSGA-II (Table~\ref{tab:frameworks_perf_nsgaii_ss}) and external-archive (Table~\ref{tab:frameworks_perf_nsgaii_archive}) variants, which required only parameter changes in \VAMOS{} but custom wrappers or workarounds in other Frameworks. This configurability is complemented by the ability to swap the compute backend (NumPy, Numba, MooCore) with a single parameter, enabling users to trade compilation overhead for throughput depending on their use case.

The third design axis---accessibility---addresses a different audience than the first two. While performance and configurability primarily benefit MOEA researchers running large-scale benchmarking studies, the minimal-boilerplate API and graphical tooling target domain experts who need multi-objective optimization but lack a background in metaheuristics. The two-line \texttt{optimize} entry point, the \texttt{make\_problem} factory, and the \texttt{algorithm="auto"} mode eliminate the need to understand algorithm internals, operator semantics, or framework-specific abstractions. VAMOS Studio further lowers the barrier by providing a browser-based environment with domain-specific templates where users can define problems, run optimizations, and explore Pareto fronts without writing any code. As \textcolor{red}{Table~\ref{tab:framework-comparison}} shows, no other Python MOEA framework combines all three axes: array-native performance, rich configurability, and a two-line API with a graphical interface. We note that usability is assessed here through objective proxies (lines of code, number of required imports, and feature availability) rather than controlled user studies; a formal evaluation with domain-expert participants is left to future work.

These speedups have direct practical implications. A 4--10$\times$ reduction in per-run wall-clock time means that the same compute budget can accommodate proportionally more independent seeds, larger population sizes, or denser hyperparameter sweeps---precisely the conditions required for statistically rigorous benchmarking studies. Moreover, the scaling study (Section~\ref{sec:scaling}) shows that the advantage grows with population size, so the benefit is amplified in exactly the large-scale regimes where it matters most.

We also note that both \VAMOS{} and pymoo obtain zero normalized hypervolume on DTLZ3 under MOEA/D with 50{,}000 evaluations. This is a known difficulty: DTLZ3 is a multimodal problem designed to test convergence, and MOEA/D with PBI scalarization and the evaluation budget used here is insufficient to escape local fronts consistently. This outcome is not framework-specific but reflects the interaction between algorithm configuration and problem difficulty.

A related observation concerns SMS-EMOA on DTLZ4 (Appendix~\ref{app:stats}): \VAMOS{} attains a median normalized HV of 0.898 versus 0.671 for pymoo, yet the Wilcoxon test reports $p=1.0$ after Holm correction. This reflects very high variance across seeds due to the deceptive density bias in DTLZ4, which causes many seeds to converge to a narrow region of the front. The high IQR makes the paired differences too noisy for the test to reject the null hypothesis despite the large median gap.

\textcolor{red}{The convergence analysis (Fig.~\ref{fig:convergence}) and Pareto front visualization (Fig.~\ref{fig:pareto_fronts}) reinforce the solution-quality equivalence: \VAMOS{} and pymoo follow nearly identical convergence trajectories on both ZDT1 and DTLZ2, and the final non-dominated sets from all three frameworks overlap closely with the reference fronts. These visual confirmations complement the statistical analyses and provide intuitive evidence that the runtime speedups do not degrade convergence behavior.}

\textcolor{red}{The memory footprint measurements further illustrate the practical advantages of the array-native design: \VAMOS{} uses $3\times$ less peak memory than pymoo and $1.8\times$ less than jMetalPy on the same problems. For large-scale studies involving many concurrent runs or high-dimensional problems, this reduced memory overhead can be a meaningful practical benefit.}

\textcolor{red}{The extended algorithm comparisons reveal both strengths and limitations of the array-native approach. The NSGA-III results (median HV 0.876 vs.\ 0.785 for pymoo) are encouraging but not directly attributable to the array architecture alone---differences in reference-direction generation and survival-operator details likely contribute. The SPEA2 comparison confirms a known limitation: for steady-state algorithms that process one individual at a time, \VAMOS{}'s array-dispatch overhead outweighs vectorization benefits, resulting in slower execution than jMetalPy's lightweight per-individual loop. The constrained-problem results demonstrate that \VAMOS{}'s feasibility-rule constraint handling is effective, with \VAMOS{} finding feasible solutions on all 150 runs where pymoo fails on 28/30 seeds for C1-DTLZ1. The three-objective WFG comparison validates many-objective support but shows room for improvement (median HV 0.740 vs.\ 0.781 for pymoo), motivating the many-objective benchmarking extension listed in future work.}

\subsection{Threats to validity}

Despite careful alignment of dimensions, budgets, problem definitions, and operator settings, cross-framework comparisons remain subject to several threats: (i) operator implementations may differ in subtle details (e.g., boundary handling, duplicate elimination, tie-breaking in survival), (ii) random number generation and seeding semantics vary across libraries, and (iii) library-specific overheads (data conversion, object materialization) may affect measured runtime. We mitigate major sources of unfairness by enforcing consistent benchmark bounds (e.g., ZDT4) and matching operator semantics where APIs differ (e.g., mutation probability in pymoo).

Our cross-framework comparison is deliberately scoped to Python MOEA libraries whose inner loops are predominantly executed in Python. Libraries such as PyGMO (Python bindings to the C++ PaGMO2 library) execute algorithms and benchmark functions fully in compiled code, yielding fundamentally different runtime profiles; we therefore treat them as an orthogonal C++ baseline and leave their inclusion to future work with an explicit language/runtime study design.

For hypervolume, reference fronts for problems without closed-form Pareto fronts are necessarily approximations; while we generate them densely and fix them across runs, remaining discrepancies can affect absolute normalized values, especially on WFG problems. We therefore treat normalized HV primarily as a comparative metric under a fixed protocol, and we report distributions across multiple seeds rather than relying on single-run outcomes.

%==============================================================================
\section{Conclusion}
\label{sec:conclusions}
%==============================================================================

We presented \VAMOS{}, a high-performance Python framework for multi-objective optimization studies that addresses three practical gaps in existing Frameworks: (i)~a vectorized core with pluggable compute kernels that reduces inner-loop overhead; (ii)~rich component-level configurability---including interchangeable variation operators, configurable offspring batch size for steady-state variants, optional bounded or unbounded external archives, multiple constraint-handling modes---that enables researchers to explore algorithmic design choices without custom code; and (iii)~an accessible API and graphical interface that lowers the entry barrier for domain experts with basic Python skills, requiring as few as two lines of code for a complete optimization run. We also release a reproducible cross-framework benchmarking pipeline with semantic alignment and fixed reference Pareto fronts for normalized hypervolume reporting. Under this protocol, \VAMOS{} achieves competitive normalized hypervolume (Table~\ref{tab:frameworks_hv}) while substantially reducing runtime (Tables~\ref{tab:frameworks_perf}--\ref{tab:frameworks_perf_moead}). \textcolor{red}{Convergence analysis (Fig.~\ref{fig:convergence}) and Pareto front visualization (Fig.~\ref{fig:pareto_fronts}) confirm that these speedups do not degrade convergence behavior or solution quality. The framework achieves $3\times$ lower peak memory than pymoo, and extended evaluations on NSGA-III, SPEA2, constrained problems (C-DTLZ, MW), and three-objective WFG demonstrate broad algorithmic coverage---with \VAMOS{} finding feasible solutions where alternative approaches fail and delivering competitive or superior solution quality across problem classes.}

\subsection{Future work}
\begin{itemize}
  \item \textbf{Many-objective benchmarking}: extend the experimental protocol beyond $m=3$ with scalable indicators.
  \item \textbf{Distributed GPU execution}: explore integration with EvoX~\cite{huang2025evox} for multi-node GPU acceleration.
  \item \textbf{Algorithm configuration}: extend the tuning module with interoperability with external configurators such as irace~\cite{lopez2016irace}, and richer tuning benchmarks and reports.
  \item \textbf{Broader comparisons}: extend the benchmark suite to additional MOEAs and frameworks and include time-to-target and other indicators alongside normalized HV.
  \item \textbf{LLM-assisted experiment planning}: we are exploring an integrated assistant that maps a natural-language problem description to a validated experiment configuration, using an optional LLM provider, to further lower the barrier to entry for non-expert users.
\end{itemize}

% --------------------------------------------------------------------------
% DATA AVAILABILITY
% --------------------------------------------------------------------------
\section*{Data and code availability}
All data and code supporting the findings of this study (benchmark
scripts, reference Pareto fronts, and CSV artifacts used to regenerate
tables) are available in the \VAMOS{} repository:\newline
\url{https://github.com/vamos-framework/vamos}.

%==============================================================================
\appendices
\section{Additional runtime comparisons}
\label{app:variants}
Tables in this appendix report supplementary runtime comparisons for NSGA-II variants (steady-state and external archive), SMS-EMOA, and MOEA/D under the same protocol as Section~\ref{sec:experiments}.

\input{frameworks_perf_nsgaii_ss.tex}
\input{frameworks_perf_nsgaii_archive.tex}
\input{frameworks_perf_smsemoa.tex}
\input{frameworks_perf_moead.tex}

\section{Detailed statistical analysis for NSGA-II}
\label{app:stats_nsgaii}
This appendix reports per-problem Wilcoxon signed-rank tests, bootstrap equivalence analyses, robustness summaries, and bootstrap confidence intervals for the NSGA-II baseline discussed in Section~\ref{sec:stats}.
\begingroup
\setlength{\tabcolsep}{2pt}
\input{stats_nsgaii.tex}
\endgroup

\section{Reference fronts and detailed benchmark results}
\label{app:detailed}
%==============================================================================

The repository includes dense reference Pareto fronts for all benchmark problems used for normalized hypervolume computation (\path{data/reference_fronts/}). These fronts are generated analytically when closed forms are available (ZDT and most DTLZ problems) and by dense sampling followed by non-dominated filtering for cases where the Pareto front is disconnected or defined implicitly (DTLZ7 and WFG). For WFG2 we use a higher sampling density (configurable via \path{VAMOS_REF_WFG2_POINTS}) to stabilize HV normalization.

Tables~\ref{tab:detailed_backends} and~\ref{tab:detailed_comparison} report per-problem median runtimes.

\begin{table}[htbp]
\centering
\caption{Detailed \VAMOS{} backend comparison: median runtime (seconds) per problem. Average is computed over all 21 per-problem medians}
\label{tab:detailed_backends}
\begin{tabular}{l|rrr}
\toprule
\textbf{Problem} & \textbf{Numba} & \textbf{MooCore} & \textbf{NumPy} \\
\midrule
dtlz1 & \textbf{2.67} & 2.74 & 5.48 \\
dtlz2 & \textbf{3.01} & 3.04 & 5.83 \\
dtlz3 & 3.60 & \textbf{3.44} & 7.54 \\
dtlz4 & \textbf{3.72} & 3.95 & 7.00 \\
dtlz5 & 3.74 & \textbf{3.68} & 6.72 \\
dtlz6 & \textbf{3.90} & 4.04 & 7.64 \\
dtlz7 & 3.90 & \textbf{3.73} & 7.12 \\
wfg1 & 4.17 & \textbf{4.13} & 7.91 \\
wfg2 & \textbf{3.90} & 3.95 & 7.49 \\
wfg3 & \textbf{3.81} & 3.96 & 7.15 \\
wfg4 & \textbf{3.93} & 4.06 & 7.65 \\
wfg5 & \textbf{4.31} & 4.46 & 7.98 \\
wfg6 & \textbf{4.40} & 4.74 & 7.63 \\
wfg7 & 3.85 & \textbf{3.83} & 7.18 \\
wfg8 & \textbf{4.18} & 4.35 & 7.30 \\
wfg9 & \textbf{3.85} & 4.18 & 6.23 \\
zdt1 & 2.63 & \textbf{2.56} & 4.82 \\
zdt2 & 2.79 & \textbf{2.72} & 5.24 \\
zdt3 & \textbf{2.76} & 2.81 & 5.19 \\
zdt4 & \textbf{2.41} & 2.51 & 5.33 \\
zdt6 & \textbf{2.44} & 2.51 & 5.16 \\
\midrule
\textbf{Average} & \textbf{3.52} & 3.59 & 6.65 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table*}[htbp]
\centering
\caption{Detailed comparison of median runtime (seconds) across all frameworks. Average is computed over all 21 per-problem medians}
\label{tab:detailed_comparison}
\begin{tabular}{l|rrrrr}
\toprule
\textbf{Problem} & \textbf{VAMOS} & \textbf{pymoo} & \textbf{jMetalPy} & \textbf{DEAP} & \textbf{Platypus} \\
\midrule
dtlz1 & \textbf{2.67} & 3.21 & 11.39 & 16.91 & 19.82 \\
dtlz2 & \textbf{3.01} & 3.38 & 12.23 & 17.62 & 25.72 \\
dtlz3 & \textbf{3.60} & 4.44 & 16.82 & 24.92 & 31.03 \\
dtlz4 & \textbf{3.72} & 3.84 & 15.15 & 22.67 & 29.29 \\
dtlz5 & \textbf{3.74} & 4.03 & 14.65 & 21.78 & 30.89 \\
dtlz6 & \textbf{3.90} & 4.52 & 15.28 & 22.82 & 30.98 \\
dtlz7 & \textbf{3.90} & 4.22 & 16.41 & 20.99 & 30.35 \\
wfg1 & \textbf{4.17} & 4.76 & 30.35 & 31.37 & 38.26 \\
wfg2 & \textbf{3.90} & 4.55 & 48.26 & 46.68 & 54.91 \\
wfg3 & \textbf{3.81} & 4.18 & 44.80 & 44.07 & 50.92 \\
wfg4 & \textbf{3.93} & 4.42 & 24.93 & 26.17 & 33.42 \\
wfg5 & \textbf{4.31} & 4.61 & 25.57 & 28.82 & 41.68 \\
wfg6 & \textbf{4.40} & 4.91 & 122.00 & 125.18 & 133.25 \\
wfg7 & \textbf{3.85} & 4.31 & 37.01 & 37.17 & 45.73 \\
wfg8 & \textbf{4.18} & 4.68 & 86.14 & 79.09 & 83.92 \\
wfg9 & \textbf{3.85} & 4.75 & 179.86 & 166.45 & 169.99 \\
zdt1 & \textbf{2.63} & 2.93 & 11.54 & 14.43 & 20.77 \\
zdt2 & \textbf{2.79} & 3.28 & 12.01 & 15.40 & 21.46 \\
zdt3 & \textbf{2.76} & 3.08 & 11.81 & 15.54 & 21.94 \\
zdt4 & \textbf{2.41} & 3.12 & 11.23 & 15.41 & 17.52 \\
zdt6 & \textbf{2.44} & 3.51 & 10.39 & 14.46 & 18.20 \\
\midrule
\textbf{Average} & \textbf{3.52} & 4.03 & 36.09 & 38.47 & 45.24 \\
\bottomrule
\end{tabular}
\end{table*}

%==============================================================================
\section{Statistical analysis for SMS-EMOA and MOEA/D}
\label{app:stats}
%==============================================================================

Tables in this appendix report Wilcoxon signed-rank tests, bootstrap equivalence analyses, and robustness summaries for SMS-EMOA and MOEA/D, following the same methodology as Section~\ref{sec:stats}.

\subsection{SMS-EMOA}

Tables~\ref{tab:stats_hypervolume_smsemoa}--\ref{tab:hv_equivalence_ci_smsemoa} present per-problem Wilcoxon tests, equivalence summaries, robustness summaries, and per-problem bootstrap confidence intervals for SMS-EMOA.

\begingroup
\setlength{\tabcolsep}{2pt}
\input{stats_smsemoa.tex}
\endgroup

\subsection{MOEA/D}

Tables~\ref{tab:stats_hypervolume_moead}--\ref{tab:hv_equivalence_ci_moead} present the corresponding analyses for MOEA/D.

\begingroup
\setlength{\tabcolsep}{2pt}
\input{stats_moead.tex}
\endgroup

%==============================================================================
\section{Algorithm Configuration Spaces}
\label{app:param_space}
%==============================================================================

\textcolor{red}{Table~\ref{tab:algo_overview} provides a compact overview of the nine MOEAs implemented in \VAMOS{} and their configurable parameter spaces across all supported encodings. Each row summarizes the algorithm's selection paradigm, supported encodings, number of available crossover and mutation operators, selection methods, algorithm-specific parameters, and total parameter count (including conditional parameters). The parameter counts reflect the tuning configuration spaces used by the built-in autotuner (Section~\ref{sec:design_goals}) and range from 8 (SMPSO) to 37 (NSGA-II), illustrating the breadth of component-level configurability that \VAMOS{} exposes as first-class configuration options.}

\begin{table*}[htbp]
\centering
\caption{\textcolor{red}{Overview of the nine MOEAs implemented in \VAMOS{} and their configurable parameter spaces across all supported encodings. Encodings: R\,=\,real, P\,=\,permutation, B\,=\,binary, I\,=\,integer, M\,=\,mixed. The ``\#Xover'' and ``\#Mut'' columns report totals across supported encodings for each algorithm. The ``\#Params'' column reports the total number of configurable parameters (base + conditional) in the tuning configuration space. Acronyms: SBX (Simulated Binary Crossover), PM (Polynomial Mutation), DE (Differential Evolution), PBI (Penalty-based Boundary Intersection), SUS (Stochastic Universal Sampling).}}
\label{tab:algo_overview}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llcccclc}
\toprule
\textbf{Algorithm} & \textbf{Paradigm} & \textbf{Encodings} & \textbf{\#Xover} & \textbf{\#Mut} & \textbf{\#Sel} & \textbf{Specific Parameters} & \textbf{\#Params} \\
\midrule
NSGA-II & Dominance + crowding & R, P, B, I, M & 25 & 24 & 4 & Offspring ratio, external archive (type, size, bounded) & 37 \\
AGE-MOEA & Adaptive geometry & R, P, B, I, M & 21 & 22 & --- & Archive policies (crowding, HV, $\varepsilon$-grid, hybrid) & 26 \\
RVEA & Reference vectors & R, P, B, I, M & 21 & 22 & --- & Partitions, $\alpha$ decay, adaptation frequency & 28 \\
MOEA/D & Decomposition & R, P, B, I, M & 17 & 20 & --- & Aggregation (Tchebycheff, WS, PBI), $T$, $\delta$, $\eta$ & 18 \\
IBEA & Indicator ($\varepsilon$/HV) & R, P, B, I, M & 21 & 22 & 1 & Indicator type, scaling factor $\kappa$ & 24 \\
SPEA2 & Strength Pareto & R, P, B, I, M & 21 & 22 & 1 & Archive size, $k$-nearest neighbors & 24 \\
NSGA-III & Reference directions & R, P, B, I, M & 21 & 22 & 1 & Reference directions (auto-computed) & 22 \\
SMS-EMOA & HV contribution & R, P, B, I, M & 21 & 22 & 1 & Reference point (adaptive) & 22 \\
SMPSO & Particle swarm & R, M & --- & 3 & --- & Inertia $\omega$, $c_1$, $c_2$, $v_{\max}$ fraction & 8 \\
\bottomrule
\end{tabular}%
}
\end{table*}

Table~\ref{tab:parameter_space} summarizes the full parameter space considered for NSGA-II in \VAMOS{}, illustrating the depth of component-level configurability described in Section~\ref{sec:design_goals}. The default configuration used throughout the experimental evaluation (Section~\ref{sec:experiments}) corresponds to the values shown in bold.

\begin{table*}[htbp]
\centering
\caption{\textcolor{red}{Summary of the considered parameter space for NSGA-II (real-valued encoding). Parameters in italics are conditional, active only when their corresponding operator is selected. Values in \textbf{bold} indicate the default configuration used in Section~\ref{sec:experiments}. The total number of configurable parameters is 37. Acronyms: LHS (Latin Hypercube Sampling), SBX (Simulated Binary Crossover), BLX (Blend Crossover), PCX (Parent Centric Crossover), UNDX (Unimodal Normal Distribution Crossover), SPX (Simplex Crossover), PM (Polynomial Mutation), SUS (Stochastic Universal Sampling).}}
\label{tab:parameter_space}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Value / Strategy} & \textbf{Conditional Parameters} \\ \midrule
\multirow{8}{*}{General} & Pop.\ Size $\in \Intdomain{20, 200}$ (log-scale); default $\mathbf{100}$ & -- \\
 & Offspring Ratio $\in \{0.25, 0.5, 0.75, \mathbf{1.0}\}$ & -- \\
 & Selection $\in$ \{\textbf{Tournament}, \textcolor{red}{Boltzmann, Ranking, SUS}\} & Pressure $\in \Intdomain{\mathbf{2}, 10}$ \\
 & Use External Archive $\in$ \{True, \textbf{False}\} & -- \\
 & \textit{Archive Unbounded} $\in$ \{True, False\} & \textit{Archive} = True \\
 & \textit{Archive Type} $\in$ \{size\_cap, hvc\_prune\} & \textit{Unbounded} = False \\
 & \textit{Archive Size Factor} $\in \{1, 2, 5, 10\}$ & \textit{Unbounded} = False \\ \midrule
\multirow{2}{*}{Initialization} & \{\textbf{Random}, LHS, Scatter\} & -- \\
 & \textit{Scatter Base Size Factor} $\in \{0.1, 0.2, 0.3, 0.5, 0.75, 1.0\}$ & \textit{Init.} = Scatter \\ \midrule
\multirow{12}{*}{Crossover} & \multicolumn{2}{l}{\textit{Global:} Probability $\in \Realdomain{0.6, 1.0}$, Repair $\in$ \{none, \textbf{clip}, reflect, random, round\}} \\
 & \textbf{SBX} & \textit{Dist.\ Index} $\in \Realdomain{5.0, 40.0}$ \\
 & BLX-$\alpha$ & $\alpha \in \Realdomain{0.0, 1.0}$, \textit{Repair} $\in$ \{clip, random, reflect, round\} \\
 & \textcolor{red}{BLX-$\alpha\beta$} & \textcolor{red}{$\alpha \in \Realdomain{0.0, 1.0}$, $\beta \in \Realdomain{0.0, 1.0}$} \\
 & Arithmetic & -- \\
 & \textcolor{red}{WholeArithmetic} & \textcolor{red}{$\alpha \in \Realdomain{0.0, 1.0}$} \\
 & \textcolor{red}{Laplace} & \textcolor{red}{$a \in \Realdomain{-1.0, 1.0}$, $b \in \Realdomain{0.01, 2.0}$} \\
 & \textcolor{red}{Fuzzy} & \textcolor{red}{$d \in \Realdomain{0.0, 2.0}$} \\
 & PCX & $\sigma_\eta \in \Realdomain{0.01, 0.5}$, $\sigma_\zeta \in \Realdomain{0.01, 0.5}$ \\
 & UNDX & $\zeta \in \Realdomain{0.1, 1.0}$, $\eta \in \Realdomain{0.1, 1.0}$ \\
 & Simplex & $\epsilon \in \Realdomain{0.1, 1.0}$ \\ \midrule
\multirow{10}{*}{Mutation} & \multicolumn{2}{l}{\textit{Global:} Prob.\ Factor $\in \Realdomain{0.25, 3.0}$ ($p_m = \text{factor}/n$), Dist.\ Index $\in \Realdomain{5.0, 40.0}$} \\
 & \textbf{PM} & (uses global dist.\ index) \\
 & LinkedPolynomial & (uses global dist.\ index) \\
 & NonUniform & \textit{Perturbation} $\in \Realdomain{0.05, 0.5}$ \\
 & Gaussian & $\sigma \in \Realdomain{0.001, 0.5}$ \\
 & Cauchy & $\gamma \in \Realdomain{0.001, 0.5}$ \\
 & Uniform & \textit{Perturbation} $\in \Realdomain{0.01, 0.5}$ \\
 & UniformReset & -- \\
 & \textcolor{red}{L\'evyFlight} & \textcolor{red}{$\beta \in \Realdomain{0.5, 2.0}$, \textit{Scale} $\in \Realdomain{0.001, 0.1}$} \\
 & \textcolor{red}{PowerLaw} & \textcolor{red}{\textit{Index} $\in \Realdomain{0.5, 5.0}$} \\ \bottomrule
\end{tabular}%
}
\end{table*}

%==============================================================================
\section{Code Examples}
\label{app:code}
%==============================================================================

\begin{lstlisting}[float=*,caption={Minimal NSGA-II run on ZDT1 in \VAMOS{}.},label={lst:quickstart}]
from vamos import optimize
result = optimize("zdt1", algorithm="nsgaii",
                  max_evaluations=50000, seed=42)
\end{lstlisting}

\begin{lstlisting}[float=*,caption={Builder API for fine-grained configuration.},label={lst:builder}]
from vamos import optimize
from vamos.algorithms import NSGAIIConfig
cfg = (NSGAIIConfig.builder()
       .pop_size(100)
       .crossover("sbx", prob=1.0, eta=20)
       .mutation("pm", prob="1/n", eta=20)
       .build())
result = optimize("zdt1", algorithm="nsgaii",
                  algorithm_config=cfg,
                  max_evaluations=50000, seed=42)
\end{lstlisting}

\begin{lstlisting}[float=*,caption={Defining and optimizing a custom problem.},label={lst:make_problem}]
from vamos import make_problem, optimize
problem = make_problem(
    lambda x: [x[0]**2 + x[1]**2,
               (x[0]-1)**2 + (x[1]-1)**2],
    n_var=2, n_obj=2, bounds=[(0, 2), (0, 2)])
result = optimize(problem, algorithm="nsgaii",
                  max_evaluations=50000, seed=42)
\end{lstlisting}

\textcolor{red}{Listing~\ref{lst:optuna_tune} shows how to autotune the full NSGA-II parameter space (Table~\ref{tab:parameter_space}) using the Optuna backend. The \texttt{build\_nsgaii\_config\_space} function returns the 28-parameter space; \texttt{ModelBasedTuner} drives the search with TPE sampling and optional multi-fidelity evaluation.}

\begin{lstlisting}[float=*,caption={\textcolor{red}{Autotuning NSGA-II with Optuna on ZDT1.}},label={lst:optuna_tune}]
import numpy as np
from vamos import optimize
from vamos.engine.tuning import (
    ModelBasedTuner, TuningTask, Instance, EvalContext,
    build_nsgaii_config_space, config_from_assignment,
)

# Build the 28-parameter NSGA-II config space
space = build_nsgaii_config_space()

# Evaluation function: returns hypervolume
def eval_fn(config, ctx: EvalContext):
    cfg = config_from_assignment("nsgaii", config)
    res = optimize("zdt1", algorithm="nsgaii",
                   algorithm_config=cfg,
                   max_evaluations=ctx.budget, seed=ctx.seed)
    return float(res.hypervolume(ref_point=[1.1, 1.1]))

# Define tuning task
task = TuningTask(
    name="tune_nsgaii_zdt1",
    param_space=space.to_param_space(),
    instances=[Instance(name="zdt1", n_var=30)],
    seeds=[1, 2, 3], aggregator=np.mean,
    budget_per_run=10000, maximize=True,
)

# Run Optuna-based tuning (TPE + multi-fidelity)
tuner = ModelBasedTuner(
    task=task, max_trials=50, backend="optuna",
    seed=42, n_jobs=4,
    budget_levels=[2000, 5000, 10000],
)
best_config, history = tuner.run(eval_fn)
\end{lstlisting}

%==============================================================================
% Bibliography
%==============================================================================
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
