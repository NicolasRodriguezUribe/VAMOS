% VAMOS Paper: Swarm and Evolutionary Computation (Elsevier)
% -----------------------------------------------------------------------------
% NOTE: Several tables in this manuscript are auto-generated from CSV artifacts:
%   - `experiments/benchmark_paper.csv`:
%       - `paper/04_update_paper_tables_from_csv.py` updates runtime tables
%         (`tab:backends`, `tab:frameworks_perf`, `tab:detailed_backends`,
%          `tab:detailed_comparison`) and the normalized-HV summary table
%         (`tab:frameworks_hv`).
%       - `paper/14_update_frameworks_perf_variant_tables_from_csv.py` updates
%         framework-variant runtime tables (`tab:frameworks_perf_nsgaii_ss`,
%         `tab:frameworks_perf_nsgaii_archive`, `tab:frameworks_perf_smsemoa`,
%         `tab:frameworks_perf_moead`).
% -----------------------------------------------------------------------------
\documentclass[preprint,12pt]{elsarticle}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\hypersetup{hypertexnames=false}
\makeatletter
\g@addto@macro\UrlBreaks{\do\.\do\_\do\-\do\/}
\makeatother
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}

% Code listing style
\lstset{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red!70!black},
  showstringspaces=false,
  frame=single,
  breaklines=true,
  numbers=left,
  numberstyle=\tiny\color{gray}
}

\journal{Swarm and Evolutionary Computation}

\newcommand{\VAMOS}{VAMOS}
\newcommand{\HV}{\mathrm{HV}}

\begin{document}

\begin{frontmatter}

\title{VAMOS: A Vectorized Architecture for Multiobjective Optimization Studies\\A High-Performance Python Framework}

\author[URJC]{Nicol\'{a}s R. Uribe}
\ead{nicolas.rodriguez@urjc.es}

\author[URJC]{Alberto Herr\'{a}n\corref{cor1}}
\ead{alberto.herran@urjc.es}

\author[LCC,ITIS]{Antonio J. Nebro}
\ead{ajnebro@uma.es}

\author[TECNALIA,UPV]{Javier del Ser}
\ead{javier.delser@tecnalia.com}

\author[URJC]{J. Manuel Colmenar}
\ead{josemanuel.colmenar@urjc.es}

\cortext[cor1]{Corresponding author: Alberto Herr\'{a}n (alberto.herran@urjc.es)}

\address[URJC]{Dept. Computer Sciences, Universidad Rey Juan Carlos\\
C/. Tulip\'{a}n, s/n, M\'{o}stoles, 28933 (Madrid), Spain}

\address[LCC]{Dept. de Lenguajes y Ciencias de la Computaci\'{o}n, ITIS Software, University of M\'{a}laga,\\
ETSI Inform\'{a}tica, Campus de Teatinos, 29071 (M\'{a}laga), Spain}

\address[ITIS]{ITIS Software, Ada Byron Research Building, C/. Arquitecto Francisco Pe\~nalosa, 18, University of M\'{a}laga, 29071 (M\'{a}laga), Spain}

\address[TECNALIA]{TECNALIA, Basque Research \& Technology Alliance (BRTA), Derio, 48160, Spain}

\address[UPV]{Department of Mathematics, University of the Basque Country (UPV/EHU), Leioa, 48940, Spain}

\begin{abstract}
\sloppy
Python has become the de facto platform for empirical studies in multi-objective optimization, yet most multi-objective evolutionary algorithm (MOEA) libraries remain dominated by object-oriented designs that incur substantial interpreter overhead in the inner loop. We introduce \VAMOS{} (Vectorized Architecture for Multiobjective Optimization Studies), a research-oriented framework that represents populations as contiguous numerical arrays and separates algorithmic logic from numerical kernels. \VAMOS{} provides interchangeable compute backends (NumPy, Numba JIT, and optional C and JAX accelerators), a unified configuration API, and nine MOEAs (NSGA-II and NSGA-III, MOEA/\allowbreak D, SMS-EMOA, SPEA2, IBEA, SMPSO, AGE-MOEA, and RVEA). We release a reproducible cross-framework benchmarking pipeline that aligns problem definitions and operator semantics across common Python frameworks and reports wall-clock runtime and \emph{normalized} hypervolume computed with fixed reference Pareto fronts. Across the ZDT, DTLZ, and WFG suites, \VAMOS{} achieves up to an order-of-magnitude reduction in runtime relative to object-centric libraries (jMetalPy, DEAP, Platypus), with more moderate but consistent gains over pymoo, while maintaining statistically equivalent solution quality.
\end{abstract}

\begin{highlights}
\item Vectorized MOEA core with pluggable NumPy/Numba/C/JAX kernels.
\item Fair cross-framework benchmarking via semantic configuration alignment.
\item Backend and cross-framework runtime comparison across multiple MOEA variants.
\item Competitive normalized hypervolume under a fixed-reference-front protocol.
\item Reproducible pipeline with scripts and versioned artifacts.
\end{highlights}

\begin{keyword}
Multi-objective optimization \sep Evolutionary algorithms \sep Benchmarking \sep Hypervolume \sep Python framework \sep Vectorization
\end{keyword}

\end{frontmatter}

%==============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

Multi-objective optimization problems (MOPs) arise whenever trade-offs exist between conflicting objectives, requiring the computation of a diverse approximation of the Pareto-optimal set rather than a single optimum~\cite{coello2007evolutionary}. Multi-objective evolutionary algorithms (MOEAs) remain a dominant approach due to their robustness, anytime behavior, and ability to return sets of non-dominated solutions.

In Python, practical benchmarking of MOEAs is often constrained by two friction points. First, the ecosystem is fragmented: libraries differ in problem APIs, operator configuration, result formats, and runtime instrumentation~\cite{blank2020pymoo,benitez2019jmetalpy,fortin2012deap}. Second, many implementations rely on per-solution objects and Python-level loops inside selection, variation, and survival, making the overhead of the interpreter comparable to (or larger than) the cost of typical benchmark functions. This limitation becomes acute for large-scale studies with many seeds, large populations, and repeated indicator computation and reporting.

\VAMOS{} is designed to address both issues with a single guiding principle: \emph{keep algorithm state in dense arrays and push hot loops into vectorized kernels}. The framework also targets reproducibility in experimental methodology: we provide a benchmark runner that standardizes problem dimensions and operators across multiple Python frameworks, and a hypervolume protocol with fixed reference Pareto fronts so that solution-quality metrics are comparable across runs and frameworks. The contributions of this work are:
\begin{enumerate}
  \item \textbf{Vectorized MOEA core}: a unified, array-based internal representation with pluggable compute kernels (NumPy/Numba/C/JAX) for selection, variation, and survival.
  \item \textbf{Modular algorithm suite}: nine MOEAs implemented on top of shared components (termination, archives, evaluation backends, and metrics).
  \item \textbf{Cross-framework benchmarking pipeline}: a reproducible runner that enforces semantic alignment of problems and operator settings across common Python frameworks and computes \emph{normalized} hypervolume against fixed reference Pareto fronts distributed with the repository.
  \item \textbf{Comprehensive empirical comparison}: backend-level and cross-framework runtime comparisons, paired with descriptive normalized-hypervolume summaries under a fixed protocol.
\end{enumerate}

The rest of the paper is organized as follows. Section~\ref{sec:related} reviews existing Python MOEA frameworks and benchmarking practices. Section~\ref{sec:framework} presents the \VAMOS{} framework, including its design goals, architecture, compute kernels, and algorithm suite. Section~\ref{sec:experiments} describes the experimental setup, reports backend-level and cross-framework runtime comparisons, and provides solution-quality summaries with statistical analyses. Finally, Section~\ref{sec:conclusions} draws conclusions and outlines directions for future work.

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

We review the Python MOEA landscape along two axes: the frameworks themselves (Section~\ref{sec:related_frameworks}) and the methodological challenges of cross-framework benchmarking (Section~\ref{sec:related_benchmarking}).

\subsection{Python MOEA frameworks}
\label{sec:related_frameworks}

Several mature Python libraries exist for MOEAs. \textbf{pymoo}~\cite{blank2020pymoo} provides a rich collection of algorithms and operators with an emphasis on modularity. \textbf{jMetalPy}~\cite{benitez2019jmetalpy} ports the jMetal architecture to Python and emphasizes end-to-end experimentation (observer-driven progress/visualization, a ``laboratory'' workflow, and built-in statistical post-processing with LaTeX export). It additionally offers modules for constrained, dynamic, and preference-based variants, as well as parallel evaluation via Spark/Dask. However, it relies on object-centric representations and does not address cross-toolkit semantic drift. We build upon this methodological focus on rigorous experimental studies and complement it with an array-based execution model and a fairness-oriented benchmarking protocol that aligns problem definitions and operator semantics across toolkits. \textbf{DEAP}~\cite{fortin2012deap} is a general evolutionary computation toolkit; while flexible, users typically assemble MOEAs manually. \textbf{Platypus}~\cite{hadka2015platypus} provides a lightweight API and a set of classic MOEAs.

Table~\ref{tab:frameworks} summarizes high-level differences relevant to this work. The main distinction is the internal representation: \VAMOS{} consistently uses dense arrays for populations and objectives, while most alternatives operate on per-solution objects. This difference enables kernel-based acceleration and reduces Python overhead in the inner loop.

\begin{table}[htbp]
\centering
\caption{Comparison of Python multi-objective optimization frameworks (high-level capabilities).}
\label{tab:frameworks}
{\scriptsize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lccc}
\toprule
\textbf{Framework} & \textbf{MOEAs} & \textbf{Array core} & \textbf{Accel. kernels} \\
\midrule
\VAMOS{} & 9 & Yes & Numba/C/JAX \\
pymoo~\cite{blank2020pymoo} (0.6) & 8+ & Partial & No \\
jMetalPy~\cite{benitez2019jmetalpy} (2019) & 11+ & No & No \\
DEAP~\cite{fortin2012deap} (1.4) & Custom & No & No \\
Platypus~\cite{hadka2015platypus} & 8+ & No & No \\
\bottomrule
\end{tabular}
}
\end{table}

Figure~\ref{fig:representation} provides an intuitive view of how these frameworks represent and manipulate a population. In \VAMOS{}, the population state is stored in dense arrays ($X \in \mathbb{R}^{N\times n}$ and $F \in \mathbb{R}^{N\times m}$), and selection/variation/survival primarily operate by computing index arrays and applying array kernels. In contrast, pymoo and jMetalPy typically manage the population as a collection of solution objects; array computations may still be used internally, but more of the algorithmic control flow involves object/container manipulation.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.98\textwidth]{figures/Representation.png}
  \caption{Comparing population representation across Python MOEA frameworks. \VAMOS{} (left) stores the full population in dense arrays ($X$, $F$) and applies operators via array-oriented kernels (NumPy/Numba/C/JAX). pymoo (center) uses a population container of individual objects backed by NumPy arrays, mixing array computation with object overhead. jMetalPy (right) manages a list of solution objects with per-solution method calls. All three implement the same selection--variation--survival loop, but differ in internal data model and hot-loop granularity.}
  \label{fig:representation}
\end{figure}

\subsection{Benchmarking and indicator reporting}
\label{sec:related_benchmarking}

Comparative MOEA studies typically combine independent runs, indicator computation, and structured post-processing (e.g., summary tables and plots). However, cross-framework results are sensitive to hidden defaults and semantic mismatches: benchmark implementations may differ in bounds or parameterizations, and operators may interpret probabilities at different levels (e.g., per-individual vs.\ per-variable mutation). Toolkits such as jMetalPy~\cite{benitez2019jmetalpy} provide laboratory-style reporting and post-processing, but they do not by themselves resolve cross-toolkit semantic drift. Our work complements these methodological efforts by making semantic alignment an explicit first-class step and by reporting normalized hypervolume under a fixed reference-front protocol, enabling more reliable cross-framework comparisons.

%==============================================================================
\section{VAMOS Framework}
\label{sec:framework}
%==============================================================================

This section presents the \VAMOS{} framework. We first state the design goals that guided its development (Section~\ref{sec:design_goals}), then describe the architecture and data model, compute kernels, algorithm suite, execution model, variation pipeline, archives and metrics, and the benchmark runner.

\subsection{Design goals}
\label{sec:design_goals}

The design of \VAMOS{} is motivated by three practical gaps identified in existing Python MOEA libraries.

\paragraph{Minimal-boilerplate API.}
Most frameworks require importing separate modules for the algorithm, problem, operators, and termination, then wiring them together (typically 10--15 lines for a standard NSGA-II run on a benchmark problem). \VAMOS{} exposes a single entry point that handles defaults, operator configuration, and termination:

\begin{lstlisting}[caption={Minimal NSGA-II run on ZDT1 in \VAMOS{}.},label={lst:quickstart}]
from vamos import optimize
result = optimize("zdt1", algorithm="nsgaii",
                  max_evaluations=50000, seed=42)
\end{lstlisting}

\noindent For comparison, the equivalent run in pymoo requires separate imports for the algorithm class, problem factory, crossover, and mutation operators, explicit operator construction, and a \texttt{minimize} wrapper (approximately 10 lines); jMetalPy additionally requires a termination-criterion object and explicit variable/objective counts (approximately 15 lines). When finer control is needed, \VAMOS{} provides a builder API for algorithm configuration:

\begin{lstlisting}[caption={Builder API for fine-grained configuration.},label={lst:builder}]
from vamos import optimize
from vamos.algorithms import NSGAIIConfig
cfg = (NSGAIIConfig.builder()
       .pop_size(100)
       .crossover("sbx", prob=1.0, eta=20)
       .mutation("pm", prob="1/n", eta=20)
       .build())
result = optimize("zdt1", algorithm="nsgaii",
                  algorithm_config=cfg,
                  max_evaluations=50000, seed=42)
\end{lstlisting}

\paragraph{Pluggable computation.}
Switching the compute backend is a single-parameter change (\texttt{engine="numba"}, \texttt{"numpy"}, etc.) that is transparent to algorithm code. No other Python MOEA framework allows swapping the entire computational substrate---from array operations to compiled kernels---without modifying the algorithm implementation. Similarly, evaluation strategies (serial, multiprocessing, or Dask-based distributed evaluation) are selected via configuration rather than code changes, and multi-seed studies are first-class: passing a list of seeds (\texttt{seed=[0,1,2]}) triggers automatic multi-run execution and returns a list of results. \VAMOS{} also provides automatic algorithm selection (\texttt{algorithm="auto"}) that maps problem traits (number of objectives, encoding type) to a suitable default algorithm, lowering the barrier for users unfamiliar with the MOEA landscape.

\paragraph{Array-native performance.}
The architectural consequence of the preceding goals is that the population state remains in dense arrays throughout the optimization loop, and hot-path computations are dispatched to vectorized or compiled kernels. Section~\ref{sec:experiments} demonstrates that this design yields substantial runtime reductions across multiple MOEA variants and benchmark families without degrading solution quality.

\subsection{Architecture and data model}

\VAMOS{} is organized into three layers:
\begin{enumerate}
  \item \textbf{Foundation}: problem definitions, kernels, metrics, and archives;
  \item \textbf{Engine}: algorithm implementations and shared components;
  \item \textbf{Experiment}: CLI, benchmarking utilities, visualization, and reporting.
\end{enumerate}

The user-facing API and configuration model are described in Section~\ref{sec:design_goals}. The core design choice is to represent a population as a pair of dense arrays: decision variables $X \in \mathbb{R}^{N \times n}$ and objective values $F \in \mathbb{R}^{N \times m}$ for population size $N$, $n$ decision variables, and $m$ objectives. Variation and survival operate on these arrays (or views thereof), enabling vectorized operations (NumPy) and compilation of hot loops (Numba) without per-individual Python objects.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/architecture.png}
\caption{\VAMOS{} architecture.}
\label{fig:architecture}
\end{figure}

\subsection{Compute kernels}

Algorithmic logic (e.g., \emph{NSGA-II selects survivors by non-dominated sorting and crowding distance}) is separated from numerical kernels that implement the corresponding computations. \VAMOS{} provides multiple backends:
\begin{itemize}
  \item \textbf{NumPy}: baseline backend using vectorized array operations.
  \item \textbf{Numba}: JIT compilation of hot operators and utilities, reducing Python overhead~\cite{lam2015numba}.
  \item \textbf{MooCore}: optional C-backed kernels for Pareto ranking and hypervolume-based utilities (via MooCore)~\cite{moocore}.
  \item \textbf{JAX}: optional backend for parts of the pipeline and autodiff-based constraints, enabling GPU/TPU acceleration when available~\cite{jax2018github}.
\end{itemize}

Backends are selected through configuration (e.g., \texttt{engine="numba"}) and are transparent to algorithm code.

In addition to accelerating arithmetic kernels, \VAMOS{} reduces overhead by minimizing Python-side allocations: variation operators can reuse pre-allocated workspaces (e.g., scratch buffers for SBX/PM) and algorithms maintain state as contiguous arrays. This design is particularly beneficial in inner-loop routines such as tournament selection, non-dominated sorting, crowding-distance computation, and archive updates.

\subsection{Algorithm suite}

\VAMOS{} implements nine multi-objective algorithms covering dominance-based, decomposition-based, indicator-based, and reference-vector paradigms. Table~\ref{tab:algorithms} summarizes the suite.

\begin{table}[htbp]
\centering
\caption{Multi-objective algorithms implemented in \VAMOS{}.}
\label{tab:algorithms}
\begin{tabular}{llc}
\toprule
\textbf{Algorithm} & \textbf{Category} & \textbf{Reference} \\
\midrule
NSGA-II & Dominance-based & \cite{deb2002fast} \\
NSGA-III & Reference-point & \cite{deb2014evolutionary} \\
MOEA/D & Decomposition & \cite{zhang2007moea} \\
SMS-EMOA & Indicator-based & \cite{beume2007sms} \\
SPEA2 & Archive-based & \cite{zitzler2001spea2} \\
IBEA & Indicator-based & \cite{zitzler2004indicator} \\
SMPSO & Swarm-based & \cite{nebro2009smpso} \\
AGE-MOEA & Adaptive geometry & \cite{panichella2019adaptive} \\
RVEA & Reference vectors & \cite{cheng2016reference} \\
\bottomrule
\end{tabular}
\end{table}

AGE-MOEA follows the adaptive geometry estimation survival scheme of Panichella~\cite{panichella2019adaptive}: after non-dominated sorting, the first front is normalized by extreme points, a curvature-dependent $p$-norm is estimated, and survival uses geometry-aware crowding based on Minkowski distances. RVEA follows Cheng et~al.~\cite{cheng2016reference} with angle-penalized distance (APD) survival and periodic reference-vector adaptation using ideal/nadir scaling; we therefore enforce \texttt{pop\_size} to match the number of simplex-lattice reference directions induced by \texttt{n\_partitions}.

\subsection{Execution model and evaluation backends}

Algorithms in \VAMOS{} expose a uniform \texttt{run(problem, termination, seed, eval\_backend, live\_viz)} interface, and several algorithms also support an \emph{ask--tell} loop for streaming or interactive use. Objective evaluations are dispatched through a small evaluation-backend protocol, enabling serial evaluation for controlled benchmarking and parallel evaluation (e.g., multiprocessing or Dask-based) when objective functions are expensive. Separating evaluation from algorithmic state keeps the algorithm core deterministic under a fixed random seed and reduces the risk of framework-specific parallelism becoming a confounder in empirical comparisons.

Algorithm~\ref{alg:nsgaii} highlights how the canonical NSGA-II loop is expressed in \VAMOS{} over dense array state. The population is represented as a decision matrix $X \in \mathbb{R}^{N\times n}$ and objective matrix $F \in \mathbb{R}^{N\times m}$; selection produces an index vector $P$; variation applies array-oriented operators to $X_P$ to generate offspring $X'$; and survival selects survivors by computing ranks/crowding on concatenated arrays and returning survivor indices. This index-and-kernel execution avoids per-solution Python objects in the hot loop and enables backend swapping (NumPy/Numba/C/JAX), while keeping the underlying algorithm semantics unchanged. Figure~\ref{fig:representation} contrasts this array-centric representation with object-centric populations in other Python frameworks.

\begin{algorithm}[htbp]
\caption{Vectorized NSGA-II execution in \VAMOS{} (high-level).}
\label{alg:nsgaii}
\begin{algorithmic}[1]
\State Sample initial population $X \in \mathbb{R}^{N \times n}$ within bounds
\State Evaluate objectives $F \gets f(X)$
\State Compute initial ranks and crowding distances
\While{termination criterion not met}
  \State Select mating indices $P$ via tournament on (rank, crowding)
  \State Generate offspring $X' \gets \mathcal{V}(X_P)$ (crossover, mutation, repair)
  \State Evaluate offspring $F' \gets f(X')$
  \State Merge: $(X,F) \gets ([X;X'], [F;F'])$
  \State Compute ranks and crowding distances
  \State Select survivors of size $N$ (rank--crowding)
\EndWhile
\State \Return final population and derived non-dominated set
\end{algorithmic}
\end{algorithm}

\subsection{Variation pipeline and encodings}

\VAMOS{} supports real-valued, binary, and permutation encodings through a variation pipeline that composes selection, crossover, mutation, and optional repair operators. For continuous domains, the default configuration uses simulated binary crossover (SBX) and polynomial mutation (PM), with mutation probability typically set to $1/n$ where $n$ is the number of decision variables. Encodings are normalized internally to ensure consistent operator resolution and avoid silent configuration mismatches.

\subsection{Archives, metrics, and analysis tooling}

The framework includes optional external archives (with configurable pruning strategies) and common indicators. Hypervolume computation can use optimized backends when installed, but \VAMOS{} also provides fallback implementations for low-dimensional cases. Results are returned in a unified result object that supports plotting and post-hoc analysis (e.g., filtering non-dominated solutions and exporting fronts).

\subsection{Benchmark runner and reporting}

The experiment layer includes a benchmark runner that executes standardized studies under a shared protocol (fixed problem definitions, evaluation budgets, and seeds) and emits machine-readable artifacts (per-seed results and summary CSVs). Table generation scripts produce LaTeX-ready tables directly from these artifacts, reducing transcription errors and enabling end-to-end reproducibility.

%==============================================================================
\section{Experimental Evaluation}
\label{sec:experiments}
%==============================================================================

We evaluate \VAMOS{} along two dimensions: runtime efficiency across backends and frameworks, and solution quality measured by normalized hypervolume under a fixed-reference-front protocol. We first describe the benchmark suite and the semantic-alignment procedure that ensures cross-framework fairness, then report backend-level and cross-framework runtime comparisons, solution-quality summaries, and statistical analyses.

\subsection{Benchmark suite and configuration alignment}

We evaluate runtime and solution quality on widely used synthetic benchmarks: ZDT1--4 and ZDT6~\cite{zitzler2000zdt} (two objectives), DTLZ1--7~\cite{deb2002dtlz} (three objectives), and WFG1--9~\cite{huband2006wfg} (two objectives). Problem dimensionalities are fixed to standard definitions: for example, DTLZ2/3/4 use $n = m + k - 1$ with $m=3$ and $k=10$ (thus $n=12$). Non-standard DTLZ dimensionalities are permitted but explicitly flagged to avoid accidental comparisons against canonical settings.
Unless otherwise stated, each (problem, framework) configuration is executed for 50{,}000 objective evaluations with 30 independent seeds.

Figure~\ref{fig:protocol} summarizes the benchmark protocol and semantic-alignment workflow used to ensure cross-framework fairness and to support the ``same quality, faster'' claim.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.98\textwidth]{figures/protocol.png}
  \caption{Benchmark protocol and semantic alignment: map equivalent NSGA-II settings across frameworks, validate objective definitions by sampling decision vectors within bounds, run independent seeds under a fixed evaluation budget, and compute normalized hypervolume using fixed reference fronts.}
  \label{fig:protocol}
\end{figure}

Table~\ref{tab:alignment_checklist} summarizes the key semantic-alignment constraints enforced in our benchmarking pipeline.

\begin{table}[htbp]
\centering
\caption{Semantic alignment checklist for cross-framework benchmarking (NSGA-II baseline).}
\label{tab:alignment_checklist}
{\scriptsize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{p{0.28\linewidth}p{0.64\linewidth}}
\toprule
\textbf{Item} & \textbf{Enforced alignment} \\
\midrule
Benchmark definitions & Shared bounds/parameterizations; validated via random decision-vector sampling across implementations. \\
Budget & Fixed number of objective evaluations (50{,}000 per run; same across frameworks). \\
Operators & SBX ($p_c{=}1.0$, $\eta_c{=}20$) and polynomial mutation ($\eta_m{=}20$) with per-variable semantics ($p_m{=}1/n$), mapped explicitly when APIs differ. \\
Selection / survival & Tournament selection; rank--crowding survival (non-dominated sorting + crowding distance). \\
Timing & Wall-clock time (median over seeds); Numba warm timings exclude one-time JIT compilation via a short warmup run. \\
Quality metric & Normalized hypervolume on the final non-dominated set using fixed reference fronts and a fixed reference point. \\
\bottomrule
\end{tabular}
}
\end{table}

Although \VAMOS{} implements nine MOEAs, our cross-framework comparison focuses on algorithms with the closest semantic matches across libraries: NSGA-II (baseline plus steady-state and archive variants), SMS-EMOA, and MOEA/D. This scope avoids confounding effects from missing implementations or materially different operator and termination semantics in specific toolkits.

To ensure comparability across frameworks, we standardize the algorithmic settings of NSGA-II: population size $N=100$, SBX crossover probability $p_c=1.0$ and distribution index $\eta_c=20$, polynomial mutation distribution index $\eta_m=20$ and mutation probability $p_m=1/n$, tournament selection, and rank--crowding survival. For each framework we map these settings to the closest available implementation.

For SMS-EMOA we follow the same protocol and align what we can: population size $N=100$, SBX ($p_c=1.0$, $\eta_c=20$), polynomial mutation ($p_m=1/n$, $\eta_m=20$), random parent selection, and a steady-state $(\mu+1)$ loop (one offspring per iteration). However, the hypervolume-contribution reference point is not defined consistently across toolkits: \VAMOS{} and jMetalPy compute contributions in raw objective space with an adaptive reference point $r = \max(F) + 1$, while pymoo normalizes objectives and uses a fixed shifted reference point $r = \mathbf{1} + \epsilon$ (we set $\epsilon=1$). We therefore report the cross-framework SMS-EMOA comparison with this caveat.

For MOEA/D we align population size $N=100$, weight vectors (uniform for two objectives; a shared $W3D\_100$ set for three objectives), neighborhood size $T=20$, neighborhood selection probability $\delta=0.9$, and replacement limit $\eta=20$ (effectively unrestricted, matching pymoo's update rule). We use PBI scalarization with $\theta=5$ and a DE/current/1/bin-style crossover ($CR=1.0$, $F=0.5$) followed by polynomial mutation ($p_m=1/n$, $\eta_m=20$), mapping these settings to the closest available implementation in each toolkit.

This workflow follows standard practice in comparative MOEA studies (independent runs, indicator computation, and structured reporting) and is consistent with the experimentation methodology described in jMetalPy~\cite{benitez2019jmetalpy}. Our main extension is to make the \emph{inputs} to this workflow comparable across toolkits by enforcing semantic alignment of both problem definitions and operator probabilities.

Crucially, we align \emph{semantics} (not only parameter names) and enforce consistent benchmark definitions. Two examples illustrate why this matters. First, in \textbf{pymoo} the polynomial mutation operator exposes both an individual-level probability (\texttt{prob}) and a per-variable probability (\texttt{prob\_var}); to match the standard ``$p_m=1/n$ per decision variable'' setting used by jMetalPy and DEAP, we set \texttt{prob}=1 and \texttt{prob\_var}=1/n. Second, some libraries ship benchmark problems under canonical names but with library-specific domains; for instance, Platypus' built-in ZDT base class uses $[0,1]^n$, whereas ZDT4 is defined with $x_1\in[0,1]$ and $x_{2..n}\in[-5,5]$.

Therefore, for toolkits or APIs where problem definitions differ (notably DEAP and Platypus for ZDT4), we evaluate individuals using a shared reference implementation of the benchmark function and bounds (validated against pymoo) while still using each framework's NSGA-II implementation and operators. For WFG we adopt pymoo's canonical parameterization for two objectives ($k=4$, $l=20$ for $n=24$) and pass parameters explicitly when a toolkit exposes different defaults; in Platypus we wrap pymoo's WFG definitions to avoid definition drift. While full equivalence cannot be guaranteed due to framework-specific details (tie-breaking, boundary handling, duplicate elimination, etc.), these choices aim to make the comparison as fair as possible by removing confounders unrelated to algorithmic overhead and numerical kernels.

As an additional validity check, we sample 64 random decision vectors per problem uniformly within the specified bounds and verify that objective values match across implementations within a numerical tolerance ($\mathrm{rtol}=10^{-6}$, $\mathrm{atol}=10^{-8}$). This guards against silent benchmark-definition drift (e.g., differing domains or parameterizations) that could invalidate hypervolume comparisons.

\subsection{Runtime measurement}

All runtimes are measured wall-clock using high-resolution timers and include algorithm overhead and objective evaluations. Unless otherwise stated, we use a fixed budget of 50{,}000 objective evaluations per run and report medians across 30 independent seeds, with per-problem and per-family summaries.

For Numba-accelerated backends, we distinguish two timing policies. \textbf{Warm} timings exclude one-time JIT compilation by performing a short warmup run in the same process before starting the timer (2{,}000 warmup evaluations; not counted in the reported time). \textbf{Cold} timings measure the first run from a fresh process and therefore include JIT compilation overhead. Unless otherwise stated, we report warm timings to reflect steady-state throughput; cold-start costs can be reproduced with the provided scripts.

Experiments were run on a workstation with an Intel(R) Core(TM) Ultra 9 185H CPU (16 cores, 22 logical processors) and 32\,GB RAM, running Microsoft Windows 11 Home (build 26200). We used Python 3.12.3 and evaluated \VAMOS{} 0.1.0 (NumPy 2.3.5 with OpenBLAS 0.3.30; Numba 0.63.1) against pymoo 0.6.1.6, jMetalPy (local source checkout at commit \texttt{fecb85c}, accessed January 2026), DEAP 1.4.3, and Platypus (local source checkout at commit \texttt{fe7aeff}, accessed January 2026).

\subsection{Normalized hypervolume protocol}
\label{sec:hv}

Hypervolume (HV), denoted $\HV(A,r)$, measures the Lebesgue measure of the region dominated by an approximation set $A$ with respect to a reference point $r$ (for minimization)~\cite{zitzler2003performance}.
Because HV is scale-dependent and sensitive to the choice of $r$, naive implementations can produce values that are not comparable across runs (e.g., if $r$ is expanded per run) and can be misleading when dominated solutions are included. To ensure a stable and comparable quality metric, we adopt the following protocol:
\begin{enumerate}
  \item \textbf{Non-dominated filtering}: for every framework we compute HV on the final non-dominated set only.
  \item \textbf{Fixed reference Pareto fronts}: for each benchmark problem we store a dense reference Pareto front $P_{\text{ref}}$ in \texttt{data/reference\_fronts/}. For ZDT and DTLZ (except DTLZ7) these fronts are generated analytically; for DTLZ7 and WFG we generate $P_{\text{ref}}$ by dense sampling followed by non-dominated filtering.
  \item \textbf{Fixed reference point}: we set $r = \max(P_{\text{ref}}) + \epsilon$ (component-wise) with a small $\epsilon$, and we disallow run-dependent expansion.
  \item \textbf{Normalization}: we report $\HV(A,r) / \HV(P_{\text{ref}},r)$, yielding a dimensionless score typically in $[0,1]$ and reducing sensitivity to objective scaling.
\end{enumerate}
For WFG problems, the reference front is necessarily an approximation; we use large Pareto-set samples and non-dominated filtering, and we increase density for particularly sensitive cases (e.g., WFG2) to avoid underestimating $\HV(P_{\text{ref}}, r)$ and obtaining normalized HV slightly above~1.

\subsection{VAMOS backend comparison}

Table~\ref{tab:backends} reports median runtime for \VAMOS{} backends aggregated by problem family.

\begin{table}[htbp]
\centering
\caption{VAMOS backend comparison: median runtime (seconds) by problem family.\protect\footnotemark}
\label{tab:backends}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Backend} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Average\textsuperscript{*}} \\
\midrule
Numba & \textbf{2.62} & \textbf{3.46} & \textbf{4.03} & \textbf{3.37} \\
MooCore & 2.69 & 3.52 & 4.16 & 3.45 \\
NumPy & 5.18 & 6.82 & 7.37 & 6.46 \\
\bottomrule
\end{tabular}
\end{table}
\footnotetext{Average\textsuperscript{*} in summary tables denotes the mean of family-level medians (3 values). Per-problem averages over all 21 problems are reported in the appendix (Tables~\ref{tab:detailed_backends} and~\ref{tab:detailed_comparison}).}

\subsection{Cross-framework comparison}

Table~\ref{tab:frameworks_perf} compares \VAMOS{} against other Python frameworks on runtime. \VAMOS{} is evaluated with its accelerated backend (Numba) for the headline comparison, while additional backends are reported in the appendix.
We repeat the same runtime protocol for NSGA-II variants (steady-state and external archive), SMS-EMOA (steady-state, one offspring per iteration), and MOEA/D (decomposition with differential-evolution variation), reported in Tables~\ref{tab:frameworks_perf_nsgaii_ss} and~\ref{tab:frameworks_perf_nsgaii_archive} and Tables~\ref{tab:frameworks_perf_smsemoa} and~\ref{tab:frameworks_perf_moead}.

\begin{table}[htbp]
\centering
\caption{NSGA-II (generational). Median runtime (seconds) by problem family across all frameworks.}
\label{tab:frameworks_perf}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Framework} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Average\textsuperscript{*}} \\
\midrule
VAMOS & \textbf{2.62} & \textbf{3.46} & \textbf{4.03} & \textbf{3.37} \\
pymoo & 3.22 & 3.86 & 4.56 & 3.88 \\
jMetalPy & 11.55 & 14.91 & 43.94 & 23.47 \\
DEAP & 14.99 & 21.42 & 43.72 & 26.71 \\
Platypus & 20.28 & 29.01 & 50.16 & 33.15 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Steady-state NSGA-II.}
Some toolkits expose a steady-state (incremental-replacement) variant of NSGA-II, typically implemented as a $(\mu+1)$ loop with one offspring per iteration and one survivor replaced.
We benchmark this mode by aligning NSGA-II settings across frameworks and setting the offspring batch size to~1.
In \VAMOS{}, we configure steady-state mode with \texttt{offspring\_size}=1 and \texttt{replacement\_size}=1; in pymoo we set \texttt{n\_offsprings}=1; in jMetalPy we set \texttt{offspring\_population\_size}=1; and for DEAP we implement the corresponding one-offspring loop around \texttt{selNSGA2}.
Platypus does not provide an equivalent steady-state NSGA-II API, so we omit it from this comparison.

\input{frameworks_perf_nsgaii_ss.tex}

\paragraph{NSGA-II with external archive.}
To assess the overhead of maintaining an external elite set, we also benchmark NSGA-II with an \emph{unbounded} external archive that accumulates non-dominated solutions throughout the run.
For this variant, we compute normalized hypervolume on the archive contents (rather than the final population) to reflect the larger retained approximation set.
In \VAMOS{}, we enable an external archive with \texttt{unbounded=True} and size initialized to the population size; in pymoo we maintain a separate unbounded archive updated from the evolving population/offspring; in jMetalPy we use an NSGA-II implementation with an archive hook; for DEAP we maintain an equivalent separate unbounded archive; and in Platypus we pass an \texttt{Archive()} object.

\input{frameworks_perf_nsgaii_archive.tex}

\input{frameworks_perf_smsemoa.tex}
\input{frameworks_perf_moead.tex}

\subsection{Solution quality summary}

Table~\ref{tab:frameworks_hv} summarizes normalized hypervolume for the NSGA-II baseline across frameworks, aggregated by benchmark family. We report per-problem medians across seeds, and summarize these per-problem medians using median and interquartile range (IQR) within each family, yielding a descriptive view of solution quality under the fixed normalization protocol (Section~\ref{sec:hv}).

\begin{table}[htbp]
\centering
\caption{NSGA-II. Normalized hypervolume summary (median (IQR)) by problem family across frameworks.}
\label{tab:frameworks_hv}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Framework} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Overall\textsuperscript{*}} \\
\midrule
VAMOS & \textbf{0.990 (0.008)} & 0.799 (0.153) & \textbf{0.934 (0.140)} & \textbf{0.934 (0.181)} \\
pymoo & \textbf{0.990 (0.008)} & 0.792 (0.180) & 0.846 (0.222) & 0.914 (0.192) \\
jMetalPy & \textbf{0.990 (0.009)} & 0.797 (0.155) & 0.846 (0.224) & 0.915 (0.192) \\
DEAP & \textbf{0.990 (0.008)} & \textbf{0.802 (0.170)} & 0.839 (0.226) & 0.917 (0.186) \\
Platypus & \textbf{0.990 (0.008)} & 0.786 (0.209) & 0.920 (0.139) & 0.920 (0.196) \\
\bottomrule
\end{tabular}
\end{table}

Across ZDT, all frameworks attain nearly identical normalized HV, suggesting that benchmark definitions and operator semantics are effectively aligned. For DTLZ, medians are tightly clustered across frameworks, while WFG exhibits the largest dispersion; \VAMOS{} achieves the highest median on WFG and overall. Together with the runtime results, these summaries support improved throughput without degraded final quality under the fixed-reference-front protocol.

\subsection{Statistical analysis}
\label{sec:stats}

To move beyond descriptive summaries, we apply Wilcoxon signed-rank tests (paired by seed) comparing VAMOS (Numba) against each competitor on every problem, with Holm step-down correction for familywise error control ($\alpha=0.05$). We report the Vargha--Delaney $\hat{A}_{12}$ effect size~\cite{vargha2000critique} alongside each test ($\hat{A}_{12}>0.5$ favors VAMOS; thresholds: negligible${}<0.56$, small${}<0.64$, medium${}<0.71$, large${\ge}0.71$). We also report paired-bootstrap 90\% confidence intervals for the relative normalized-HV difference $\Delta = (\HV_{\text{VAMOS}} - \HV_{\text{fw}})/\HV_{\text{fw}}$, and we declare \emph{equivalence} when the entire CI falls within $\pm 1\%$.

Table~\ref{tab:stats_hypervolume_nsgaii} presents the per-problem Wilcoxon results for the NSGA-II baseline. Of 21 problems, only one (ZDT6) shows a statistically significant difference in normalized HV after Holm correction, and the magnitude is small ($\Delta \approx 0.3\%$). Table~\ref{tab:hv_equivalence_summary_nsgaii} summarizes the equivalence analysis across all frameworks: VAMOS is equivalent to pymoo on 14/21 problems and non-inferior on 16/21, with a median $\Delta$HV of $-0.03\%$. Similar patterns hold for jMetalPy, DEAP, and Platypus. Tables~\ref{tab:hv_robustness_summary_nsgaii} and~\ref{tab:hv_equivalence_ci_nsgaii} detail robustness and per-problem CIs. Corresponding analyses for SMS-EMOA and MOEA/D are provided in Appendix~\ref{app:stats}.

\input{stats_nsgaii.tex}

\subsection{Discussion}

The observed speedups are primarily due to (i) the array-based representation that removes per-solution Python overhead, and (ii) JIT compilation of hot operators and utilities in the Numba backend. Table~\ref{tab:frameworks_hv} summarizes that these performance gains are not obtained at the expense of final solution quality under the fixed-reference-front normalized-HV protocol, avoiding artifacts caused by run-dependent reference points.

The magnitude of the speedup varies across algorithm variants in a pattern consistent with this overhead model. Steady-state variants (Tables~\ref{tab:frameworks_perf_nsgaii_ss} and~\ref{tab:frameworks_perf_smsemoa}) show the largest relative speedups because they perform one selection--variation--survival cycle per offspring: frameworks with high per-iteration Python overhead incur that cost 50{,}000 times (one per evaluation) rather than 500 times (one per generation of 100 offspring). \VAMOS{}'s array-kernel dispatch amortizes this overhead more effectively, widening the gap.

Not all comparisons favor \VAMOS{}. For MOEA/D on WFG problems (Table~\ref{tab:frameworks_perf_moead}), jMetalPy achieves lower median runtime (8.34\,s vs.\ 11.12\,s). This is attributable to differences in the decomposition inner loop: jMetalPy's MOEA/D processes one subproblem at a time with lightweight scalar operations, a pattern where Python-level overhead is modest and \VAMOS{}'s array-batching strategy does not yield the same advantage as in generational algorithms with population-wide survival.

These speedups have direct practical implications. A 4--10$\times$ reduction in per-run wall-clock time means that the same compute budget can accommodate proportionally more independent seeds, larger population sizes, or denser hyperparameter sweeps---precisely the conditions required for statistically rigorous benchmarking studies.

We also note that both \VAMOS{} and pymoo obtain zero normalized hypervolume on DTLZ3 under MOEA/D with 50{,}000 evaluations. This is a known difficulty: DTLZ3 is a multimodal problem designed to test convergence, and MOEA/D with PBI scalarization and the evaluation budget used here is insufficient to escape local fronts consistently. This outcome is not framework-specific but reflects the interaction between algorithm configuration and problem difficulty.

A related observation concerns SMS-EMOA on DTLZ4 (Appendix~\ref{app:stats}): \VAMOS{} attains a median normalized HV of 0.898 versus 0.671 for pymoo, yet the Wilcoxon test reports $p=1.0$ after Holm correction. This reflects very high variance across seeds due to the deceptive density bias in DTLZ4, which causes many seeds to converge to a narrow region of the front. The high IQR makes the paired differences too noisy for the test to reject the null hypothesis despite the large median gap.

\subsection{Threats to validity}

Despite careful alignment of dimensions, budgets, problem definitions, and operator settings, cross-framework comparisons remain subject to several threats: (i) operator implementations may differ in subtle details (e.g., boundary handling, duplicate elimination, tie-breaking in survival), (ii) random number generation and seeding semantics vary across libraries, and (iii) library-specific overheads (data conversion, object materialization) may affect measured runtime. We mitigate major sources of unfairness by enforcing consistent benchmark bounds (e.g., ZDT4) and matching operator semantics where APIs differ (e.g., mutation probability in pymoo).

Our cross-framework comparison is deliberately scoped to Python MOEA libraries whose inner loops are predominantly executed in Python. Libraries such as PyGMO (Python bindings to the C++ PaGMO2 library) execute algorithms and benchmark functions fully in compiled code, yielding fundamentally different runtime profiles; we therefore treat them as an orthogonal C++ baseline and leave their inclusion to future work with an explicit language/runtime study design.

For hypervolume, reference fronts for problems without closed-form Pareto fronts are necessarily approximations; while we generate them densely and fix them across runs, remaining discrepancies can affect absolute normalized values, especially on WFG problems. We therefore treat normalized HV primarily as a comparative metric under a fixed protocol, and we report distributions across multiple seeds rather than relying on single-run outcomes.

%==============================================================================
\section{Conclusions and Future Work}
\label{sec:conclusions}
%==============================================================================

We presented \VAMOS{}, a high-performance Python framework for multi-objective optimization studies. By combining a vectorized core with pluggable compute kernels, \VAMOS{} reduces inner-loop overhead and enables scalable benchmarking. We also release a reproducible cross-framework benchmarking pipeline with semantic alignment and fixed reference Pareto fronts for normalized hypervolume reporting. Under this protocol, \VAMOS{} achieves competitive normalized hypervolume (Table~\ref{tab:frameworks_hv}) while substantially reducing runtime (Tables~\ref{tab:frameworks_perf}--\ref{tab:frameworks_perf_moead}).

\subsection{Future Work}
\begin{itemize}
  \item \textbf{Many-objective benchmarking}: extend the experimental protocol beyond $m=3$ with scalable indicators.
  \item \textbf{Deeper GPU integration}: expand JAX-based kernels and enable GPU-friendly operators.
  \item \textbf{Broader comparisons}: extend the benchmark suite to additional MOEAs and frameworks and include time-to-target and other indicators alongside normalized HV.
\end{itemize}

\section*{Data and code availability}
All data and code supporting the findings of this study (benchmark scripts, reference Pareto fronts, and CSV artifacts used to regenerate tables) are available in the \VAMOS{} repository:\newline
\url{https://github.com/NicolasRodriguezUribe/VAMOS}.

%==============================================================================
\appendix
\section{Reference fronts and detailed benchmark results}
\label{app:detailed}
%==============================================================================

The repository includes dense reference Pareto fronts for all benchmark problems used for normalized hypervolume computation (\path{data/reference_fronts/}). These fronts are generated analytically when closed forms are available (ZDT and most DTLZ problems) and by dense sampling followed by non-dominated filtering for cases where the Pareto front is disconnected or defined implicitly (DTLZ7 and WFG). For WFG2 we use a higher sampling density (configurable via \path{VAMOS_REF_WFG2_POINTS}) to stabilize HV normalization.

Tables~\ref{tab:detailed_backends} and~\ref{tab:detailed_comparison} report per-problem median runtimes.

\begin{table}[htbp]
\centering
\caption{Detailed VAMOS backend comparison: median runtime (seconds) per problem. Average is computed over all 21 per-problem medians.}
\label{tab:detailed_backends}
\begin{tabular}{l|rrr}
\toprule
\textbf{Problem} & \textbf{Numba} & \textbf{MooCore} & \textbf{NumPy} \\
\midrule
dtlz1 & \textbf{2.67} & 2.74 & 5.48 \\
dtlz2 & \textbf{3.01} & 3.04 & 5.83 \\
dtlz3 & 3.60 & \textbf{3.44} & 7.54 \\
dtlz4 & \textbf{3.72} & 3.95 & 7.00 \\
dtlz5 & 3.74 & \textbf{3.68} & 6.72 \\
dtlz6 & \textbf{3.90} & 4.04 & 7.64 \\
dtlz7 & 3.90 & \textbf{3.73} & 7.12 \\
wfg1 & 4.17 & \textbf{4.13} & 7.91 \\
wfg2 & \textbf{3.90} & 3.95 & 7.49 \\
wfg3 & \textbf{3.81} & 3.96 & 7.15 \\
wfg4 & \textbf{3.93} & 4.06 & 7.65 \\
wfg5 & \textbf{4.31} & 4.46 & 7.98 \\
wfg6 & \textbf{4.40} & 4.74 & 7.63 \\
wfg7 & 3.85 & \textbf{3.83} & 7.18 \\
wfg8 & \textbf{4.18} & 4.35 & 7.30 \\
wfg9 & \textbf{3.85} & 4.18 & 6.23 \\
zdt1 & 2.63 & \textbf{2.56} & 4.82 \\
zdt2 & 2.79 & \textbf{2.72} & 5.24 \\
zdt3 & \textbf{2.76} & 2.81 & 5.19 \\
zdt4 & \textbf{2.41} & 2.51 & 5.33 \\
zdt6 & \textbf{2.44} & 2.51 & 5.16 \\
\midrule
\textbf{Average} & \textbf{3.52} & 3.59 & 6.65 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table*}[htbp]
\tiny
\centering
\caption{Detailed comparison of median runtime (seconds) across all frameworks. Average is computed over all 21 per-problem medians.}
\label{tab:detailed_comparison}
\begin{tabular}{l|rrrrr}
\toprule
\textbf{Problem} & \textbf{VAMOS} & \textbf{pymoo} & \textbf{jMetalPy} & \textbf{DEAP} & \textbf{Platypus} \\
\midrule
dtlz1 & \textbf{2.67} & 3.21 & 11.39 & 16.91 & 19.82 \\
dtlz2 & \textbf{3.01} & 3.38 & 12.23 & 17.62 & 25.72 \\
dtlz3 & \textbf{3.60} & 4.44 & 16.82 & 24.92 & 31.03 \\
dtlz4 & \textbf{3.72} & 3.84 & 15.15 & 22.67 & 29.29 \\
dtlz5 & \textbf{3.74} & 4.03 & 14.65 & 21.78 & 30.89 \\
dtlz6 & \textbf{3.90} & 4.52 & 15.28 & 22.82 & 30.98 \\
dtlz7 & \textbf{3.90} & 4.22 & 16.41 & 20.99 & 30.35 \\
wfg1 & \textbf{4.17} & 4.76 & 30.35 & 31.37 & 38.26 \\
wfg2 & \textbf{3.90} & 4.55 & 48.26 & 46.68 & 54.91 \\
wfg3 & \textbf{3.81} & 4.18 & 44.80 & 44.07 & 50.92 \\
wfg4 & \textbf{3.93} & 4.42 & 24.93 & 26.17 & 33.42 \\
wfg5 & \textbf{4.31} & 4.61 & 25.57 & 28.82 & 41.68 \\
wfg6 & \textbf{4.40} & 4.91 & 122.00 & 125.18 & 133.25 \\
wfg7 & \textbf{3.85} & 4.31 & 37.01 & 37.17 & 45.73 \\
wfg8 & \textbf{4.18} & 4.68 & 86.14 & 79.09 & 83.92 \\
wfg9 & \textbf{3.85} & 4.75 & 179.86 & 166.45 & 169.99 \\
zdt1 & \textbf{2.63} & 2.93 & 11.54 & 14.43 & 20.77 \\
zdt2 & \textbf{2.79} & 3.28 & 12.01 & 15.40 & 21.46 \\
zdt3 & \textbf{2.76} & 3.08 & 11.81 & 15.54 & 21.94 \\
zdt4 & \textbf{2.41} & 3.12 & 11.23 & 15.41 & 17.52 \\
zdt6 & \textbf{2.44} & 3.51 & 10.39 & 14.46 & 18.20 \\
\midrule
\textbf{Average} & \textbf{3.52} & 4.03 & 36.09 & 38.47 & 45.24 \\
\bottomrule
\end{tabular}
\end{table*}

%==============================================================================
\section{Statistical analysis for SMS-EMOA and MOEA/D}
\label{app:stats}
%==============================================================================

Tables in this appendix report Wilcoxon signed-rank tests, bootstrap equivalence analyses, and robustness summaries for SMS-EMOA and MOEA/D, following the same methodology as Section~\ref{sec:stats}.

\subsection{SMS-EMOA}
\input{stats_smsemoa.tex}

\subsection{MOEA/D}
\input{stats_moead.tex}

%==============================================================================
% Bibliography
%==============================================================================
\bibliographystyle{elsarticle-num}
\begin{thebibliography}{33}

\bibitem{coello2007evolutionary}
C.~A.~C. Coello, G.~B. Lamont, D.~A. Van~Veldhuizen,
Evolutionary Algorithms for Solving Multi-Objective Problems,
Springer, 2007.

\bibitem{deb2002fast}
K.~Deb, A.~Pratap, S.~Agarwal, T.~Meyarivan,
A fast and elitist multiobjective genetic algorithm: NSGA-II,
IEEE Trans. Evol. Comput. 6~(2) (2002) 182--197.

\bibitem{blank2020pymoo}
J.~Blank, K.~Deb,
pymoo: Multi-objective optimization in Python,
IEEE Access 8 (2020) 89497--89509.

\bibitem{fortin2012deap}
F.-A. Fortin, F.-M. De~Rainville, M.-A. Gardner, M.~Parizeau, C.~Gagn\'{e},
DEAP: Evolutionary algorithms made easy,
J. Mach. Learn. Res. 13 (2012) 2171--2175.

\bibitem{benitez2019jmetalpy}
A.~Ben{\'\i}tez-Hidalgo, A.~J. Nebro, J.~Garc{\'\i}a-Nieto, I.~Oregi, J.~Del~Ser,
jMetalPy: A Python framework for multi-objective optimization with metaheuristics,
Swarm Evol. Comput. 51 (2019) 100598, doi:10.1016/j.swevo.2019.100598.

\bibitem{hadka2015platypus}
D.~Hadka,
Platypus: a free and open source Python library for multiobjective optimization,
\url{https://github.com/Project-Platypus/Platypus}, accessed 2026.

\bibitem{lam2015numba}
S.~K. Lam, A.~Pitrou, S.~Seibert,
Numba: A LLVM-based Python JIT compiler,
in: LLVM-HPC, 2015.

\bibitem{jax2018github}
J.~Bradbury, R.~Frostig, P.~Hawkins, M.~J. Johnson, C.~Leary, D.~Maclaurin, G.~Necula, A.~Paszke, J.~Vander{P}las, S.~Wanderman-{M}ilne, Q.~Zhang,
JAX: composable transformations of Python+NumPy programs,
\url{https://github.com/google/jax}, accessed 2026.

\bibitem{moocore}
moocore: multi-objective optimization core library,
\url{https://github.com/multi-objective/moocore}, accessed 2026.

\bibitem{zhang2007moea}
Q.~Zhang, H.~Li,
MOEA/D: A multiobjective evolutionary algorithm based on decomposition,
IEEE Trans. Evol. Comput. 11~(6) (2007) 712--731.

\bibitem{deb2014evolutionary}
K.~Deb, H.~Jain,
An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting,
IEEE Trans. Evol. Comput. 18~(4) (2014) 577--601.

\bibitem{beume2007sms}
N.~Beume, B.~Naujoks, M.~Emmerich,
SMS-EMOA: Multiobjective selection based on dominated hypervolume,
Eur. J. Oper. Res. 181~(3) (2007) 1653--1669.

\bibitem{zitzler2001spea2}
E.~Zitzler, M.~Laumanns, L.~Thiele,
SPEA2: Improving the strength Pareto evolutionary algorithm,
TIK-Report 103, ETH Zurich, 2001.

\bibitem{zitzler2004indicator}
E.~Zitzler, S.~K{\"u}nzli,
Indicator-based selection in multiobjective search,
in: PPSN, Springer, 2004, pp.~832--842.

\bibitem{nebro2009smpso}
A.~J. Nebro, J.~J. Durillo, C.~A.~C. Coello Coello,
SMPSO: A new PSO-based metaheuristic for multi-objective optimization,
in: IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making, 2009, pp.~66--73.

\bibitem{panichella2019adaptive}
A.~Panichella,
An adaptive evolutionary algorithm based on non-Euclidean geometry for many-objective optimization,
in: GECCO, ACM, 2019, pp.~595--603.

\bibitem{cheng2016reference}
R.~Cheng, Y.~Jin, M.~Olhofer, B.~Sendhoff,
A reference vector guided evolutionary algorithm for many-objective optimization,
IEEE Trans. Evol. Comput. 20~(5) (2016) 773--791.

\bibitem{zitzler2000zdt}
E.~Zitzler, K.~Deb, L.~Thiele,
Comparison of multiobjective evolutionary algorithms: Empirical results,
Evol. Comput. 8~(2) (2000) 173--195.

\bibitem{deb2002dtlz}
K.~Deb, L.~Thiele, M.~Laumanns, E.~Zitzler,
Scalable multi-objective optimization test problems,
in: Congress on Evolutionary Computation (CEC), 2002, pp.~825--830.

\bibitem{huband2006wfg}
S.~Huband, P.~Hingston, L.~Barone, L.~While,
A review of multiobjective test problems and a scalable test problem toolkit,
IEEE Trans. Evol. Comput. 10~(5) (2006) 477--506.

\bibitem{zitzler2003performance}
E.~Zitzler, L.~Thiele, M.~Laumanns, C.~M. Fonseca, V.~G. da Fonseca,
Performance assessment of multiobjective optimizers: An analysis and review,
IEEE Trans. Evol. Comput. 7~(2) (2003) 117--132.

\bibitem{vargha2000critique}
A.~Vargha, H.~D. Delaney,
A critique and improvement of the CL common language effect size statistics of {M}c{G}raw and {W}ong,
J. Educ. Behav. Stat. 25~(2) (2000) 101--132.

\end{thebibliography}

\end{document}
