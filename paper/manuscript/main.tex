% VAMOS Paper: Swarm and Evolutionary Computation (Elsevier)
% -----------------------------------------------------------------------------
% NOTE: Several tables in this manuscript are auto-generated from
% `experiments/benchmark_paper.csv`:
%   - `paper/update_paper_tables_from_csv.py` updates runtime tables
%     (`tab:backends`, `tab:frameworks_perf`, `tab:detailed_backends`,
%      `tab:detailed_comparison`).
%   - `paper/run_statistical_tests.py` updates Wilcoxon tables
%     (`tab:stats_runtime`, `tab:stats_hypervolume`).
% -----------------------------------------------------------------------------
\documentclass[preprint,12pt]{elsarticle}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\hypersetup{hypertexnames=false}
\makeatletter
\g@addto@macro\UrlBreaks{\do\.\do\_\do\-}
\makeatother
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}

% Code listing style
\lstset{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red!70!black},
  showstringspaces=false,
  frame=single,
  breaklines=true,
  numbers=left,
  numberstyle=\tiny\color{gray}
}

\journal{Swarm and Evolutionary Computation}

\newcommand{\VAMOS}{VAMOS}
\newcommand{\HV}{\mathrm{HV}}

\begin{document}

\begin{frontmatter}

\title{VAMOS: A Vectorized Architecture for Multiobjective Optimization Studies\\A High-Performance Python Framework with Adaptive and Auto-Configurable Components}

\author[uma]{First Author\corref{cor1}}
\ead{author@university.edu}
\author[uma]{Second Author}

\cortext[cor1]{Corresponding author}
\address[uma]{Department of Computer Science, University, Country}

\begin{abstract}
Python has become the de facto platform for empirical studies in multi-objective optimization, yet most multi-objective evolutionary algorithm (MOEA) libraries remain dominated by object-oriented designs that incur substantial interpreter overhead in the inner loop. We introduce \VAMOS{} (Vectorized Architecture for Multiobjective Optimization Studies), a research-oriented framework that represents populations as contiguous numerical arrays and factors algorithmic logic from numerical kernels. \VAMOS{} provides interchangeable compute backends (NumPy, Numba JIT, and optional C/JAX accelerators), a unified configuration API, and nine MOEAs (NSGA-II/III, MOEA/D, SMS-EMOA, SPEA2, IBEA, SMPSO, AGE-MOEA, and RVEA). Two adaptive modules complement the core algorithms: (i) an irace-inspired racing tuner with optional multi-fidelity (Hyperband-style) warm-starting, and (ii) bandit-based adaptive operator selection for configurable operator portfolios. We also release a reproducible benchmarking pipeline that aligns problem definitions and operator settings across common Python frameworks and reports runtime and \emph{normalized} hypervolume computed with fixed reference Pareto fronts. Across the ZDT, DTLZ, and WFG suites, \VAMOS{} achieves up to an order-of-magnitude reduction in runtime relative to established libraries while maintaining competitive solution quality.
\end{abstract}

\begin{keyword}
Multi-objective optimization \sep Evolutionary algorithms \sep Benchmarking \sep Hypervolume \sep Python framework \sep Vectorization
\end{keyword}

\end{frontmatter}

%==============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

Multi-objective optimization problems (MOPs) arise whenever trade-offs exist between conflicting objectives, requiring the computation of a diverse approximation of the Pareto-optimal set rather than a single optimum~\cite{coello2007evolutionary}. Multi-objective evolutionary algorithms (MOEAs) remain a dominant approach due to their robustness, anytime behavior, and ability to return sets of non-dominated solutions.

In Python, practical benchmarking of MOEAs is often constrained by two friction points. First, the ecosystem is fragmented: libraries differ in problem APIs, operator configuration, result formats, and runtime instrumentation~\cite{blank2020pymoo,fortin2012deap,benitez2019jmetalpy}. Second, many implementations rely on per-solution objects and Python-level loops inside selection, variation, and survival, making the overhead of the interpreter comparable to (or larger than) the cost of typical benchmark functions. This limitation becomes acute for large-scale studies with many seeds, large populations, and repeated statistical tests.

\VAMOS{} is designed to address both issues with a single guiding principle: \emph{keep algorithm state in dense arrays and push hot loops into vectorized kernels}. The framework also targets reproducibility in experimental methodology: we provide a benchmark runner that standardizes problem dimensions and operators across multiple Python frameworks, and a hypervolume protocol with fixed reference Pareto fronts so that solution-quality metrics are comparable across runs and frameworks.

The contributions of this work are:
\begin{enumerate}
  \item \textbf{Vectorized MOEA core}: a unified, array-based internal representation with pluggable compute kernels (NumPy/Numba/C/JAX) for selection, variation, and survival.
  \item \textbf{Modular algorithm suite}: nine MOEAs implemented on top of shared components (termination, archives, evaluation backends, and metrics).
  \item \textbf{Adaptive components}: an irace-inspired racing tuner with optional multi-fidelity warm-starting, and bandit-based adaptive operator selection (AOS) for operator portfolios.
  \item \textbf{Reproducible benchmarking}: a pipeline that aligns cross-framework configurations and computes \emph{normalized} hypervolume against fixed reference Pareto fronts distributed with the repository.
\end{enumerate}

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

\subsection{Python MOEA frameworks}

Several mature Python libraries exist for MOEAs. \textbf{pymoo}~\cite{blank2020pymoo} provides a rich collection of algorithms and operators with an emphasis on modularity. \textbf{DEAP}~\cite{fortin2012deap} is a general evolutionary computation toolkit; while flexible, users typically assemble MOEAs manually. \textbf{jMetalPy}~\cite{benitez2019jmetalpy} ports the jMetal architecture to Python and emphasizes end-to-end experimentation (observer-driven progress/visualization, a ``laboratory'' workflow, and built-in statistical post-processing with LaTeX export), but it relies on object-centric representations and does not address cross-toolkit semantic drift. We build upon this methodological focus on rigorous experimental studies and complement it with an array-based execution model and a fairness-oriented benchmarking protocol that aligns problem definitions and operator semantics across toolkits. \textbf{Platypus}~\cite{hadka2015platypus} provides a lightweight API and a set of classic MOEAs.

Table~\ref{tab:frameworks} summarizes high-level differences relevant to this work. The main distinction is the internal representation: \VAMOS{} consistently uses dense arrays for populations and objectives, while most alternatives operate on per-solution objects. This difference enables kernel-based acceleration and reduces Python overhead in the inner loop.

\begin{table}[htbp]
\centering
\caption{Comparison of Python multi-objective optimization frameworks (high-level capabilities).}
\label{tab:frameworks}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Framework} & \textbf{MOEAs} & \textbf{Array-based core} & \textbf{Accelerated kernels} & \textbf{Auto-config} & \textbf{AOS} \\
\midrule
pymoo~\cite{blank2020pymoo} & 8+ & Partial & No & No & No \\
DEAP~\cite{fortin2012deap} & Custom & No & No & No & No \\
jMetalPy~\cite{benitez2019jmetalpy} & 11+ & No & No & No & Limited \\
Platypus~\cite{hadka2015platypus} & 8+ & No & No & No & No \\
\VAMOS{} & 9 & Yes & Numba/C/JAX & Yes & Yes \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Automatic algorithm configuration}

Automatic algorithm configuration (AAC) aims to select hyperparameters that maximize performance on a target distribution of instances. irace~\cite{lopez2016irace} popularized iterated racing with non-parametric tests to discard poorly performing configurations early. Multi-fidelity methods such as successive halving and Hyperband evaluate many configurations with small budgets before promoting a subset to larger budgets, improving sample efficiency. \VAMOS{} combines racing with optional multi-fidelity evaluation and warm-start checkpoints to reduce redundant computation between fidelity levels.

\subsection{Adaptive operator selection}

Adaptive operator selection (AOS) dynamically allocates sampling probability across operators based on online feedback~\cite{fialho2010analyzing}. Bandit-based approaches are attractive because they explicitly manage the exploration--exploitation trade-off and can be implemented with minimal assumptions. \VAMOS{} provides a portfolio-driven AOS interface and integrates bandit policies into NSGA-II, using reward signals derived from survival and non-dominated insertions.

%==============================================================================
\section{VAMOS Framework}
\label{sec:framework}
%==============================================================================

\subsection{Architecture and data model}

\VAMOS{} is organized into four layers:
\begin{enumerate}
  \item \textbf{Foundation}: problem definitions, kernels, metrics, and archives;
  \item \textbf{Engine}: algorithm implementations and shared components;
  \item \textbf{Adaptation}: tuning and AOS modules;
  \item \textbf{Experiment}: CLI, benchmarking utilities, visualization, and reporting.
\end{enumerate}

The core design choice is to represent a population as a pair of dense arrays: decision variables $X \in \mathbb{R}^{N \times n}$ and objective values $F \in \mathbb{R}^{N \times m}$ for population size $N$, $n$ decision variables, and $m$ objectives. Variation and survival operate on these arrays (or views thereof), enabling vectorized operations (NumPy) and compilation of hot loops (Numba) without per-individual Python objects.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/architecture.png}
\caption{\VAMOS{} four-layer architecture.}
\label{fig:architecture}
\end{figure}

\subsection{Compute kernels}

Algorithmic logic (e.g., \emph{NSGA-II selects survivors by non-dominated sorting and crowding distance}) is separated from numerical kernels that implement the corresponding computations. \VAMOS{} provides multiple backends:
\begin{itemize}
  \item \textbf{NumPy}: baseline backend using vectorized array operations.
  \item \textbf{Numba}: JIT compilation of hot operators and utilities, reducing Python overhead~\cite{lam2015numba}.
  \item \textbf{C-backed indicators}: optional acceleration for performance indicators such as hypervolume (e.g., via MooCore)~\cite{moocore}.
  \item \textbf{JAX}: optional backend for parts of the pipeline and autodiff-based constraints, enabling GPU/TPU acceleration when available~\cite{jax2018github}.
\end{itemize}

Backends are selected through configuration (e.g., \texttt{engine="numba"}) and are transparent to algorithm code.

In addition to accelerating arithmetic kernels, \VAMOS{} reduces overhead by minimizing Python-side allocations: variation operators can reuse pre-allocated workspaces (e.g., scratch buffers for SBX/PM) and algorithms maintain state as contiguous arrays. This design is particularly beneficial in inner-loop routines such as tournament selection, non-dominated sorting, crowding-distance computation, and archive updates.

\subsection{Algorithm suite}

\VAMOS{} implements nine multi-objective algorithms covering dominance-based, decomposition-based, indicator-based, and reference-vector paradigms. Table~\ref{tab:algorithms} summarizes the suite.

\begin{table}[htbp]
\centering
\caption{Multi-objective algorithms implemented in \VAMOS{}.}
\label{tab:algorithms}
\begin{tabular}{llc}
\toprule
\textbf{Algorithm} & \textbf{Category} & \textbf{Reference} \\
\midrule
NSGA-II & Dominance-based & \cite{deb2002fast} \\
NSGA-III & Reference-point & \cite{deb2014evolutionary} \\
MOEA/D & Decomposition & \cite{zhang2007moea} \\
SMS-EMOA & Indicator-based & \cite{beume2007sms} \\
SPEA2 & Archive-based & \cite{zitzler2001spea2} \\
IBEA & Indicator-based & \cite{zitzler2004indicator} \\
SMPSO & Swarm-based & \cite{nebro2009smpso} \\
AGE-MOEA & Adaptive geometry & \cite{panichella2019adaptive} \\
RVEA & Reference vectors & \cite{cheng2016reference} \\
\bottomrule
\end{tabular}
\end{table}

AGE-MOEA follows the adaptive geometry estimation survival scheme of Panichella~\cite{panichella2019adaptive}: after non-dominated sorting, the first front is normalized by extreme points, a curvature-dependent $p$-norm is estimated, and survival uses geometry-aware crowding based on Minkowski distances. RVEA follows Cheng et~al.~\cite{cheng2016reference} with angle-penalized distance (APD) survival and periodic reference-vector adaptation using ideal/nadir scaling; we therefore enforce \texttt{pop\_size} to match the number of simplex-lattice reference directions induced by \texttt{n\_partitions}.

\subsection{Execution model and evaluation backends}

Algorithms in \VAMOS{} expose a uniform \texttt{run(problem, termination, seed, eval\_backend, live\_viz)} interface, and several algorithms also support an \emph{ask--tell} loop for streaming or interactive use. Objective evaluations are dispatched through a small evaluation-backend protocol, enabling serial evaluation for controlled benchmarking and parallel evaluation (e.g., multiprocessing or Dask-based) when objective functions are expensive. Separating evaluation from algorithmic state keeps the algorithm core deterministic under a fixed random seed and reduces the risk of framework-specific parallelism becoming a confounder in empirical comparisons.

\begin{algorithm}[htbp]
\caption{Vectorized NSGA-II execution in \VAMOS{} (high-level).}
\label{alg:nsgaii}
\begin{algorithmic}[1]
\State Sample initial population $X \in \mathbb{R}^{N \times n}$ within bounds
\State Evaluate objectives $F \gets f(X)$
\While{termination criterion not met}
  \State Select mating indices $P$ via tournament on (rank, crowding)
  \State Generate offspring $X' \gets \mathcal{V}(X_P)$ (crossover, mutation, repair)
  \State Evaluate offspring $F' \gets f(X')$
  \State Merge: $(X,F) \gets ([X;X'], [F;F'])$
  \State Compute ranks and crowding distances
  \State Select survivors of size $N$ (rank--crowding)
\EndWhile
\State \Return final population and derived non-dominated set
\end{algorithmic}
\end{algorithm}

\subsection{Variation pipeline and encodings}

\VAMOS{} supports real-valued, binary, and permutation encodings through a variation pipeline that composes selection, crossover, mutation, and optional repair operators. For continuous domains, the default configuration uses simulated binary crossover (SBX) and polynomial mutation (PM), with mutation probability typically set to $1/n$ where $n$ is the number of decision variables. Encodings are normalized internally to ensure consistent operator resolution and avoid silent configuration mismatches.

\subsection{Archives, metrics, and analysis tooling}

The framework includes optional external archives (with configurable pruning strategies) and common indicators. Hypervolume computation can use optimized backends when installed, but \VAMOS{} also provides fallback implementations for low-dimensional cases. Results are returned in a unified result object that supports plotting and post-hoc analysis (e.g., filtering non-dominated solutions and exporting fronts).

\subsection{Adaptive operator selection (AOS)}

AOS in \VAMOS{} is portfolio-based: users specify an operator pool (e.g., multiple SBX/PM parameterizations) and a bandit method that selects one operator per generation. The NSGA-II integration supports $\epsilon$-greedy, UCB, EXP3, Thompson sampling, and sliding-window UCB~\cite{fialho2010analyzing}, with an optional exploration floor that mixes uniform sampling. Rewards are computed from survival rate and non-dominated insertions, with support for weighted combinations.

\subsection{Racing-based hyperparameter tuning}

\VAMOS{} provides an irace-inspired racing tuner~\cite{lopez2016irace} for automatic algorithm configuration. Candidates are evaluated across instances and seeds; underperforming configurations are eliminated as evidence accumulates. At each fidelity level we evaluate all remaining configurations on the same instance $\times$ seed blocks, and promotions are based on scores from the current level only. For sample efficiency, the tuner optionally uses a multi-fidelity strategy (Hyperband-style successive halving~\cite{li2017hyperband}) and can pass warm-start checkpoints between fidelity levels to reuse computation.

%==============================================================================
\section{Experimental Evaluation}
\label{sec:experiments}
%==============================================================================

\subsection{Benchmark suite and configuration alignment}

We evaluate runtime and solution quality on widely used synthetic benchmarks: ZDT1--4 and ZDT6~\cite{zitzler2000zdt} (two objectives), DTLZ1--4 and DTLZ7~\cite{deb2002dtlz} (three objectives), and WFG1--9~\cite{huband2006wfg} (two objectives). Problem dimensionalities are fixed to standard definitions: for example, DTLZ2/3/4 use $n = m + k - 1$ with $m=3$ and $k=10$ (thus $n=12$).

To ensure comparability across frameworks, we standardize the algorithmic settings of NSGA-II: population size $N=100$, SBX crossover probability $p_c=1.0$ and distribution index $\eta_c=20$, polynomial mutation distribution index $\eta_m=20$ and mutation probability $p_m=1/n$, tournament selection, and rank--crowding survival. For each framework we map these settings to the closest available implementation.

This workflow follows standard practice in comparative MOEA studies (independent runs, indicator computation, and statistical testing) and is consistent with the experimentation methodology described in jMetalPy~\cite{benitez2019jmetalpy}. Our main extension is to make the \emph{inputs} to this workflow comparable across toolkits by enforcing semantic alignment of both problem definitions and operator probabilities.

Crucially, we align \emph{semantics} (not only parameter names) and enforce consistent benchmark definitions. Two examples illustrate why this matters. First, in \textbf{pymoo} the polynomial mutation operator exposes both an individual-level probability (\texttt{prob}) and a per-variable probability (\texttt{prob\_var}); to match the standard ``$p_m=1/n$ per decision variable'' setting used by DEAP/jMetalPy, we set \texttt{prob}=1 and \texttt{prob\_var}=1/n. Second, some libraries ship benchmark problems under canonical names but with library-specific domains; for instance, Platypus' built-in ZDT base class uses $[0,1]^n$, whereas ZDT4 is defined with $x_1\in[0,1]$ and $x_{2..n}\in[-5,5]$.

Therefore, for toolkits or APIs where problem definitions differ (notably DEAP and Platypus for ZDT4), we evaluate individuals using a shared reference implementation of the benchmark function and bounds (validated against pymoo) while still using each framework's NSGA-II implementation and operators. For WFG in Platypus we similarly wrap pymoo's WFG definitions to avoid definition drift. While full equivalence cannot be guaranteed due to framework-specific details (tie-breaking, boundary handling, duplicate elimination, etc.), these choices aim to make the comparison as fair as possible by removing confounders unrelated to algorithmic overhead and numerical kernels.

\subsection{Runtime measurement}

All runtimes are measured wall-clock using high-resolution timers and include algorithm overhead and objective evaluations. We report medians across 30 independent seeds and provide per-problem and per-family summaries.

\subsection{Normalized hypervolume protocol}
\label{sec:hv}

Hypervolume (HV) measures the Lebesgue measure of the region dominated by an approximation set $A$ with respect to a reference point $r$ (for minimization)~\cite{zitzler2003performance}:
\begin{equation}
  \HV(A, r) = \lambda\left(\bigcup_{a \in A} [a_1, r_1] \times \cdots \times [a_m, r_m]\right).
\end{equation}
Because HV is scale-dependent and sensitive to the choice of $r$, naive implementations can produce values that are not comparable across runs (e.g., if $r$ is expanded per run) and can be misleading when dominated solutions are included. To ensure a stable and comparable quality metric, we adopt the following protocol:
\begin{enumerate}
  \item \textbf{Non-dominated filtering}: for every framework we compute HV on the final non-dominated set only.
  \item \textbf{Fixed reference Pareto fronts}: for each benchmark problem we store a dense reference Pareto front $P_{\text{ref}}$ in \texttt{data/reference\_fronts/}. For ZDT and DTLZ (except DTLZ7) these fronts are generated analytically; for DTLZ7 and WFG we generate $P_{\text{ref}}$ by dense sampling followed by non-dominated filtering.
  \item \textbf{Fixed reference point}: we set $r = \max(P_{\text{ref}}) + \epsilon$ (component-wise) with a small $\epsilon$, and we disallow run-dependent expansion.
  \item \textbf{Normalization}: we report $\HV(A,r) / \HV(P_{\text{ref}},r)$, yielding a dimensionless score typically in $[0,1]$ and reducing sensitivity to objective scaling.
\end{enumerate}
For WFG problems, the reference front is necessarily an approximation; we use large Pareto-set samples and non-dominated filtering, and we increase density for particularly sensitive cases (e.g., WFG2) to avoid underestimating $\HV(P_{\text{ref}}, r)$ and obtaining normalized HV slightly above~1.

\subsection{VAMOS backend comparison}

Table~\ref{tab:backends} reports median runtime for \VAMOS{} backends aggregated by problem family.

\begin{table}[htbp]
\centering
\caption{VAMOS backend comparison: median runtime (seconds) by problem family.}
\label{tab:backends}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Backend} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Average} \\
\midrule
Numba & \textbf{7.72} & \textbf{6.85} & \textbf{7.91} & \textbf{7.50} \\
Moocore & 7.80 & 7.79 & 8.08 & 7.89 \\
Numpy & 13.33 & 13.56 & 13.41 & 13.44 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cross-framework comparison}

Table~\ref{tab:frameworks_perf} compares \VAMOS{} against other Python frameworks on runtime. \VAMOS{} is evaluated with its accelerated backend (Numba) for the headline comparison, while additional backends are reported in the appendix.

\begin{table}[htbp]
\centering
\caption{Median runtime (seconds) by problem family across all frameworks.}
\label{tab:frameworks_perf}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Framework} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Average} \\
\midrule
VAMOS & \textbf{7.72} & \textbf{6.85} & \textbf{7.91} & \textbf{7.50} \\
pymoo & 12.04 & 10.87 & 11.33 & 11.41 \\
DEAP & 38.36 & 42.93 & 76.02 & 52.44 \\
jMetalPy & 25.82 & 23.80 & 75.05 & 41.56 \\
Platypus & 50.82 & 40.38 & 84.89 & 58.69 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Statistical analysis}
\label{sec:stats}

We use the Wilcoxon signed-rank test~\cite{wilcoxon1945} at significance level $\alpha=0.05$ to compare paired performance distributions across seeds, per problem. Runtime is treated as a \emph{minimization} metric, and normalized hypervolume as a \emph{maximization} metric. While jMetalPy~\cite{benitez2019jmetalpy} illustrates both frequentist and Bayesian statistical workflows (and exemplifies Wilcoxon rank-sum tests in a multi-algorithm setting), our design is explicitly paired by construction (shared seeds), so we report the signed-rank variant as a minimal, widely used test under this protocol.

\begin{table}[htbp]
\centering
\caption{Runtime comparison with Wilcoxon signed-rank test results.}
\label{tab:stats_runtime}
\begin{tabular}{l|rr|r|l}
\toprule
\textbf{Problem} & \textbf{VAMOS (numba)} & \textbf{pymoo} & \textbf{p-value} & \textbf{Sig.} \\
\midrule
zdt1 & \textbf{7.47} & 12.49 & 0.0000 & $\checkmark$ \\
zdt2 & \textbf{6.99} & 10.36 & 0.0000 & $\checkmark$ \\
zdt3 & \textbf{7.98} & 12.39 & 0.0000 & $\checkmark$ \\
zdt4 & \textbf{8.05} & 12.59 & 0.0000 & $\checkmark$ \\
zdt6 & \textbf{7.60} & 16.02 & 0.0000 & $\checkmark$ \\
dtlz1 & \textbf{7.86} & 11.51 & 0.0000 & $\checkmark$ \\
dtlz2 & \textbf{7.56} & 12.00 & 0.0000 & $\checkmark$ \\
dtlz3 & \textbf{5.60} & 9.86 & 0.0000 & $\checkmark$ \\
dtlz4 & \textbf{6.09} & 10.97 & 0.0000 & $\checkmark$ \\
dtlz7 & \textbf{6.07} & 8.76 & 0.0000 & $\checkmark$ \\
wfg1 & \textbf{6.96} & 9.47 & 0.0000 & $\checkmark$ \\
wfg2 & \textbf{6.92} & 9.26 & 0.0000 & $\checkmark$ \\
wfg3 & \textbf{7.95} & 11.85 & 0.0000 & $\checkmark$ \\
wfg4 & \textbf{7.52} & 10.42 & 0.0000 & $\checkmark$ \\
wfg5 & \textbf{6.48} & 9.07 & 0.0000 & $\checkmark$ \\
wfg6 & \textbf{7.56} & 10.08 & 0.0000 & $\checkmark$ \\
wfg7 & \textbf{8.06} & 9.11 & 0.0000 & $\checkmark$ \\
wfg8 & \textbf{8.33} & 12.89 & 0.0000 & $\checkmark$ \\
wfg9 & \textbf{9.38} & 11.87 & 0.0000 & $\checkmark$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Normalized hypervolume comparison with Wilcoxon signed-rank test results.}
\label{tab:stats_hypervolume}
\begin{tabular}{l|rr|r|l}
\toprule
\textbf{Problem} & \textbf{VAMOS (numba)} & \textbf{pymoo} & \textbf{p-value} & \textbf{Sig.} \\
\midrule
zdt1 & 0.99 & \textbf{0.99} & 0.005 & $\checkmark$ \\
zdt2 & 0.98 & \textbf{0.98} & 0.036 & $\checkmark$ \\
zdt3 & 1.00 & \textbf{1.00} & 0.001 & $\checkmark$ \\
zdt4 & 0.99 & \textbf{0.99} & 0.129 &  \\
zdt6 & 0.98 & \textbf{0.98} & 0.0000 & $\checkmark$ \\
dtlz1 & \textbf{0.92} & 0.91 & 0.404 &  \\
dtlz2 & 0.79 & \textbf{0.79} & 0.146 &  \\
dtlz3 & 0.79 & \textbf{0.79} & 0.584 &  \\
dtlz4 & 0.79 & \textbf{0.80} & 0.012 & $\checkmark$ \\
dtlz7 & \textbf{0.89} & 0.88 & 0.871 &  \\
wfg1 & 0.59 & \textbf{0.62} & 0.158 &  \\
wfg2 & 0.99 & \textbf{0.99} & 0.213 &  \\
wfg3 & \textbf{0.98} & 0.98 & 0.871 &  \\
wfg4 & 0.97 & \textbf{0.97} & 0.004 & $\checkmark$ \\
wfg5 & 0.83 & \textbf{0.83} & 0.020 & $\checkmark$ \\
wfg6 & 0.84 & \textbf{0.85} & 0.073 &  \\
wfg7 & 0.97 & \textbf{0.97} & 0.100 &  \\
wfg8 & 0.76 & \textbf{0.76} & 0.221 &  \\
wfg9 & \textbf{0.81} & 0.71 & 0.205 &  \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Discussion}

The observed speedups are primarily due to (i) the array-based representation that removes per-solution Python overhead, and (ii) JIT compilation of hot operators and utilities in the Numba backend. The normalized hypervolume protocol ensures that performance gains are not obtained at the expense of solution quality and avoids artifacts caused by run-dependent reference points.

\subsection{Threats to validity}

Despite careful alignment of dimensions, budgets, problem definitions, and operator settings, cross-framework comparisons remain subject to several threats: (i) operator implementations may differ in subtle details (e.g., boundary handling, duplicate elimination, tie-breaking in survival), (ii) random number generation and seeding semantics vary across libraries, and (iii) library-specific overheads (data conversion, object materialization) may affect measured runtime. We mitigate major sources of unfairness by enforcing consistent benchmark bounds (e.g., ZDT4) and matching operator semantics where APIs differ (e.g., mutation probability in pymoo).

For hypervolume, reference fronts for problems without closed-form Pareto fronts are necessarily approximations; while we generate them densely and fix them across runs, remaining discrepancies can affect absolute normalized values, especially on WFG problems. We therefore treat normalized HV primarily as a comparative metric under a fixed protocol, and we report distributions across multiple seeds rather than relying on single-run outcomes.

\subsection{Reproducibility}

All experiments in this paper are reproducible from the repository. Benchmarks are executed via \path{python -m paper.run_paper_benchmark}. We control the number of seeds and workers with \texttt{VAMOS\_N\_SEEDS} and \texttt{VAMOS\_N\_JOBS}; unless otherwise stated, we use \texttt{VAMOS\_N\_SEEDS=30}. LaTeX tables are regenerated by \path{python paper/update_paper_tables_from_csv.py} (runtime tables) and \path{python paper/run_statistical_tests.py} (Wilcoxon tables). Benchmark reports also emit jMetalPy-compatible laboratory outputs (QualityIndicatorSummary.csv, Wilcoxon tables, and boxplots) under \path{summary/lab/} for interoperable post-processing. Reference Pareto fronts used for normalized hypervolume are stored in \path{data/reference_fronts/} and can be regenerated with the provided scripts under \path{experiments/scripts/} (front density is configurable via environment variables, e.g., \path{VAMOS_REF_WFG2_POINTS}).

\subsection{LLM-assisted development workflow}

We used a large language model (LLM) as a code assistant to accelerate mechanical refactors, surface configuration mismatches across frameworks, and propose minimal patches. The LLM was treated strictly as a proposal generator: changes were applied as explicit diffs, reviewed by the authors, and accepted only if they passed automated verification gates. In practice we followed an agentic propose--apply--verify loop: propose a targeted change \(\rightarrow\) implement the patch \(\rightarrow\) run fast-fail checks (architecture/health gates and static checks) \(\rightarrow\) run the project test/build suite and targeted benchmark smoke tests \(\rightarrow\) iterate until all gates pass. This constrains AI assistance to a productivity role rather than a source of truth; all reported results are produced solely by the released code and fixed reference data.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.98\textwidth]{figures/methodology_pro.png}
  \caption{Agentic propose--apply--verify loop used to control LLM-assisted changes in the benchmarking pipeline.}
  \label{fig:llm_workflow}
\end{figure}

%==============================================================================
\section{Conclusions and Future Work}
\label{sec:conclusions}
%==============================================================================

We presented \VAMOS{}, a high-performance Python framework for multi-objective optimization studies. By combining a vectorized core with pluggable compute kernels, \VAMOS{} reduces inner-loop overhead and enables scalable benchmarking. The framework also includes adaptive modules for automatic configuration and operator selection, and a reproducible experimental pipeline with fixed reference Pareto fronts for normalized hypervolume reporting.

\subsection{Future Work}
\begin{itemize}
  \item \textbf{Many-objective benchmarking}: extend the experimental protocol beyond $m=3$ with scalable indicators.
  \item \textbf{Deeper GPU integration}: expand JAX-based kernels and enable GPU-friendly operators.
  \item \textbf{Richer AOS rewards}: incorporate indicator deltas (e.g., HV improvements) in a principled credit assignment scheme.
\end{itemize}

%==============================================================================
\appendix
\section{Reference fronts and detailed benchmark results}
\label{app:detailed}
%==============================================================================

The repository includes dense reference Pareto fronts for all benchmark problems used for normalized hypervolume computation (\path{data/reference_fronts/}). These fronts are generated analytically when closed forms are available (ZDT and most DTLZ problems) and by dense sampling followed by non-dominated filtering for cases where the Pareto front is disconnected or defined implicitly (DTLZ7 and WFG). For WFG2 we use a higher sampling density (configurable via \path{VAMOS_REF_WFG2_POINTS}) to stabilize HV normalization.

Tables~\ref{tab:detailed_backends} and~\ref{tab:detailed_comparison} report per-problem median runtimes.

\begin{table}[htbp]
\centering
\caption{Detailed VAMOS backend comparison: median runtime (seconds) per problem.}
\label{tab:detailed_backends}
\begin{tabular}{l|rrr}
\toprule
\textbf{Problem} & \textbf{Numba} & \textbf{Moocore} & \textbf{Numpy} \\
\midrule
dtlz1 & \textbf{7.86} & 8.19 & 15.52 \\
dtlz2 & \textbf{7.56} & 8.33 & 14.32 \\
dtlz3 & \textbf{5.60} & 6.41 & 12.22 \\
dtlz4 & \textbf{6.09} & 7.05 & 11.88 \\
dtlz7 & \textbf{6.07} & 8.63 & 14.15 \\
wfg1 & \textbf{6.96} & 8.20 & 11.75 \\
wfg2 & 6.92 & \textbf{6.75} & 11.52 \\
wfg3 & \textbf{7.95} & 8.23 & 11.60 \\
wfg4 & 7.52 & \textbf{7.31} & 11.17 \\
wfg5 & \textbf{6.48} & 7.89 & 13.84 \\
wfg6 & \textbf{7.56} & 7.73 & 12.29 \\
wfg7 & 8.06 & \textbf{6.99} & 11.60 \\
wfg8 & 8.33 & \textbf{7.85} & 15.55 \\
wfg9 & \textbf{9.38} & 9.77 & 14.00 \\
zdt1 & 7.47 & \textbf{7.38} & 12.56 \\
zdt2 & \textbf{6.99} & 7.25 & 12.23 \\
zdt3 & \textbf{7.98} & 8.69 & 13.81 \\
zdt4 & 8.05 & \textbf{7.88} & 15.09 \\
zdt6 & 7.60 & \textbf{6.45} & 11.81 \\
\midrule
\textbf{Average} & \textbf{7.39} & 7.74 & 12.99 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table*}[htbp]
\tiny
\centering
\caption{Detailed comparison of median runtime (seconds) across all frameworks.}
\label{tab:detailed_comparison}
\begin{tabular}{l|rrrrr}
\toprule
\textbf{Problem} & \textbf{VAMOS} & \textbf{pymoo} & \textbf{DEAP} & \textbf{jMetalPy} & \textbf{Platypus} \\
\midrule
dtlz1 & \textbf{7.86} & 11.51 & 44.64 & 27.83 & 43.81 \\
dtlz2 & \textbf{7.56} & 12.00 & 43.21 & 24.46 & 44.41 \\
dtlz3 & \textbf{5.60} & 9.86 & 41.05 & 23.88 & 35.31 \\
dtlz4 & \textbf{6.09} & 10.97 & 42.52 & 22.39 & 41.10 \\
dtlz7 & \textbf{6.07} & 8.76 & 38.96 & 20.74 & 38.68 \\
wfg1 & \textbf{6.96} & 9.47 & 47.04 & 55.68 & 57.42 \\
wfg2 & \textbf{6.92} & 9.26 & 87.82 & 76.53 & 83.89 \\
wfg3 & \textbf{7.95} & 11.85 & 96.22 & 76.52 & 85.23 \\
wfg4 & \textbf{7.52} & 10.42 & 43.31 & 42.71 & 55.68 \\
wfg5 & \textbf{6.48} & 9.07 & 43.87 & 37.57 & 73.03 \\
wfg6 & \textbf{7.56} & 10.08 & 200.89 & 223.06 & 241.54 \\
wfg7 & \textbf{8.06} & 9.11 & 60.41 & 47.03 & 70.96 \\
wfg8 & \textbf{8.33} & 12.89 & 172.66 & 150.11 & 160.46 \\
wfg9 & \textbf{9.38} & 11.87 & 364.86 & 413.85 & 373.63 \\
zdt1 & \textbf{7.47} & 12.49 & 38.91 & 27.92 & 51.50 \\
zdt2 & \textbf{6.99} & 10.36 & 35.39 & 24.08 & 46.55 \\
zdt3 & \textbf{7.98} & 12.39 & 43.52 & 29.38 & 62.73 \\
zdt4 & \textbf{8.05} & 12.59 & 49.89 & 27.07 & 43.81 \\
zdt6 & \textbf{7.60} & 16.02 & 38.17 & 28.58 & 60.10 \\
\midrule
\textbf{Average} & \textbf{7.39} & 11.10 & 80.70 & 72.60 & 87.89 \\
\bottomrule
\end{tabular}
\end{table*}

%==============================================================================
% Bibliography
%==============================================================================
\bibliographystyle{elsarticle-num}
\begin{thebibliography}{32}

\bibitem{coello2007evolutionary}
C.~A.~C. Coello, G.~B. Lamont, D.~A. Van~Veldhuizen,
Evolutionary Algorithms for Solving Multi-Objective Problems,
Springer, 2007.

\bibitem{deb2002fast}
K.~Deb, A.~Pratap, S.~Agarwal, T.~Meyarivan,
A fast and elitist multiobjective genetic algorithm: NSGA-II,
IEEE Trans. Evol. Comput. 6~(2) (2002) 182--197.

\bibitem{blank2020pymoo}
J.~Blank, K.~Deb,
pymoo: Multi-objective optimization in Python,
IEEE Access 8 (2020) 89497--89509.

\bibitem{fortin2012deap}
F.-A. Fortin, et~al.,
DEAP: Evolutionary algorithms made easy,
J. Mach. Learn. Res. 13 (2012) 2171--2175.

\bibitem{benitez2019jmetalpy}
A.~Ben{\'\i}tez-Hidalgo, et~al.,
jMetalPy: A Python framework for multi-objective optimization with metaheuristics,
Swarm Evol. Comput. 51 (2019) 100598, doi:10.1016/j.swevo.2019.100598.

\bibitem{hadka2015platypus}
D.~Hadka,
Platypus: a free and open source Python library for multiobjective optimization,
\url{https://github.com/Project-Platypus/Platypus}, accessed 2026.

\bibitem{lopez2016irace}
M.~L{\'o}pez-Ib{\'a}{\~n}ez, et~al.,
The irace package: Iterated racing for automatic algorithm configuration,
Oper. Res. Perspect. 3 (2016) 43--58.

\bibitem{li2017hyperband}
L.~Li, K.~Jamieson, G.~DeSalvo, A.~Rostamizadeh, A.~Talwalkar,
Hyperband: A novel bandit-based approach to hyperparameter optimization,
J. Mach. Learn. Res. 18~(185) (2017) 1--52.

\bibitem{fialho2010analyzing}
{\'A}.~Fialho, M.~Schoenauer, M.~Sebag,
Analyzing bandit-based adaptive operator selection mechanisms,
Ann. Math. Artif. Intell. 60 (2010) 25--64.

\bibitem{lam2015numba}
S.~K. Lam, et~al.,
Numba: A LLVM-based Python JIT compiler,
in: LLVM-HPC, 2015.

\bibitem{jax2018github}
J.~Bradbury, et~al.,
JAX: composable transformations of Python+NumPy programs,
\url{https://github.com/google/jax}, 2018.

\bibitem{moocore}
moocore: multi-objective optimization core library,
\url{https://github.com/multi-objective/moocore}, accessed 2026.

\bibitem{zhang2007moea}
Q.~Zhang, H.~Li,
MOEA/D: A multiobjective evolutionary algorithm based on decomposition,
IEEE Trans. Evol. Comput. 11~(6) (2007) 712--731.

\bibitem{deb2014evolutionary}
K.~Deb, H.~Jain,
An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting,
IEEE Trans. Evol. Comput. 18~(4) (2014) 577--601.

\bibitem{beume2007sms}
N.~Beume, B.~Naujoks, M.~Emmerich,
SMS-EMOA: Multiobjective selection based on dominated hypervolume,
Eur. J. Oper. Res. 181~(3) (2007) 1653--1669.

\bibitem{zitzler2001spea2}
E.~Zitzler, M.~Laumanns, L.~Thiele,
SPEA2: Improving the strength Pareto evolutionary algorithm,
TIK-Report 103, ETH Zurich, 2001.

\bibitem{zitzler2004indicator}
E.~Zitzler, S.~K{\"u}nzli,
Indicator-based selection in multiobjective search,
in: PPSN, Springer, 2004, pp.~832--842.

\bibitem{nebro2009smpso}
A.~J. Nebro, J.~J. Durillo, C.~A.~C. Coello Coello,
SMPSO: A new PSO-based metaheuristic for multi-objective optimization,
in: IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making, 2009, pp.~66--73.

\bibitem{panichella2019adaptive}
A.~Panichella,
An adaptive evolutionary algorithm based on non-Euclidean geometry for many-objective optimization,
in: GECCO, ACM, 2019, pp.~595--603.

\bibitem{cheng2016reference}
R.~Cheng, et~al.,
A reference vector guided evolutionary algorithm for many-objective optimization,
IEEE Trans. Evol. Comput. 20~(5) (2016) 773--791.

\bibitem{zitzler2000zdt}
E.~Zitzler, K.~Deb, L.~Thiele,
Comparison of multiobjective evolutionary algorithms: Empirical results,
Evol. Comput. 8~(2) (2000) 173--195.

\bibitem{deb2002dtlz}
K.~Deb, L.~Thiele, M.~Laumanns, E.~Zitzler,
Scalable multi-objective optimization test problems,
in: Congress on Evolutionary Computation (CEC), 2002, pp.~825--830.

\bibitem{huband2006wfg}
S.~Huband, P.~Hingston, L.~Barone, L.~While,
A review of multiobjective test problems and a scalable test problem toolkit,
IEEE Trans. Evol. Comput. 10~(5) (2006) 477--506.

\bibitem{zitzler2003performance}
E.~Zitzler, L.~Thiele, M.~Laumanns, C.~M. Fonseca, V.~G. da Fonseca,
Performance assessment of multiobjective optimizers: An analysis and review,
IEEE Trans. Evol. Comput. 7~(2) (2003) 117--132.

\bibitem{wilcoxon1945}
F.~Wilcoxon,
Individual comparisons by ranking methods,
Biometrics Bull. 1~(6) (1945) 80--83.

\end{thebibliography}

\end{document}
