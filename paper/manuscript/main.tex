% VAMOS Paper: Swarm and Evolutionary Computation (Elsevier)
% -----------------------------------------------------------------------------
% NOTE: Several tables in this manuscript are auto-generated from CSV artifacts:
%   - `experiments/benchmark_paper.csv`:
%       - `paper/04_update_paper_tables_from_csv.py` updates runtime tables
%         (`tab:backends`, `tab:frameworks_perf`, `tab:detailed_backends`,
%          `tab:detailed_comparison`).
%       - `paper/05_run_statistical_tests.py` updates statistical tables
%         (`tab:stats_hypervolume`, `tab:hv_equivalence_summary`,
%          `tab:hv_robustness_summary`, `tab:hv_equivalence_ci`).
%   - `experiments/ablation_aos_racing_tuner.csv`:
%       - `paper/06_update_ablation_tables_from_csv.py` updates ablation tables
%         (`tab:ablation_runtime`, `tab:ablation_hypervolume`).
%   - `experiments/ablation_aos_anytime.csv`:
%       - `paper/10_update_anytime_tables_from_csv.py` updates anytime tables
%         (`tab:anytime_hv_10000`, `tab:anytime_hv_20000`, `tab:anytime_auc`).
%   - `experiments/tuned_nsgaii_resolved.json`:
%       - `paper/09_update_tuned_config_from_json.py` updates the tuned-config table
%         (`tab:racing_tuned_config`).
% -----------------------------------------------------------------------------
\documentclass[preprint,12pt]{elsarticle}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\hypersetup{hypertexnames=false}
\makeatletter
\g@addto@macro\UrlBreaks{\do\.\do\_\do\-\do\/}
\makeatother
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}

% Code listing style
\lstset{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red!70!black},
  showstringspaces=false,
  frame=single,
  breaklines=true,
  numbers=left,
  numberstyle=\tiny\color{gray}
}

\journal{Swarm and Evolutionary Computation}

\newcommand{\VAMOS}{VAMOS}
\newcommand{\HV}{\mathrm{HV}}

\begin{document}

\begin{frontmatter}

\title{VAMOS: A Vectorized Architecture for Multiobjective Optimization Studies\\A High-Performance Python Framework with Adaptive and Auto-Configurable Components}

\author[uma]{First Author\corref{cor1}}
\ead{author@university.edu}
\author[uma]{Second Author}

\cortext[cor1]{Corresponding author}
\address[uma]{Department of Computer Science, University, Country}

\begin{abstract}
\sloppy
Python has become the de facto platform for empirical studies in multi-objective optimization, yet most multi-objective evolutionary algorithm (MOEA) libraries remain dominated by object-oriented designs that incur substantial interpreter overhead in the inner loop. We introduce \VAMOS{} (Vectorized Architecture for Multiobjective Optimization Studies), a research-oriented framework that represents populations as contiguous numerical arrays and factors algorithmic logic from numerical kernels. \VAMOS{} provides interchangeable compute backends (NumPy, Numba JIT, and optional C and JAX accelerators), a unified configuration API, and nine MOEAs (NSGA-II and NSGA-III, MOEA/\allowbreak D, SMS-EMOA, SPEA2, IBEA, SMPSO, AGE-MOEA, and RVEA). Two adaptive modules complement the core algorithms: (i) an irace-inspired racing tuner with optional multi-fidelity (Hyperband-style) warm-starting, and (ii) bandit-based adaptive operator selection for configurable operator portfolios. We also release a reproducible benchmarking pipeline that aligns problem definitions and operator settings across common Python frameworks and reports runtime and \emph{normalized} hypervolume computed with fixed reference Pareto fronts. Across the ZDT, DTLZ, and WFG suites, \VAMOS{} achieves up to an order-of-magnitude reduction in runtime relative to established libraries while maintaining competitive solution quality.
\end{abstract}

\begin{highlights}
\item Vectorized MOEA core with pluggable NumPy/Numba/C/JAX kernels.
\item Cross-framework benchmarking with semantic configuration alignment.
\item Order-of-magnitude runtime gains with competitive normalized hypervolume.
\item Adaptive modules: bandit-based operator selection and racing-based tuning.
\item Reproducible pipeline with fixed reference fronts for normalized HV.
\end{highlights}

\begin{keyword}
Multi-objective optimization \sep Evolutionary algorithms \sep Benchmarking \sep Hypervolume \sep Python framework \sep Vectorization
\end{keyword}

\end{frontmatter}

%==============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

Multi-objective optimization problems (MOPs) arise whenever trade-offs exist between conflicting objectives, requiring the computation of a diverse approximation of the Pareto-optimal set rather than a single optimum~\cite{coello2007evolutionary}. Multi-objective evolutionary algorithms (MOEAs) remain a dominant approach due to their robustness, anytime behavior, and ability to return sets of non-dominated solutions.

In Python, practical benchmarking of MOEAs is often constrained by two friction points. First, the ecosystem is fragmented: libraries differ in problem APIs, operator configuration, result formats, and runtime instrumentation~\cite{blank2020pymoo,fortin2012deap,benitez2019jmetalpy}. Second, many implementations rely on per-solution objects and Python-level loops inside selection, variation, and survival, making the overhead of the interpreter comparable to (or larger than) the cost of typical benchmark functions. This limitation becomes acute for large-scale studies with many seeds, large populations, and repeated statistical tests.

\VAMOS{} is designed to address both issues with a single guiding principle: \emph{keep algorithm state in dense arrays and push hot loops into vectorized kernels}. The framework also targets reproducibility in experimental methodology: we provide a benchmark runner that standardizes problem dimensions and operators across multiple Python frameworks, and a hypervolume protocol with fixed reference Pareto fronts so that solution-quality metrics are comparable across runs and frameworks.

The contributions of this work are:
\begin{enumerate}
  \item \textbf{Vectorized MOEA core}: a unified, array-based internal representation with pluggable compute kernels (NumPy/Numba/C/JAX) for selection, variation, and survival.
  \item \textbf{Modular algorithm suite}: nine MOEAs implemented on top of shared components (termination, archives, evaluation backends, and metrics).
  \item \textbf{Adaptive components}: an irace-inspired racing tuner with optional multi-fidelity warm-starting, and bandit-based adaptive operator selection (AOS) for operator portfolios.
  \item \textbf{Reproducible benchmarking}: a pipeline that aligns cross-framework configurations and computes \emph{normalized} hypervolume against fixed reference Pareto fronts distributed with the repository.
\end{enumerate}

Figure~\ref{fig:at_glance} summarizes the high-level architecture, execution backends, and experimental pipeline that structure the remainder of the paper.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.98\textwidth]{figures/AT_GLANCE.png}
  \caption{\VAMOS{} at a glance: architecture layers, execution backends, benchmark pipeline, and main experimental artifacts.}
  \label{fig:at_glance}
\end{figure}

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

\subsection{Python MOEA frameworks}

Several mature Python libraries exist for MOEAs. \textbf{pymoo}~\cite{blank2020pymoo} provides a rich collection of algorithms and operators with an emphasis on modularity. \textbf{DEAP}~\cite{fortin2012deap} is a general evolutionary computation toolkit; while flexible, users typically assemble MOEAs manually. \textbf{jMetalPy}~\cite{benitez2019jmetalpy} ports the jMetal architecture to Python and emphasizes end-to-end experimentation (observer-driven progress/visualization, a ``laboratory'' workflow, and built-in statistical post-processing with LaTeX export). It additionally offers modules for constrained, dynamic, and preference-based variants, as well as parallel evaluation via Spark/Dask. However, it relies on object-centric representations and does not address cross-toolkit semantic drift. We build upon this methodological focus on rigorous experimental studies and complement it with an array-based execution model and a fairness-oriented benchmarking protocol that aligns problem definitions and operator semantics across toolkits. \textbf{Platypus}~\cite{hadka2015platypus} provides a lightweight API and a set of classic MOEAs.

Table~\ref{tab:frameworks} summarizes high-level differences relevant to this work. The main distinction is the internal representation: \VAMOS{} consistently uses dense arrays for populations and objectives, while most alternatives operate on per-solution objects. This difference enables kernel-based acceleration and reduces Python overhead in the inner loop.

\begin{table}[htbp]
\centering
\caption{Comparison of Python multi-objective optimization frameworks (high-level capabilities).}
\label{tab:frameworks}
{\scriptsize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lccccc}
\toprule
\textbf{Framework} & \textbf{MOEAs} & \textbf{Array core} & \textbf{Accel. kernels} & \textbf{Auto-conf.} & \textbf{AOS} \\
\midrule
pymoo~\cite{blank2020pymoo} & 8+ & Partial & No & No & No \\
DEAP~\cite{fortin2012deap} & Custom & No & No & No & No \\
jMetalPy~\cite{benitez2019jmetalpy} & 11+ & No & No & No & No \\
Platypus~\cite{hadka2015platypus} & 8+ & No & No & No & No \\
\VAMOS{} & 9 & Yes & Numba/C/JAX & Yes & Yes \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Automatic algorithm configuration}

Automatic algorithm configuration (AAC), also known as algorithm configuration or hyperparameter optimization, seeks a parameter vector $\theta \in \Theta$ that optimizes expected performance on a target distribution of problem instances and random seeds. In evolutionary multi-objective optimization, $\Theta$ typically includes categorical choices (e.g., operator families), continuous parameters (e.g., SBX/PM distribution indices), and budget-related settings (e.g., population size), and performance is commonly defined by a quality indicator under a fixed evaluation budget or by time-to-target.

Because evaluating a configuration requires running a stochastic algorithm, AAC methods aim to allocate most of the budget to promising candidates while discarding clearly inferior ones early. Racing approaches are widely used in evolutionary computation: configurations are evaluated on an increasing number of instance$\times$seed blocks and are progressively eliminated using non-parametric tests. irace~\cite{lopez2016irace} popularized \emph{iterated} racing, where new configurations are sampled from adaptive distributions learned around elites across successive iterations.

Multi-fidelity strategies further improve sample efficiency by treating the evaluation budget (number of evaluations, wall-clock time, or problem size) as a fidelity level. Successive halving and Hyperband~\cite{li2017hyperband} evaluate many candidates cheaply and promote only a fraction to larger budgets, yielding strong anytime behavior. \VAMOS{} adopts an irace-inspired racing loop and optionally integrates Hyperband-style promotion; when fidelity is increased, runs can be continued from warm-start checkpoints to reduce redundant computation between levels.

\subsection{Adaptive operator selection}

Adaptive operator selection (AOS) addresses the complementary problem of \emph{online} adaptation: instead of selecting a single static configuration $\theta$ before a run, the algorithm chooses among a set of operators while the search progresses. AOS mechanisms are typically characterized by (i) an operator pool, (ii) a credit assignment scheme that maps an operator application to a reward signal, and (iii) a selection policy that updates operator probabilities based on accumulated credit.

Multi-armed bandit formulations are popular because they directly manage the exploration--exploitation trade-off and yield simple, robust policies such as $\epsilon$-greedy~\cite{sutton2018reinforcement}, UCB variants~\cite{auer2002finite}, EXP3~\cite{auer2002nonstochastic}, and Thompson sampling~\cite{thompson1933likelihood}. In multi-objective settings, rewards cannot rely solely on a single scalar fitness and are often based on inexpensive proxies (e.g., survival, dominance relations, or archive updates) rather than expensive indicators. \VAMOS{} provides a portfolio-driven AOS interface and integrates bandit policies into NSGA-II, using reward signals derived from survival and non-dominated insertions~\cite{fialho2010analyzing}.

%==============================================================================
\section{VAMOS Framework}
\label{sec:framework}
%==============================================================================

\subsection{Architecture and data model}

\VAMOS{} is organized into four layers:
\begin{enumerate}
  \item \textbf{Foundation}: problem definitions, kernels, metrics, and archives;
  \item \textbf{Engine}: algorithm implementations and shared components;
  \item \textbf{Adaptation}: tuning and AOS modules;
  \item \textbf{Experiment}: CLI, benchmarking utilities, visualization, and reporting.
\end{enumerate}

The primary user entry point is a unified \texttt{optimize(...)} API; explicit configuration objects are reserved for fully specified, reproducible runs and advanced algorithm customization.
For minimal-code users, a guided CLI quickstart generates a reusable config file, with domain-flavored templates that map to common workflows.

The core design choice is to represent a population as a pair of dense arrays: decision variables $X \in \mathbb{R}^{N \times n}$ and objective values $F \in \mathbb{R}^{N \times m}$ for population size $N$, $n$ decision variables, and $m$ objectives. Variation and survival operate on these arrays (or views thereof), enabling vectorized operations (NumPy) and compilation of hot loops (Numba) without per-individual Python objects.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/architecture.png}
\caption{\VAMOS{} four-layer architecture.}
\label{fig:architecture}
\end{figure}

\subsection{Compute kernels}

Algorithmic logic (e.g., \emph{NSGA-II selects survivors by non-dominated sorting and crowding distance}) is separated from numerical kernels that implement the corresponding computations. \VAMOS{} provides multiple backends:
\begin{itemize}
  \item \textbf{NumPy}: baseline backend using vectorized array operations.
  \item \textbf{Numba}: JIT compilation of hot operators and utilities, reducing Python overhead~\cite{lam2015numba}.
  \item \textbf{MooCore}: optional C-backed kernels for Pareto ranking and hypervolume-based utilities (via MooCore)~\cite{moocore}.
  \item \textbf{JAX}: optional backend for parts of the pipeline and autodiff-based constraints, enabling GPU/TPU acceleration when available~\cite{jax2018github}.
\end{itemize}

Backends are selected through configuration (e.g., \texttt{engine="numba"}) and are transparent to algorithm code.

In addition to accelerating arithmetic kernels, \VAMOS{} reduces overhead by minimizing Python-side allocations: variation operators can reuse pre-allocated workspaces (e.g., scratch buffers for SBX/PM) and algorithms maintain state as contiguous arrays. This design is particularly beneficial in inner-loop routines such as tournament selection, non-dominated sorting, crowding-distance computation, and archive updates.

\subsection{Algorithm suite}

\VAMOS{} implements nine multi-objective algorithms covering dominance-based, decomposition-based, indicator-based, and reference-vector paradigms. Table~\ref{tab:algorithms} summarizes the suite.

\begin{table}[htbp]
\centering
\caption{Multi-objective algorithms implemented in \VAMOS{}.}
\label{tab:algorithms}
\begin{tabular}{llc}
\toprule
\textbf{Algorithm} & \textbf{Category} & \textbf{Reference} \\
\midrule
NSGA-II & Dominance-based & \cite{deb2002fast} \\
NSGA-III & Reference-point & \cite{deb2014evolutionary} \\
MOEA/D & Decomposition & \cite{zhang2007moea} \\
SMS-EMOA & Indicator-based & \cite{beume2007sms} \\
SPEA2 & Archive-based & \cite{zitzler2001spea2} \\
IBEA & Indicator-based & \cite{zitzler2004indicator} \\
SMPSO & Swarm-based & \cite{nebro2009smpso} \\
AGE-MOEA & Adaptive geometry & \cite{panichella2019adaptive} \\
RVEA & Reference vectors & \cite{cheng2016reference} \\
\bottomrule
\end{tabular}
\end{table}

AGE-MOEA follows the adaptive geometry estimation survival scheme of Panichella~\cite{panichella2019adaptive}: after non-dominated sorting, the first front is normalized by extreme points, a curvature-dependent $p$-norm is estimated, and survival uses geometry-aware crowding based on Minkowski distances. RVEA follows Cheng et~al.~\cite{cheng2016reference} with angle-penalized distance (APD) survival and periodic reference-vector adaptation using ideal/nadir scaling; we therefore enforce \texttt{pop\_size} to match the number of simplex-lattice reference directions induced by \texttt{n\_partitions}.

\subsection{Execution model and evaluation backends}

Algorithms in \VAMOS{} expose a uniform \texttt{run(problem, termination, seed, eval\_backend, live\_viz)} interface, and several algorithms also support an \emph{ask--tell} loop for streaming or interactive use. Objective evaluations are dispatched through a small evaluation-backend protocol, enabling serial evaluation for controlled benchmarking and parallel evaluation (e.g., multiprocessing or Dask-based) when objective functions are expensive. Separating evaluation from algorithmic state keeps the algorithm core deterministic under a fixed random seed and reduces the risk of framework-specific parallelism becoming a confounder in empirical comparisons.

\begin{algorithm}[htbp]
\caption{Vectorized NSGA-II execution in \VAMOS{} (high-level).}
\label{alg:nsgaii}
\begin{algorithmic}[1]
\State Sample initial population $X \in \mathbb{R}^{N \times n}$ within bounds
\State Evaluate objectives $F \gets f(X)$
\While{termination criterion not met}
  \State Select mating indices $P$ via tournament on (rank, crowding)
  \State Generate offspring $X' \gets \mathcal{V}(X_P)$ (crossover, mutation, repair)
  \State Evaluate offspring $F' \gets f(X')$
  \State Merge: $(X,F) \gets ([X;X'], [F;F'])$
  \State Compute ranks and crowding distances
  \State Select survivors of size $N$ (rank--crowding)
\EndWhile
\State \Return final population and derived non-dominated set
\end{algorithmic}
\end{algorithm}

\subsection{Variation pipeline and encodings}

\VAMOS{} supports real-valued, binary, and permutation encodings through a variation pipeline that composes selection, crossover, mutation, and optional repair operators. For continuous domains, the default configuration uses simulated binary crossover (SBX) and polynomial mutation (PM), with mutation probability typically set to $1/n$ where $n$ is the number of decision variables. Encodings are normalized internally to ensure consistent operator resolution and avoid silent configuration mismatches.

\subsection{Archives, metrics, and analysis tooling}

The framework includes optional external archives (with configurable pruning strategies) and common indicators. Hypervolume computation can use optimized backends when installed, but \VAMOS{} also provides fallback implementations for low-dimensional cases. Results are returned in a unified result object that supports plotting and post-hoc analysis (e.g., filtering non-dominated solutions and exporting fronts).

\subsection{Adaptive operator selection (AOS)}

Adaptive operator selection aims to reduce the need for a single, globally optimal operator setting by adapting variation operators online as the search progresses. In practice, different operators can be beneficial at different stages (e.g., exploration early vs.\ exploitation near convergence) or on different regions of the Pareto front. The goal of \VAMOS{}' AOS module is to provide a lightweight, reproducible mechanism that improves robustness across problems and reduces manual trial-and-error in operator tuning.

In \VAMOS{}, AOS is portfolio-based: users specify an \emph{operator pool} (a finite set of \emph{arms}, where each arm is a crossover--mutation pipeline with fixed hyperparameters) and a \emph{selection policy} that chooses which arm to apply during the run. We model this decision as a non-stationary multi-armed bandit problem and implement several standard policies: $\epsilon$-greedy~\cite{sutton2018reinforcement}, UCB1~\cite{auer2002finite} (and a sliding-window variant for non-stationary rewards~\cite{garivier2011ucb}), EXP3~\cite{auer2002nonstochastic}, and Thompson sampling~\cite{thompson1933likelihood}. To ensure continued exploration and avoid premature lock-in, policies can enforce an exploration floor by mixing learned probabilities with uniform sampling. Importantly, AOS does not modify the operator implementations themselves; it only changes \emph{which} predefined operator pipeline is applied over time.

\begin{table}[htbp]
\centering
\caption{Bandit selection policies available for AOS in \VAMOS{} (bounded rewards $r_t \in [0,1]$). $\hat{\mu}_i$ denotes the empirical mean reward of arm $i$, $n_i$ the pull count, $t$ the total pulls, and $K$ the number of arms.}
\label{tab:aos_policies}
\begin{tabular}{l|p{0.58\linewidth}|p{0.25\linewidth}}
\toprule
\textbf{Policy} & \textbf{Selection / update (sketch)} & \textbf{Key hyperparameters} \\
\midrule
$\epsilon$-greedy~\cite{sutton2018reinforcement} &
With probability $\epsilon$, sample an arm uniformly; otherwise select $a=\arg\max_i \hat{\mu}_i$. Update $\hat{\mu}_a$ by incremental mean. &
$\epsilon$; warm-up min-usage $m$ \\
UCB1~\cite{auer2002finite} &
Select $a=\arg\max_i \hat{\mu}_i + c\,\sqrt{\log(t)/n_i}$; update $\hat{\mu}_a$ by incremental mean. &
$c$; warm-up min-usage $m$ \\
Sliding-window UCB~\cite{garivier2011ucb} &
As UCB1, but compute $\hat{\mu}_i$ and $n_i$ from the last $W$ rewards per arm (non-stationary setting). &
$c$; window size $W$; min-usage $m$ \\
EXP3~\cite{auer2002nonstochastic} &
Maintain weights $w_i$ and sample $a$ from $p_i=(1-\gamma)\frac{w_i}{\sum_j w_j}+\frac{\gamma}{K}$. Update $w_a \leftarrow w_a \exp\!\left(\frac{\gamma}{K}\frac{r_t}{p_a}\right)$. &
$\gamma$ \\
Thompson sampling~\cite{thompson1933likelihood} &
For each arm, sample $\theta_i \sim \mathrm{Beta}(\alpha_i,\beta_i)$ with $(\alpha_i,\beta_i)$ derived from reward history; select $a=\arg\max_i \theta_i$. (Optionally use a sliding window.) &
min-usage $m$; optional window size $W$ \\
\bottomrule
\end{tabular}
\end{table}

In the NSGA-II integration used in this paper, the controller selects one arm per generation and applies it to generate the entire offspring batch for that generation. After survival selection, the chosen arm receives credit based on inexpensive, generation-level signals. The default signals are (i) the offspring survival rate and (ii) the rate of non-dominated insertions; optionally, a small hypervolume-change term can be included when it is inexpensive to compute (up to three objectives in our implementation). These rewards update the policy and thereby the arm-selection probabilities for subsequent generations. This design keeps the AOS overhead small relative to the variation and survival kernels and avoids dependence on expensive indicators in the inner loop.

\begin{algorithm}[htbp]
\caption{Adaptive operator selection loop (NSGA-II integration in \VAMOS{}).}
\label{alg:aos}
\begin{algorithmic}[1]
\Require Operator pool (arms) $\mathcal{A}=\{a_1,\dots,a_K\}$, bandit policy $\pi$, reward weights $w=(w_s,w_d,w_h)$
\State Initialize population $(X,F)$
\For{generation $t=0,1,\dots$ until termination}
  \State Select arm $a_t \gets \pi.\textsc{SelectArm}()$
  \State Generate offspring using $a_t$: $(X',F') \gets \textsc{VariationAndEvaluate}(X; a_t)$
  \State Survival selection: $(X,F) \gets \textsc{NSGA2Survival}((X,F),(X',F'))$
  \State Credit signals: $s_t \gets \#\text{offspring survivors}/\#\text{offspring}$, $d_t \gets \#\text{offspring in ND}/\#\text{offspring}$
  \State Optional indicator proxy (two objectives): $h_t \gets \textsc{Normalize}(\Delta \HV)$; otherwise $h_t \gets 0$
  \State Aggregate reward: $r_t \gets w_s s_t + w_d d_t + w_h h_t$ \Comment{$r_t$ clipped to $[0,1]$}
  \State Update policy: $\pi.\textsc{Update}(a_t, r_t)$
\EndFor
\State \Return final population (and derived non-dominated set)
\end{algorithmic}
\end{algorithm}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.98\textwidth]{figures/AOS.png}
  \caption{Adaptive operator selection (AOS) in \VAMOS{}. Each arm corresponds to a predefined crossover--mutation pipeline; the bandit policy selects an arm each generation, applies it to generate offspring, and updates its selection probabilities using reward feedback. Reward components can combine survival and non-dominated insertion rates and an optional hypervolume-change proxy when inexpensive (up to three objectives), e.g., $r=w_s\,\text{survival}+w_d\,\text{ND-insertions}+w_h\,\Delta \HV$.}
  \label{fig:aos}
\end{figure}

\subsection{Racing-based hyperparameter tuning}

Automatic algorithm configuration targets the complementary \emph{offline} problem: before running an algorithm at scale, select a configuration $\theta$ (operator families and their parameters, population size, and other hyperparameters) that performs well on a representative set of problem instances. The goal of \VAMOS{}' racing tuner is to make this process reproducible and sample-efficient by concentrating evaluations on promising configurations while quickly discarding clearly inferior ones.

\VAMOS{} provides an irace-inspired racing tuner~\cite{lopez2016irace} that iteratively samples candidate configurations and evaluates them across \emph{instance$\times$seed blocks}. A block is defined as one training instance paired with one random seed; in each stage, every surviving configuration is evaluated on the same next block (``common random numbers''), producing a score $s(\theta; i, \xi, B)$ for configuration $\theta$ on instance $i$ with seed $\xi$ under an evaluation budget $B$. Scores are aggregated across the blocks seen so far (mean aggregation by default) to obtain an estimate of expected performance, $\hat{S}(\theta)$. Racing then repeatedly eliminates clearly inferior candidates while ensuring that at least a minimum number of survivors remain. Using shared blocks yields paired comparisons, reduces noise from stochasticity, and makes elimination decisions more stable than unpaired evaluation.

In \VAMOS{}, elimination is staged and conservative: if too few blocks have been observed, or if score histories are not aligned across candidates, the tuner falls back to rank-based elimination that discards a fixed fraction of the worst candidates. Once enough common blocks have been evaluated, the tuner can apply a statistical gate to avoid premature pruning: first, a global test checks whether any performance differences are detectable across candidates; if so, paired tests against the incumbent best configuration are performed with multiple-comparison correction (Holm step-down), eliminating only candidates that are significantly worse. This design mirrors the ``evaluate on shared blocks $\rightarrow$ eliminate'' structure of irace while keeping the implementation simple and reproducible.

For sample efficiency, the tuner optionally supports a multi-fidelity strategy (Hyperband-style successive halving~\cite{li2017hyperband}) by treating the evaluation budget as a fidelity level. Candidates are first evaluated at low budgets; only the top fraction is promoted to higher budgets. When fidelity increases, \VAMOS{} can warm-start from checkpoints of lower-fidelity runs, reusing partial trajectories instead of restarting from scratch. Figure~\ref{fig:racing_tuner} summarizes the workflow.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.98\textwidth]{figures/RACING.png}
  \caption{Racing-based hyperparameter tuning in \VAMOS{}: sample candidate configurations, evaluate them on instance$\times$seed blocks, eliminate underperformers via racing, optionally promote survivors to higher budgets (with warm-start checkpoints), and output the best configuration and a complete history.}
  \label{fig:racing_tuner}
\end{figure}

\begin{algorithm}[htbp]
\caption{Racing-based tuning with optional multi-fidelity.}
\label{alg:racing}
\begin{algorithmic}[1]
\Require Parameter space $\Theta$, training instances $\mathcal{I}$, training seeds $\Xi$, budgets $B_1 < \dots < B_L$ (or $L{=}1$), max initial candidates $N_0$, total experiment budget $E_{\max}$, promotion ratio $\rho$, minimum survivors per fidelity $N_{\min}$, elimination fraction $\phi$, significance level $\alpha$
\State Sample initial candidates $\mathcal{C} \gets \{\theta_1,\dots,\theta_{N_0}\}$ from $\Theta$ (optionally including injected defaults)
\State Build a randomized block schedule $\mathcal{B} \gets$ permutations of $(i,\xi) \in \mathcal{I}\times\Xi$
\State Initialize score histories $H(\theta) \gets [\ ]$ for all $\theta\in\mathcal{C}$
\For{$\ell = 1,\dots,L$} \Comment{Fidelity levels (successive halving when $L>1$)}
  \For{each block $(i,\xi)\in\mathcal{B}$}
    \For{each alive $\theta \in \mathcal{C}$ \textbf{in parallel}}
      \State Evaluate and record: $H(\theta) \mathrel{+}= s(\theta; i,\xi,B_\ell)$ (optionally warm-start from checkpoints)
    \EndFor
    \State Stop if total evaluations exceed $E_{\max}$
  \EndFor
  \State Aggregate per candidate: $\hat{S}(\theta) \gets \textsc{Aggregate}(H(\theta))$ (mean by default)
  \If{$L>1$ and $\ell < L$} \Comment{Multi-fidelity promotion}
    \State Keep top $\max(\lceil\rho|\mathcal{C}|\rceil, N_{\min})$ candidates by $\hat{S}$; eliminate the rest
  \Else \Comment{Single-fidelity racing (or final fidelity)}
    \State Eliminate conservatively using rank-based pruning and/or paired tests vs the incumbent with Holm correction (subject to a minimum number of common blocks)
  \EndIf
\EndFor
\State \Return selected $\theta^\star = \arg\max_{\theta\in\mathcal{C}} \hat{S}(\theta)$ and full history $\{H(\theta)\}$
\end{algorithmic}
\end{algorithm}

%==============================================================================
\section{Experimental Evaluation}
\label{sec:experiments}
%==============================================================================

\subsection{Benchmark suite and configuration alignment}

We evaluate runtime and solution quality on widely used synthetic benchmarks: ZDT1--4 and ZDT6~\cite{zitzler2000zdt} (two objectives), DTLZ1--7~\cite{deb2002dtlz} (three objectives), and WFG1--9~\cite{huband2006wfg} (two objectives). Problem dimensionalities are fixed to standard definitions: for example, DTLZ2/3/4 use $n = m + k - 1$ with $m=3$ and $k=10$ (thus $n=12$). Non-standard DTLZ dimensionalities are permitted but explicitly flagged to avoid accidental comparisons against canonical settings.

Figure~\ref{fig:protocol} summarizes the benchmark protocol and semantic-alignment workflow used to ensure cross-framework fairness and to support the ``same quality, faster'' claim.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.98\textwidth]{figures/PROTOCOL.png}
  \caption{Benchmark protocol and semantic alignment: map equivalent NSGA-II settings across frameworks, validate objective definitions by sampling decision vectors within bounds, run independent seeds under a fixed evaluation budget, compute normalized hypervolume using fixed reference fronts, and apply corrected statistical analysis (Holm, equivalence, robustness).}
  \label{fig:protocol}
\end{figure}

To ensure comparability across frameworks, we standardize the algorithmic settings of NSGA-II: population size $N=100$, SBX crossover probability $p_c=1.0$ and distribution index $\eta_c=20$, polynomial mutation distribution index $\eta_m=20$ and mutation probability $p_m=1/n$, tournament selection, and rank--crowding survival. For each framework we map these settings to the closest available implementation.

For SMS-EMOA we follow the same protocol and align what we can: population size $N=100$, SBX ($p_c=1.0$, $\eta_c=20$), polynomial mutation ($p_m=1/n$, $\eta_m=20$), random parent selection, and a steady-state $(\mu+1)$ loop (one offspring per iteration). However, the hypervolume-contribution reference point is not defined consistently across toolkits: \VAMOS{} and jMetalPy compute contributions in raw objective space with an adaptive reference point $r = \max(F) + 1$, while pymoo normalizes objectives and uses a fixed shifted reference point $r = \mathbf{1} + \epsilon$ (we set $\epsilon=1$). We therefore report the cross-framework SMS-EMOA comparison with this caveat.

For MOEA/D we align population size $N=100$, weight vectors (uniform for two objectives; a shared $W3D\_100$ set for three objectives), neighborhood size $T=20$, neighborhood selection probability $\delta=0.9$, and replacement limit $\eta=20$ (effectively unrestricted, matching pymoo's update rule). We use PBI scalarization with $\theta=5$ and a DE/current/1/bin-style crossover ($CR=1.0$, $F=0.5$) followed by polynomial mutation ($p_m=1/n$, $\eta_m=20$), mapping these settings to the closest available implementation in each toolkit.

This workflow follows standard practice in comparative MOEA studies (independent runs, indicator computation, and statistical testing) and is consistent with the experimentation methodology described in jMetalPy~\cite{benitez2019jmetalpy}. Our main extension is to make the \emph{inputs} to this workflow comparable across toolkits by enforcing semantic alignment of both problem definitions and operator probabilities.

Crucially, we align \emph{semantics} (not only parameter names) and enforce consistent benchmark definitions. Two examples illustrate why this matters. First, in \textbf{pymoo} the polynomial mutation operator exposes both an individual-level probability (\texttt{prob}) and a per-variable probability (\texttt{prob\_var}); to match the standard ``$p_m=1/n$ per decision variable'' setting used by DEAP/jMetalPy, we set \texttt{prob}=1 and \texttt{prob\_var}=1/n. Second, some libraries ship benchmark problems under canonical names but with library-specific domains; for instance, Platypus' built-in ZDT base class uses $[0,1]^n$, whereas ZDT4 is defined with $x_1\in[0,1]$ and $x_{2..n}\in[-5,5]$.

Therefore, for toolkits or APIs where problem definitions differ (notably DEAP and Platypus for ZDT4), we evaluate individuals using a shared reference implementation of the benchmark function and bounds (validated against pymoo) while still using each framework's NSGA-II implementation and operators. For WFG we adopt pymoo's canonical parameterization for two objectives ($k=4$, $l=20$ for $n=24$) and pass parameters explicitly when a toolkit exposes different defaults; in Platypus we wrap pymoo's WFG definitions to avoid definition drift. While full equivalence cannot be guaranteed due to framework-specific details (tie-breaking, boundary handling, duplicate elimination, etc.), these choices aim to make the comparison as fair as possible by removing confounders unrelated to algorithmic overhead and numerical kernels.

As an additional validity check, we sample 64 random decision vectors per problem uniformly within the specified bounds and verify that objective values match across implementations within a numerical tolerance ($\mathrm{rtol}=10^{-6}$, $\mathrm{atol}=10^{-8}$). This guards against silent benchmark-definition drift (e.g., differing domains or parameterizations) that could invalidate hypervolume comparisons.

\subsection{Runtime measurement}

All runtimes are measured wall-clock using high-resolution timers and include algorithm overhead and objective evaluations. We report medians across 30 independent seeds and provide per-problem and per-family summaries.

For Numba-accelerated backends, we distinguish two timing policies. \textbf{Warm} timings exclude one-time JIT compilation by performing a short warmup run in the same process before starting the timer (2{,}000 warmup evaluations; not counted in the reported time). \textbf{Cold} timings measure the first run from a fresh process and therefore include JIT compilation overhead. Unless otherwise stated, we report warm timings to reflect steady-state throughput; cold-start costs can be reproduced with the provided scripts.

Experiments were run on a workstation with an Intel(R) Core(TM) Ultra 9 185H CPU (16 cores, 22 logical processors) and 32\,GB RAM, running Microsoft Windows 11 Home (build 26200). We used Python 3.12.3 and evaluated \VAMOS{} 0.1.0 (NumPy 2.3.5 with OpenBLAS 0.3.30; Numba 0.63.1) against pymoo 0.6.1.6 and DEAP 1.4.3. jMetalPy and Platypus were evaluated from local source checkouts at commits \texttt{fecb85c} and \texttt{fe7aeff}, respectively.

\subsection{Normalized hypervolume protocol}
\label{sec:hv}

Hypervolume (HV) measures the Lebesgue measure of the region dominated by an approximation set $A$ with respect to a reference point $r$ (for minimization)~\cite{zitzler2003performance}:
\begin{equation}
  \HV(A, r) = \lambda\left(\bigcup_{a \in A} [a_1, r_1] \times \cdots \times [a_m, r_m]\right).
\end{equation}
Because HV is scale-dependent and sensitive to the choice of $r$, naive implementations can produce values that are not comparable across runs (e.g., if $r$ is expanded per run) and can be misleading when dominated solutions are included. To ensure a stable and comparable quality metric, we adopt the following protocol:
\begin{enumerate}
  \item \textbf{Non-dominated filtering}: for every framework we compute HV on the final non-dominated set only.
  \item \textbf{Fixed reference Pareto fronts}: for each benchmark problem we store a dense reference Pareto front $P_{\text{ref}}$ in \texttt{data/reference\_fronts/}. For ZDT and DTLZ (except DTLZ7) these fronts are generated analytically; for DTLZ7 and WFG we generate $P_{\text{ref}}$ by dense sampling followed by non-dominated filtering.
  \item \textbf{Fixed reference point}: we set $r = \max(P_{\text{ref}}) + \epsilon$ (component-wise) with a small $\epsilon$, and we disallow run-dependent expansion.
  \item \textbf{Normalization}: we report $\HV(A,r) / \HV(P_{\text{ref}},r)$, yielding a dimensionless score typically in $[0,1]$ and reducing sensitivity to objective scaling.
\end{enumerate}
For WFG problems, the reference front is necessarily an approximation; we use large Pareto-set samples and non-dominated filtering, and we increase density for particularly sensitive cases (e.g., WFG2) to avoid underestimating $\HV(P_{\text{ref}}, r)$ and obtaining normalized HV slightly above~1.

\subsection{VAMOS backend comparison}

Table~\ref{tab:backends} reports median runtime for \VAMOS{} backends aggregated by problem family.

\begin{table}[htbp]
\centering
\caption{VAMOS backend comparison: median runtime (seconds) by problem family.}
\label{tab:backends}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Backend} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Average} \\
\midrule
Numba & \textbf{2.62} & \textbf{3.46} & \textbf{4.03} & \textbf{3.37} \\
MooCore & 2.69 & 3.52 & 4.16 & 3.45 \\
NumPy & 5.18 & 6.82 & 7.37 & 6.46 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cross-framework comparison}

Table~\ref{tab:frameworks_perf} compares \VAMOS{} against other Python frameworks on runtime. \VAMOS{} is evaluated with its accelerated backend (Numba) for the headline comparison, while additional backends are reported in the appendix.
We repeat the same runtime protocol for NSGA-II variants (steady-state and external archive), SMS-EMOA (steady-state, one offspring per iteration), and MOEA/D (decomposition with differential-evolution variation), reported in Tables~\ref{tab:frameworks_perf_nsgaii_ss} and~\ref{tab:frameworks_perf_nsgaii_archive} and Tables~\ref{tab:frameworks_perf_smsemoa} and~\ref{tab:frameworks_perf_moead}.

\begin{table}[htbp]
\centering
\caption{Median runtime (seconds) by problem family across all frameworks.}
\label{tab:frameworks_perf}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Framework} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Average} \\
\midrule
VAMOS & \textbf{2.62} & \textbf{3.46} & \textbf{4.03} & \textbf{3.37} \\
pymoo & 3.22 & 3.86 & 4.56 & 3.88 \\
DEAP & 14.99 & 21.42 & 43.72 & 26.71 \\
jMetalPy & 11.55 & 14.91 & 43.94 & 23.47 \\
Platypus & 20.28 & 29.01 & 50.16 & 33.15 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Steady-state NSGA-II.}
Some toolkits expose a steady-state (incremental-replacement) variant of NSGA-II, typically implemented as a $(\mu+1)$ loop with one offspring per iteration and one survivor replaced.
We benchmark this mode by aligning NSGA-II settings across frameworks and setting the offspring batch size to~1.
In \VAMOS{}, we configure steady-state mode with \texttt{offspring\_size}=1 and \texttt{replacement\_size}=1; in pymoo we set \texttt{n\_offsprings}=1; in jMetalPy we set \texttt{offspring\_population\_size}=1; and for DEAP we implement the corresponding one-offspring loop around \texttt{selNSGA2}.
Platypus does not provide an equivalent steady-state NSGA-II API, so we omit it from this comparison.

\input{frameworks_perf_nsgaii_ss.tex}

\paragraph{NSGA-II with external archive.}
To assess the overhead of maintaining an external elite set, we also benchmark NSGA-II with an \emph{unbounded} external archive that accumulates non-dominated solutions throughout the run.
For this variant, we compute normalized hypervolume on the archive contents (rather than the final population) to reflect the larger retained approximation set.
In \VAMOS{}, we enable an external archive with \texttt{unbounded=True} and size initialized to the population size; in pymoo and DEAP we maintain a separate unbounded archive updated from the evolving population/offspring; in jMetalPy we use an NSGA-II implementation with an archive hook; and in Platypus we pass an \texttt{Archive()} object.

\input{frameworks_perf_nsgaii_archive.tex}

\input{frameworks_perf_smsemoa.tex}
\input{frameworks_perf_moead.tex}

\subsection{Statistical analysis}
\label{sec:stats}

We use the Wilcoxon signed-rank test~\cite{wilcoxon1945} at significance level $\alpha=0.05$ to compare paired normalized-hypervolume distributions across seeds, per problem. While jMetalPy~\cite{benitez2019jmetalpy} illustrates both frequentist and Bayesian statistical workflows (and exemplifies Wilcoxon rank-sum tests in a multi-algorithm setting), our design is explicitly paired by construction (shared seeds), so we report the signed-rank variant as a minimal, widely used test under this protocol. Runtime results are summarized descriptively using medians across seeds.
The same statistical procedure is applied to SMS-EMOA (steady-state, one offspring per iteration) and MOEA/D (DE-based variation with decomposition), and the corresponding per-problem tables are given in Tables~\ref{tab:stats_hypervolume_smsemoa} and~\ref{tab:stats_hypervolume_moead}.

Because we perform per-problem hypothesis tests, we apply Holm's step-down correction to control the family-wise error rate over the set of comparisons in each table. To support a stronger ``same quality'' claim (as opposed to merely ``no significant difference''), we additionally perform equivalence testing for normalized hypervolume using a practical margin of $\pm 1\%$ relative difference. We estimate 90\% confidence intervals for the paired relative HV difference via bootstrap resampling of seeds; equivalence is declared when the confidence interval lies entirely within the margin. Finally, we report robustness using the interquartile range (IQR) and the fraction of seeds reaching a near-best threshold, defined per problem as $0.99$ times the best median HV observed among frameworks.

\begin{table}[htbp]
\centering
\caption{Normalized hypervolume comparison with Wilcoxon signed-rank test results (Holm-corrected).}
\label{tab:stats_hypervolume}
\begin{tabular}{l|rr|r|l}
\toprule
\textbf{Problem} & \textbf{VAMOS (numba)} & \textbf{pymoo} & \textbf{p-value} & \textbf{Sig.} \\
\midrule
zdt1 & 0.991 & \textbf{0.991} & 1.000 &  \\
zdt2 & \textbf{0.982} & 0.982 & 1.000 &  \\
zdt3 & 0.996 & \textbf{0.996} & 1.000 &  \\
zdt4 & 0.990 & \textbf{0.990} & 1.000 &  \\
zdt6 & 0.980 & \textbf{0.983} & <0.0001 & $\checkmark$ \\
dtlz1 & \textbf{0.915} & 0.914 & 1.000 &  \\
dtlz2 & 0.786 & \textbf{0.790} & 0.527 &  \\
dtlz3 & \textbf{0.707} & 0.651 & 1.000 &  \\
dtlz4 & \textbf{0.799} & 0.792 & 1.000 &  \\
dtlz5 & 0.964 & \textbf{0.964} & 1.000 &  \\
dtlz6 & 0.408 & \textbf{0.440} & 1.000 &  \\
dtlz7 & 0.883 & \textbf{0.888} & 1.000 &  \\
wfg1 & 0.437 & \textbf{0.457} & 1.000 &  \\
wfg2 & 0.982 & \textbf{0.983} & 1.000 &  \\
wfg3 & 0.974 & \textbf{0.974} & 1.000 &  \\
wfg4 & 0.960 & \textbf{0.961} & 1.000 &  \\
wfg5 & 0.826 & \textbf{0.827} & 0.291 &  \\
wfg6 & 0.833 & \textbf{0.846} & 1.000 &  \\
wfg7 & 0.966 & \textbf{0.966} & 1.000 &  \\
wfg8 & 0.744 & \textbf{0.745} & 1.000 &  \\
wfg9 & \textbf{0.934} & 0.714 & 1.000 &  \\
\bottomrule
\end{tabular}
\end{table}

\input{stats_hypervolume_smsemoa.tex}
\input{stats_hypervolume_moead.tex}

\begin{table}[htbp]
\centering
\caption{Normalized hypervolume equivalence summary (paired bootstrap 90\% CI; equivalence margin $\pm 1\%$ relative). Generated by \texttt{paper/05\_run\_statistical\_tests.py}.}
\label{tab:hv_equivalence_summary}
\begin{tabular}{l|rrr}
\toprule
\textbf{Framework} & \textbf{Eq.\ problems} & \textbf{Non-inf.\ problems} & \textbf{Median $\Delta$HV (\%)} \\
\midrule
pymoo & 14/21 & 16/21 & -0.03 \\
DEAP & 14/21 & 16/21 & -0.04 \\
jMetalPy & 15/21 & 16/21 & -0.04 \\
Platypus & 13/21 & 16/21 & -0.10 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Robustness summary for normalized hypervolume across seeds (median across problems). ``Near-best'' is defined per problem as HV $\ge 0.99 \cdot \max$ median(HV) among frameworks. Generated by \texttt{paper/05\_run\_statistical\_tests.py}.}
\label{tab:hv_robustness_summary}
\begin{tabular}{l|rr}
\toprule
\textbf{Framework} & \textbf{Median IQR(HV)} & \textbf{Median \% seeds near-best} \\
\midrule
VAMOS (numba) & 0.005 & 86.7 \\
pymoo & 0.006 & 93.3 \\
DEAP & 0.004 & 100.0 \\
jMetalPy & 0.006 & 90.0 \\
Platypus & 0.004 & 96.7 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation: AOS and racing tuner}
\label{sec:ablation}

To evaluate the practical impact of \VAMOS{}' optional adaptation modules beyond the cross-framework baseline, we run an internal ablation study on the same benchmark suite as the runtime experiments, using a 50k evaluation budget per run. We compare four \VAMOS{} variants of NSGA-II in a $2\times2$ factorial design (tuned vs.\ untuned) $\times$ (AOS disabled vs.\ enabled):
\begin{enumerate}
  \item \textbf{Baseline}: the fixed-operator configuration used in the cross-framework benchmark (SBX crossover and polynomial mutation).
  \item \textbf{Baseline + AOS}: the same configuration augmented with adaptive operator selection (AOS), where an $\epsilon$-greedy bandit~\cite{sutton2018reinforcement} selects between a small portfolio of diverse real-coded variation pipelines (e.g., SBX+PM, PCX+PM, UNDX+Gaussian, Simplex+UniformReset, BLX-$\alpha$+Cauchy). Unless otherwise stated, we use $\epsilon=0.05$ with a deterministic per-run RNG seed. Rewards combine survival and non-dominated insertion rates with a lightweight per-generation hypervolume-change term aligned with our normalized HV protocol (fixed reference point and normalization) to reduce the risk of selecting operators that improve selection statistics while degrading Pareto-front quality.
  \item \textbf{Racing-tuned}: a single tuned NSGA-II configuration obtained by the racing tuner on a broader training set (\texttt{zdt4}, \texttt{zdt6}, \texttt{dtlz2}, \texttt{dtlz6}, \texttt{dtlz7}, \texttt{wfg1}, \texttt{wfg9}) using multi-fidelity racing up to the full evaluation budget (10k $\rightarrow$ 30k $\rightarrow$ 50k evaluations). The search space enforces $\texttt{pop\_size}\ge 100$ and sets offspring size as a fraction of population size via $\texttt{offspring\_ratio}\in\{0.25,0.5,0.75,1.0\}$ (thus $\texttt{offspring\_size}\le\texttt{pop\_size}$), and uses a dimension-normalized mutation probability factor for better cross-problem transfer. We tune on 10 training seeds, selecting a single configuration by the final-fidelity score and evaluating it on the full benchmark suite without per-problem re-tuning.
  \item \textbf{Racing-tuned + AOS}: the racing-tuned configuration augmented with adaptive operator selection (AOS), where the operator portfolio includes the tuned variation pipeline plus additional variation pipelines derived from the top-5 tuner candidates, to quantify whether online operator adaptation provides additional benefit beyond offline tuning.
\end{enumerate}
The selected racing-tuned configuration is exported for reproducibility and summarized in Table~\ref{tab:racing_tuned_config}. All variants are evaluated for 30 independent seeds and reported using the same normalized hypervolume protocol (Section~\ref{sec:hv}) and wall-clock runtime measurement setup. Tables~\ref{tab:ablation_runtime} and \ref{tab:ablation_hypervolume} summarize the final-budget ablation outcomes aggregated by benchmark family.

To better capture online adaptation effects under large evaluation budgets, we additionally report anytime performance by recording normalized hypervolume at fixed evaluation checkpoints (5k, 10k, 20k, 50k evaluations). To make early-budget differences explicit, we report median normalized hypervolume at 10k and 20k evaluations (Tables~\ref{tab:anytime_hv_10000} and \ref{tab:anytime_hv_20000}). From the same checkpoints we compute AUC(HV vs.\ evals) to summarize overall anytime behavior (Table~\ref{tab:anytime_auc}).

We additionally log per-generation AOS arm selections and summarize, for representative problems, the most-selected operator in coarse budget stages (Figure~\ref{fig:aos_switching}).

\begin{figure}[htbp]
\centering
\small
% BEGIN AOS_SWITCHING_TABLE
\textbf{Racing-tuned + AOS}\\
\begin{tabular}{l|ccc}
\toprule
\textbf{Problem} & \textbf{$\le 10k$} & \textbf{$10k\!-\!20k$} & \textbf{$20k\!-\!50k$} \\
\midrule
\texttt{zdt4} & \texttt{sbx+cauchy} (91\%) & \texttt{sbx+cauchy} (89\%) & \texttt{sbx+cauchy} (89\%) \\
\texttt{dtlz3} & \texttt{sbx+cauchy} (82\%) & \texttt{sbx+linked\_polynomial} (78\%) & \texttt{sbx+linked\_polynomial} (96\%) \\
\texttt{wfg9} & \texttt{sbx+cauchy} (94\%) & \texttt{sbx+cauchy} (90\%) & \texttt{sbx+cauchy} (89\%) \\
\bottomrule
\end{tabular}
% END AOS_SWITCHING_TABLE
\caption{Stage-wise AOS operator usage: most-selected operator and median selection fraction across seeds for representative problems (generated by \texttt{paper/10\_update\_anytime\_tables\_from\_csv.py} from \texttt{experiments/ablation\_aos\_trace.csv}).}
\label{fig:aos_switching}
\end{figure}

\begin{table}[htbp]
\centering
\caption{Racing-tuned NSGA-II configuration used in the ablation study (generated by \texttt{paper/09\_update\_tuned\_config\_from\_json.py}).}
\label{tab:racing_tuned_config}
\begin{tabular}{l|p{0.68\linewidth}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\texttt{pop\_size} & 153 \\
\texttt{offspring\_size} & 153 \\
Crossover & \texttt{sbx(eta=7.94, prob=0.97)} \\
Mutation & \texttt{cauchy(gamma=0.16, prob=0.10, prob\_factor=1.73)} \\
Selection & \texttt{tournament(pressure=4)} \\
Repair & \texttt{clip} \\
External archive & \texttt{unbounded (size=153)} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Ablation study: median runtime (seconds) by problem family for \VAMOS{} variants (generated by \texttt{paper/06\_update\_ablation\_tables\_from\_csv.py}).}
\label{tab:ablation_runtime}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Variant} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Average} \\
\midrule
Baseline & \textbf{0.89} & \textbf{1.00} & \textbf{1.02} & \textbf{0.97} \\
Baseline + AOS & 1.36 & 1.39 & 1.84 & 1.53 \\
Racing-tuned & 11.01 & 10.94 & 5.61 & 9.19 \\
Racing-tuned + AOS & 11.78 & 15.70 & 6.29 & 11.25 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Ablation study: median normalized hypervolume by problem family for \VAMOS{} variants (generated by \texttt{paper/06\_update\_ablation\_tables\_from\_csv.py}).}
\label{tab:ablation_hypervolume}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Variant} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Average} \\
\midrule
Baseline & 0.990 & 0.799 & 0.934 & 0.908 \\
Baseline + AOS & 0.990 & 0.880 & 0.923 & 0.931 \\
Racing-tuned & \textbf{0.993} & \textbf{0.917} & \textbf{0.947} & \textbf{0.952} \\
Racing-tuned + AOS & 0.992 & 0.909 & 0.945 & 0.948 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Anytime ablation: median normalized hypervolume at 10,000 evaluations (generated by \texttt{paper/10\_update\_anytime\_tables\_from\_csv.py}).}
\label{tab:anytime_hv_10000}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Variant} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Average} \\
\midrule
Baseline & 0.890 & 0.729 & 0.890 & 0.836 \\
Baseline + AOS & 0.867 & 0.735 & 0.876 & 0.826 \\
Racing-tuned & 0.985 & 0.804 & \textbf{0.895} & 0.895 \\
Racing-tuned + AOS & \textbf{0.986} & \textbf{0.817} & 0.892 & \textbf{0.898} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Anytime ablation: median normalized hypervolume at 20,000 evaluations (generated by \texttt{paper/10\_update\_anytime\_tables\_from\_csv.py}).}
\label{tab:anytime_hv_20000}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Variant} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Average} \\
\midrule
Baseline & 0.976 & 0.788 & 0.921 & 0.895 \\
Baseline + AOS & 0.978 & 0.798 & 0.902 & 0.892 \\
Racing-tuned & 0.989 & 0.832 & \textbf{0.926} & 0.916 \\
Racing-tuned + AOS & \textbf{0.989} & \textbf{0.839} & 0.924 & \textbf{0.917} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Anytime ablation: median AUC of normalized hypervolume over evaluations (HV checkpoints integrated over the evaluation budget; generated by \texttt{paper/10\_update\_anytime\_tables\_from\_csv.py}).}
\label{tab:anytime_auc}
\begin{tabular}{l|rrr|r}
\toprule
\textbf{Variant} & \textbf{ZDT} & \textbf{DTLZ} & \textbf{WFG} & \textbf{Average} \\
\midrule
Baseline & 0.836 & 0.732 & 0.863 & 0.810 \\
Baseline + AOS & 0.830 & 0.738 & 0.852 & 0.807 \\
Racing-tuned & 0.913 & 0.778 & \textbf{0.874} & 0.855 \\
Racing-tuned + AOS & \textbf{0.917} & \textbf{0.786} & 0.871 & \textbf{0.858} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Discussion}

The observed speedups are primarily due to (i) the array-based representation that removes per-solution Python overhead, and (ii) JIT compilation of hot operators and utilities in the Numba backend. The normalized hypervolume protocol ensures that performance gains are not obtained at the expense of solution quality and avoids artifacts caused by run-dependent reference points.

\subsection{Threats to validity}

Despite careful alignment of dimensions, budgets, problem definitions, and operator settings, cross-framework comparisons remain subject to several threats: (i) operator implementations may differ in subtle details (e.g., boundary handling, duplicate elimination, tie-breaking in survival), (ii) random number generation and seeding semantics vary across libraries, and (iii) library-specific overheads (data conversion, object materialization) may affect measured runtime. We mitigate major sources of unfairness by enforcing consistent benchmark bounds (e.g., ZDT4) and matching operator semantics where APIs differ (e.g., mutation probability in pymoo).

Our cross-framework comparison is deliberately scoped to Python MOEA libraries whose inner loops are predominantly executed in Python. Some libraries (e.g., PyGMO, which provides Python bindings to the C++ PaGMO2 library) are performance-oriented but are not directly comparable under our primary research question, which targets reductions in interpreter overhead via array-based representations and kernel acceleration. In PyGMO, both algorithms and benchmark problems may execute fully in C++, while user-defined problems can also be evaluated via callbacks into Python; these modes yield materially different runtime profiles and can shift the dominant cost to foreign-function or language-boundary overhead. In addition, PyGMO supports island-model parallelism and asynchronous execution; accounting for these features would require additional normalization beyond the single-process budgeted protocol used here. For these reasons, we treat PyGMO and PaGMO2 as an orthogonal C++ baseline and leave their inclusion to future work with an explicit language/runtime study design.

For hypervolume, reference fronts for problems without closed-form Pareto fronts are necessarily approximations; while we generate them densely and fix them across runs, remaining discrepancies can affect absolute normalized values, especially on WFG problems. We therefore treat normalized HV primarily as a comparative metric under a fixed protocol, and we report distributions across multiple seeds rather than relying on single-run outcomes.

\subsection{Reproducibility}

All experiments in this paper are reproducible from the repository:
\begin{itemize}
  \item Cross-framework benchmark:\newline \texttt{python}\ \path{paper/01_run_paper_benchmark.py}
  \item AOS/racing-tuner ablation:\newline \texttt{python}\ \path{paper/02_run_ablation_aos_racing_tuner.py}
\end{itemize}

We control the number of seeds, evaluation budget, and workers with \path{VAMOS_N_SEEDS}, \path{VAMOS_N_EVALS}, and \path{VAMOS_N_JOBS}; unless otherwise stated, we use \path{VAMOS_N_SEEDS=30} and \path{VAMOS_N_EVALS=100000}.

Paper tables are regenerated by:
\begin{itemize}
  \item Runtime tables:\newline \texttt{python}\ \path{paper/04_update_paper_tables_from_csv.py}
  \item Statistical tables:\newline \texttt{python}\ \path{paper/05_run_statistical_tests.py}
  \item Ablation tables:\newline \texttt{python}\ \path{paper/06_update_ablation_tables_from_csv.py}
  \item Anytime ablation tables:\newline \texttt{python}\ \path{paper/10_update_anytime_tables_from_csv.py}
  \item Tuned-configuration summary:\newline \texttt{python}\ \path{paper/09_update_tuned_config_from_json.py}
\end{itemize}

The tuned configuration used in the ablation study is stored at:\newline \path{experiments/tuned_nsgaii_resolved.json}.

Benchmark reports also emit jMetalPy-compatible laboratory outputs (\path{QualityIndicatorSummary.csv}, Wilcoxon tables, and boxplots) under:\newline \path{summary/lab/}, following the laboratory summary format described by jMetalPy~\cite{benitez2019jmetalpy}.
Reference Pareto fronts used for normalized hypervolume are stored in \path{data/reference_fronts/} and can be regenerated with the provided scripts under \path{experiments/scripts/} (front density is configurable via environment variables, e.g., \path{VAMOS_REF_WFG2_POINTS}).

As a development aid, we used a large language model (LLM) to accelerate mechanical refactors and surface configuration mismatches across frameworks; proposed changes were applied as explicit diffs, reviewed by the authors, and accepted only if they passed automated checks. This does not affect the experimental results, which are produced solely by the released code and fixed reference data.

%==============================================================================
\section{Conclusions and Future Work}
\label{sec:conclusions}
%==============================================================================

We presented \VAMOS{}, a high-performance Python framework for multi-objective optimization studies. By combining a vectorized core with pluggable compute kernels, \VAMOS{} reduces inner-loop overhead and enables scalable benchmarking. The framework also includes adaptive modules for automatic configuration and operator selection, and a reproducible experimental pipeline with fixed reference Pareto fronts for normalized hypervolume reporting.

\subsection{Future Work}
\begin{itemize}
  \item \textbf{Many-objective benchmarking}: extend the experimental protocol beyond $m=3$ with scalable indicators.
  \item \textbf{Deeper GPU integration}: expand JAX-based kernels and enable GPU-friendly operators.
  \item \textbf{Richer AOS rewards}: incorporate indicator deltas (e.g., HV improvements) in a principled credit assignment scheme.
\end{itemize}

%==============================================================================
\appendix
\section{Reference fronts and detailed benchmark results}
\label{app:detailed}
%==============================================================================

The repository includes dense reference Pareto fronts for all benchmark problems used for normalized hypervolume computation (\path{data/reference_fronts/}). These fronts are generated analytically when closed forms are available (ZDT and most DTLZ problems) and by dense sampling followed by non-dominated filtering for cases where the Pareto front is disconnected or defined implicitly (DTLZ7 and WFG). For WFG2 we use a higher sampling density (configurable via \path{VAMOS_REF_WFG2_POINTS}) to stabilize HV normalization.

Tables~\ref{tab:detailed_backends} and~\ref{tab:detailed_comparison} report per-problem median runtimes.

\begin{table}[htbp]
\centering
\caption{Detailed VAMOS backend comparison: median runtime (seconds) per problem.}
\label{tab:detailed_backends}
\begin{tabular}{l|rrr}
\toprule
\textbf{Problem} & \textbf{Numba} & \textbf{MooCore} & \textbf{NumPy} \\
\midrule
dtlz1 & \textbf{2.67} & 2.74 & 5.48 \\
dtlz2 & \textbf{3.01} & 3.04 & 5.83 \\
dtlz3 & 3.60 & \textbf{3.44} & 7.54 \\
dtlz4 & \textbf{3.72} & 3.95 & 7.00 \\
dtlz5 & 3.74 & \textbf{3.68} & 6.72 \\
dtlz6 & \textbf{3.90} & 4.04 & 7.64 \\
dtlz7 & 3.90 & \textbf{3.73} & 7.12 \\
wfg1 & 4.17 & \textbf{4.13} & 7.91 \\
wfg2 & \textbf{3.90} & 3.95 & 7.49 \\
wfg3 & \textbf{3.81} & 3.96 & 7.15 \\
wfg4 & \textbf{3.93} & 4.06 & 7.65 \\
wfg5 & \textbf{4.31} & 4.46 & 7.98 \\
wfg6 & \textbf{4.40} & 4.74 & 7.63 \\
wfg7 & 3.85 & \textbf{3.83} & 7.18 \\
wfg8 & \textbf{4.18} & 4.35 & 7.30 \\
wfg9 & \textbf{3.85} & 4.18 & 6.23 \\
zdt1 & 2.63 & \textbf{2.56} & 4.82 \\
zdt2 & 2.79 & \textbf{2.72} & 5.24 \\
zdt3 & \textbf{2.76} & 2.81 & 5.19 \\
zdt4 & \textbf{2.41} & 2.51 & 5.33 \\
zdt6 & \textbf{2.44} & 2.51 & 5.16 \\
\midrule
\textbf{Average} & \textbf{3.52} & 3.59 & 6.65 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table*}[htbp]
\tiny
\centering
\caption{Detailed comparison of median runtime (seconds) across all frameworks.}
\label{tab:detailed_comparison}
\begin{tabular}{l|rrrrr}
\toprule
\textbf{Problem} & \textbf{VAMOS} & \textbf{pymoo} & \textbf{DEAP} & \textbf{jMetalPy} & \textbf{Platypus} \\
\midrule
dtlz1 & \textbf{2.67} & 3.21 & 16.91 & 11.39 & 19.82 \\
dtlz2 & \textbf{3.01} & 3.38 & 17.62 & 12.23 & 25.72 \\
dtlz3 & \textbf{3.60} & 4.44 & 24.92 & 16.82 & 31.03 \\
dtlz4 & \textbf{3.72} & 3.84 & 22.67 & 15.15 & 29.29 \\
dtlz5 & \textbf{3.74} & 4.03 & 21.78 & 14.65 & 30.89 \\
dtlz6 & \textbf{3.90} & 4.52 & 22.82 & 15.28 & 30.98 \\
dtlz7 & \textbf{3.90} & 4.22 & 20.99 & 16.41 & 30.35 \\
wfg1 & \textbf{4.17} & 4.76 & 31.37 & 30.35 & 38.26 \\
wfg2 & \textbf{3.90} & 4.55 & 46.68 & 48.26 & 54.91 \\
wfg3 & \textbf{3.81} & 4.18 & 44.07 & 44.80 & 50.92 \\
wfg4 & \textbf{3.93} & 4.42 & 26.17 & 24.93 & 33.42 \\
wfg5 & \textbf{4.31} & 4.61 & 28.82 & 25.57 & 41.68 \\
wfg6 & \textbf{4.40} & 4.91 & 125.18 & 122.00 & 133.25 \\
wfg7 & \textbf{3.85} & 4.31 & 37.17 & 37.01 & 45.73 \\
wfg8 & \textbf{4.18} & 4.68 & 79.09 & 86.14 & 83.92 \\
wfg9 & \textbf{3.85} & 4.75 & 166.45 & 179.86 & 169.99 \\
zdt1 & \textbf{2.63} & 2.93 & 14.43 & 11.54 & 20.77 \\
zdt2 & \textbf{2.79} & 3.28 & 15.40 & 12.01 & 21.46 \\
zdt3 & \textbf{2.76} & 3.08 & 15.54 & 11.81 & 21.94 \\
zdt4 & \textbf{2.41} & 3.12 & 15.41 & 11.23 & 17.52 \\
zdt6 & \textbf{2.44} & 3.51 & 14.46 & 10.39 & 18.20 \\
\midrule
\textbf{Average} & \textbf{3.52} & 4.03 & 38.47 & 36.09 & 45.24 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[htbp]
\tiny
\centering
\caption{Per-problem paired bootstrap 90\% confidence intervals (CI) for the relative normalized hypervolume difference $\Delta = (\HV_{\VAMOS} - \HV_{\text{fw}})/\HV_{\text{fw}}$, reported in percent. Equivalence margin is $\pm 1\%$. Generated by \texttt{paper/05\_run\_statistical\_tests.py}.}
\label{tab:hv_equivalence_ci}
\begin{tabular}{l|rrrr}
\toprule
\textbf{Problem} & \textbf{pymoo CI} & \textbf{DEAP CI} & \textbf{jMetalPy CI} & \textbf{Platypus CI} \\
\midrule
dtlz1 & $[-0.43,+0.29]$ & $[-0.48,+0.05]$ & $[-0.72,+0.29]$ & $[-0.07,+0.85]$ \\
dtlz2 & $[-1.17,+0.11]$ & $[-1.31,-0.51]$ & $[-1.16,+0.30]$ & $[-0.94,+0.38]$ \\
dtlz3 & $[+1.98,+26.42]$ & $[-6.27,+12.61]$ & $[-10.86,+9.93]$ & $[+9.90,+40.20]$ \\
dtlz4 & $[-0.35,+1.10]$ & $[-0.77,+0.03]$ & $[-0.91,+0.78]$ & $[-1.02,+84.30]$ \\
dtlz5 & $[-0.09,+0.08]$ & $[-0.07,+0.11]$ & $[-0.15,+0.11]$ & $[-0.19,+0.02]$ \\
dtlz6 & $[-16.50,-3.07]$ & $[-20.05,+6.16]$ & $[-14.09,+19.49]$ & $[-36.54,-15.09]$ \\
dtlz7 & $[-1.08,+0.21]$ & $[-0.67,-0.22]$ & $[-0.82,+0.57]$ & $[-1.02,+21.20]$ \\
wfg1 & $[-10.74,+6.15]$ & $[-13.93,+1.49]$ & $[-5.66,+3.29]$ & $[+41.87,+81.97]$ \\
wfg2 & $[-0.21,+0.11]$ & $[-0.14,+0.08]$ & $[-0.11,+0.16]$ & $[-0.23,+0.05]$ \\
wfg3 & $[-0.15,+0.03]$ & $[-0.19,+0.08]$ & $[-0.19,+0.41]$ & $[-0.28,-0.12]$ \\
wfg4 & $[-0.32,-0.04]$ & $[-0.34,-0.02]$ & $[-0.29,+0.03]$ & $[-0.64,-0.29]$ \\
wfg5 & $[-0.22,+0.04]$ & $[-0.11,+0.04]$ & $[-0.11,+0.03]$ & $[-0.43,-0.24]$ \\
wfg6 & $[-1.09,+0.30]$ & $[-1.59,+1.52]$ & $[-2.31,+0.46]$ & $[-1.85,+0.97]$ \\
wfg7 & $[-0.10,+0.08]$ & $[-0.21,+0.03]$ & $[-0.13,+0.09]$ & $[-0.20,-0.08]$ \\
wfg8 & $[-0.54,+0.47]$ & $[+0.23,+1.07]$ & $[-0.44,+0.53]$ & $[-3.75,-3.09]$ \\
wfg9 & $[-0.13,+0.61]$ & $[+0.09,+31.54]$ & $[-0.28,+1.11]$ & $[-0.58,+1.75]$ \\
zdt1 & $[-0.03,+0.00]$ & $[-0.04,+0.01]$ & $[-0.02,+0.01]$ & $[-0.09,-0.06]$ \\
zdt2 & $[-0.01,+0.02]$ & $[-0.06,-0.02]$ & $[-0.03,+0.04]$ & $[-0.13,-0.08]$ \\
zdt3 & $[-0.01,+0.00]$ & $[-0.02,+0.01]$ & $[-0.01,+0.01]$ & $[-0.03,-0.02]$ \\
zdt4 & $[-0.10,+0.04]$ & $[-0.09,+0.10]$ & $[-0.05,+0.08]$ & $[-0.02,+0.05]$ \\
zdt6 & $[-0.30,-0.20]$ & $[+0.11,+0.19]$ & $[+0.06,+0.15]$ & $[-0.40,-0.32]$ \\
\bottomrule
\end{tabular}
\end{table*}

%==============================================================================
% Bibliography
%==============================================================================
\bibliographystyle{elsarticle-num}
\begin{thebibliography}{32}

\bibitem{coello2007evolutionary}
C.~A.~C. Coello, G.~B. Lamont, D.~A. Van~Veldhuizen,
Evolutionary Algorithms for Solving Multi-Objective Problems,
Springer, 2007.

\bibitem{deb2002fast}
K.~Deb, A.~Pratap, S.~Agarwal, T.~Meyarivan,
A fast and elitist multiobjective genetic algorithm: NSGA-II,
IEEE Trans. Evol. Comput. 6~(2) (2002) 182--197.

\bibitem{blank2020pymoo}
J.~Blank, K.~Deb,
pymoo: Multi-objective optimization in Python,
IEEE Access 8 (2020) 89497--89509.

\bibitem{fortin2012deap}
F.-A. Fortin, et~al.,
DEAP: Evolutionary algorithms made easy,
J. Mach. Learn. Res. 13 (2012) 2171--2175.

\bibitem{benitez2019jmetalpy}
A.~Ben{\'\i}tez-Hidalgo, et~al.,
jMetalPy: A Python framework for multi-objective optimization with metaheuristics,
Swarm Evol. Comput. 51 (2019) 100598, doi:10.1016/j.swevo.2019.100598.

\bibitem{hadka2015platypus}
D.~Hadka,
Platypus: a free and open source Python library for multiobjective optimization,
\url{https://github.com/Project-Platypus/Platypus}, accessed 2026.

\bibitem{lopez2016irace}
M.~L{\'o}pez-Ib{\'a}{\~n}ez, et~al.,
The irace package: Iterated racing for automatic algorithm configuration,
Oper. Res. Perspect. 3 (2016) 43--58.

\bibitem{li2017hyperband}
L.~Li, K.~Jamieson, G.~DeSalvo, A.~Rostamizadeh, A.~Talwalkar,
Hyperband: A novel bandit-based approach to hyperparameter optimization,
J. Mach. Learn. Res. 18~(185) (2017) 1--52.

\bibitem{fialho2010analyzing}
{\'A}.~Fialho, M.~Schoenauer, M.~Sebag,
Analyzing bandit-based adaptive operator selection mechanisms,
Ann. Math. Artif. Intell. 60 (2010) 25--64.

\bibitem{sutton2018reinforcement}
R.~S. Sutton, A.~G. Barto,
Reinforcement Learning: An Introduction,
MIT Press, 2018.

\bibitem{auer2002finite}
P.~Auer, N.~Cesa-Bianchi, P.~Fischer,
Finite-time analysis of the multiarmed bandit problem,
Mach. Learn. 47 (2002) 235--256.

\bibitem{auer2002nonstochastic}
P.~Auer, N.~Cesa-Bianchi, Y.~Freund, R.~E. Schapire,
The nonstochastic multiarmed bandit problem,
SIAM J. Comput. 32~(1) (2002) 48--77.

\bibitem{thompson1933likelihood}
W.~R. Thompson,
On the likelihood that one unknown probability exceeds another in view of the evidence of two samples,
Biometrika 25 (1933) 285--294.

\bibitem{garivier2011ucb}
A.~Garivier, E.~Moulines,
On upper-confidence bound policies for switching bandit problems,
in: Proc. Algorithmic Learning Theory (ALT), 2011, pp.~174--188.

\bibitem{lam2015numba}
S.~K. Lam, et~al.,
Numba: A LLVM-based Python JIT compiler,
in: LLVM-HPC, 2015.

\bibitem{jax2018github}
J.~Bradbury, et~al.,
JAX: composable transformations of Python+NumPy programs,
\url{https://github.com/google/jax}, 2018.

\bibitem{moocore}
moocore: multi-objective optimization core library,
\url{https://github.com/multi-objective/moocore}, accessed 2026.

\bibitem{zhang2007moea}
Q.~Zhang, H.~Li,
MOEA/D: A multiobjective evolutionary algorithm based on decomposition,
IEEE Trans. Evol. Comput. 11~(6) (2007) 712--731.

\bibitem{deb2014evolutionary}
K.~Deb, H.~Jain,
An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting,
IEEE Trans. Evol. Comput. 18~(4) (2014) 577--601.

\bibitem{beume2007sms}
N.~Beume, B.~Naujoks, M.~Emmerich,
SMS-EMOA: Multiobjective selection based on dominated hypervolume,
Eur. J. Oper. Res. 181~(3) (2007) 1653--1669.

\bibitem{zitzler2001spea2}
E.~Zitzler, M.~Laumanns, L.~Thiele,
SPEA2: Improving the strength Pareto evolutionary algorithm,
TIK-Report 103, ETH Zurich, 2001.

\bibitem{zitzler2004indicator}
E.~Zitzler, S.~K{\"u}nzli,
Indicator-based selection in multiobjective search,
in: PPSN, Springer, 2004, pp.~832--842.

\bibitem{nebro2009smpso}
A.~J. Nebro, J.~J. Durillo, C.~A.~C. Coello Coello,
SMPSO: A new PSO-based metaheuristic for multi-objective optimization,
in: IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making, 2009, pp.~66--73.

\bibitem{panichella2019adaptive}
A.~Panichella,
An adaptive evolutionary algorithm based on non-Euclidean geometry for many-objective optimization,
in: GECCO, ACM, 2019, pp.~595--603.

\bibitem{cheng2016reference}
R.~Cheng, et~al.,
A reference vector guided evolutionary algorithm for many-objective optimization,
IEEE Trans. Evol. Comput. 20~(5) (2016) 773--791.

\bibitem{zitzler2000zdt}
E.~Zitzler, K.~Deb, L.~Thiele,
Comparison of multiobjective evolutionary algorithms: Empirical results,
Evol. Comput. 8~(2) (2000) 173--195.

\bibitem{deb2002dtlz}
K.~Deb, L.~Thiele, M.~Laumanns, E.~Zitzler,
Scalable multi-objective optimization test problems,
in: Congress on Evolutionary Computation (CEC), 2002, pp.~825--830.

\bibitem{huband2006wfg}
S.~Huband, P.~Hingston, L.~Barone, L.~While,
A review of multiobjective test problems and a scalable test problem toolkit,
IEEE Trans. Evol. Comput. 10~(5) (2006) 477--506.

\bibitem{zitzler2003performance}
E.~Zitzler, L.~Thiele, M.~Laumanns, C.~M. Fonseca, V.~G. da Fonseca,
Performance assessment of multiobjective optimizers: An analysis and review,
IEEE Trans. Evol. Comput. 7~(2) (2003) 117--132.

\bibitem{wilcoxon1945}
F.~Wilcoxon,
Individual comparisons by ranking methods,
Biometrics Bull. 1~(6) (1945) 80--83.

\end{thebibliography}

\end{document}
