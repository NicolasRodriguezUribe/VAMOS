\section{Results}
\label{sec:results}

We first compare the three methods at the benchmark-family level (ZDT, DTLZ, WFG), then inspect convergence speed, per-problem deltas, and operator usage.
Unless stated otherwise, we report means over \AOSNSeeds{} seeds.

\subsection{Solution quality}
Table~\ref{tab:hv_family} shows mean normalized hypervolume grouped by benchmark family.
AOS achieves the highest overall mean HV (\AOSHVMeanAOS{}) compared to the baseline (\AOSHVMeanBaseline{}) and random arm (\AOSHVMeanRandom{}), outperforming the baseline on \AOSWins{} of \AOSNProblems{} problems.
The largest gain is on \texttt{\AOSBestProblem{}} ($\Delta{=}\AOSBestDelta{}$), where the baseline's fixed SBX+PM pipeline fails to converge; AOS's adaptive arm selection discovers effective operators that reach the true front.
The Wilcoxon tests (Table~\ref{tab:stat_test}) confirm \StatWinsBase{} significant wins and \StatLossesBase{} significant losses against the baseline after Holm--Bonferroni correction.
Against the random arm, AOS achieves \StatWinsRand{} significant wins with \StatLossesRand{} significant losses, confirming that intelligent selection adds measurable value beyond random operator switching.

\input{tables/ablation_hv_family}

\subsection{Convergence speed}
\label{sec:convergence}
The most striking result is not the final HV but the convergence \emph{speed}.
Figure~\ref{fig:anytime_agg} shows the mean normalized HV across all 21 problems at each evaluation checkpoint.
At 10{,}000 evaluations (20\% of budget), AOS achieves a mean HV of \AOSHVAOSTenK{} vs.\ the baseline's \AOSHVBaselineTenK{}---a \AOSConvergenceAdvPct{}\% advantage.
This early-stage dominance stems from AOS's ability to deploy exploratory operators (low-$\eta$ SBX, DE) when the population is spread across the search space, then shift toward exploitation operators (high-$\eta$ SBX) as the population converges.
The gap narrows toward the full budget as all methods approach the reference front, but AOS maintains its advantage throughout.

\begin{figure}[t]
\centering
\IfFileExists{figures/anytime_hv_aggregate.pdf}{
  \includegraphics[width=0.75\linewidth]{figures/anytime_hv_aggregate.pdf}
}{
  \fbox{\parbox{0.95\linewidth}{\vspace{2mm}Missing: figures/anytime\_hv\_aggregate.pdf (run the asset script).\vspace{2mm}}}
}
\caption{Mean normalized HV across all 21 problems vs.\ evaluations. AOS converges significantly faster than both the baseline and random arm selection, with the largest advantage at early checkpoints.}
\label{fig:anytime_agg}
\end{figure}

Figure~\ref{fig:anytime} shows per-problem convergence for four representative problems where AOS's convergence advantage is most visible.

\begin{figure}[t]
\centering
\IfFileExists{figures/anytime_hv_selected.pdf}{
  \includegraphics[width=\linewidth]{figures/anytime_hv_selected.pdf}
}{
  \fbox{\parbox{0.95\linewidth}{\vspace{2mm}Missing: figures/anytime\_hv\_selected.pdf (run the asset script).\vspace{2mm}}}
}
\caption{Mean$\pm$std normalized hypervolume vs.\ evaluations for selected problems. AOS converges substantially faster on ZDT2 and DTLZ7, and maintains or extends the lead through the full budget on WFG1 and WFG2.}
\label{fig:anytime}
\end{figure}

\subsection{Per-problem analysis}

\begin{figure}[t]
\centering
\IfFileExists{figures/hv_delta_by_problem.pdf}{
  \includegraphics[width=\linewidth]{figures/hv_delta_by_problem.pdf}
}{
  \fbox{\parbox{0.95\linewidth}{\vspace{2mm}Missing: figures/hv\_delta\_by\_problem.pdf (run the asset script).\vspace{2mm}}}
}
\caption{Per-problem mean HV change (AOS $-$ Baseline), sorted by $\Delta$. Positive bars indicate problems where AOS outperforms the fixed operator.}
\label{fig:hv_delta}
\end{figure}

Figure~\ref{fig:hv_delta} shows per-problem HV deltas.
The pattern reveals a family-dependent effect:
AOS achieves its strongest gains on DTLZ and ZDT problems, where landscape features (multi-modality, degenerate fronts, disconnected regions) reward operator adaptation.
On WFG4--9, which feature concave fronts where SBX with $\eta{=}20$ is already near-optimal, AOS shows a small deficit attributable to the exploration cost of trying alternative arms.
Crucially, on every WFG problem where AOS loses, the random arm loses even more (Section~\ref{sec:discussion}), confirming that the deficit is not a failure of the policy but an inherent cost of maintaining a diverse portfolio.

\subsection{Statistical significance}
We apply the Wilcoxon signed-rank test~\cite{wilcoxon1945} (paired, two-sided) across \AOSNSeeds{} seeds for each problem, with Holm--Bonferroni correction for \AOSNProblems{} comparisons at $\alpha{=}0.05$.
Table~\ref{tab:stat_test} summarizes the results.
AOS achieves \StatWinsBase{} significant wins vs.\ the baseline with \StatLossesBase{} losses, and \StatWinsRand{} significant wins vs.\ the random arm.

\input{tables/stat_test}

\subsection{Runtime}
Table~\ref{tab:runtime_family} reports median runtime by benchmark family.
The AOS overhead is \AOSRuntimeOverheadPct{}\% (\AOSRuntimeMedianAOS{}s vs.\ \AOSRuntimeMedianBaseline{}s), reflecting policy updates and reward computation.
The random arm is slower (\AOSRuntimeMedianRandom{}s) because it selects all operators with equal probability, including the 3-parent DE crossover; AOS learns to favor efficient operators, yielding an implicit computational benefit.

\input{tables/ablation_runtime_family}

\subsection{Operator usage}
Figure~\ref{fig:usage} shows operator selection fractions by search stage for the trace-exported problems.
Thompson Sampling learns to shift usage from exploratory arms toward exploitation arms as the search progresses, confirming genuine landscape-driven adaptation.
Figure~\ref{fig:reward_evolution} illustrates the reward dynamics for a representative run.

\begin{figure}[t]
\centering
\IfFileExists{figures/aos_operator_usage.pdf}{
  \includegraphics[width=\linewidth]{figures/aos_operator_usage.pdf}
}{
  \fbox{\parbox{0.95\linewidth}{\vspace{2mm}Missing: figures/aos\_operator\_usage.pdf (run the asset script).\vspace{2mm}}}
}
\caption{Mean operator selection fractions by search stage for trace-exported problems (AOS variant). The policy shifts from exploration (low-$\eta$ SBX, DE) toward exploitation (high-$\eta$ SBX) over the course of the run.}
\label{fig:usage}
\end{figure}

\begin{figure}[t]
\centering
\IfFileExists{figures/reward_evolution.pdf}{
  \includegraphics[width=\linewidth]{figures/reward_evolution.pdf}
}{
  \fbox{\parbox{0.95\linewidth}{\vspace{2mm}Missing: figures/reward\_evolution.pdf (run the asset script).\vspace{2mm}}}
}
\caption{Per-generation reward (top) and selected arm index (bottom) for a representative problem. The warm-up phase is visible in the first generations; afterward the policy converges while the exploration floor ensures occasional arm switches.}
\label{fig:reward_evolution}
\end{figure}
