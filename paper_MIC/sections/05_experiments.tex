\section{Experimental setup}
\label{sec:experiments}

\subsection{Benchmarks and budget}
We evaluate on 21 continuous multiobjective benchmarks drawn from three established families:
\begin{itemize}
  \item \textbf{ZDT}~\cite{zitzler2000zdt}: 5 bi-objective problems (ZDT1--4, ZDT6) with 10--30 decision variables.
  \item \textbf{DTLZ}~\cite{deb2002dtlz}: 7 tri-objective problems (DTLZ1--7) with 7--22 decision variables and diverse landscape features (multi-modal, degenerate, disconnected fronts).
  \item \textbf{WFG}~\cite{huband2006wfg}: 9 tri-objective problems (WFG1--9) with 24 decision variables and systematic variation in separability, modality, and front geometry.
\end{itemize}
Together these 21 problems cover a wide range of landscape characteristics---unimodal, multi-modal, deceptive, separable, non-separable, convex, concave, mixed, disconnected, and degenerate fronts---making them a standard testbed for evaluating algorithmic mechanisms in the multiobjective community.
Each run is terminated after a fixed evaluation budget of 50{,}000 function evaluations, using \AOSNSeeds{} independent seeds per problem.

\subsection{Methods compared}
We compare three methods, all based on NSGA-II with the same 5-arm operator portfolio (Appendix, Table~\ref{tab:portfolio}):

\begin{enumerate}
  \item \textbf{Baseline}: Fixed SBX + polynomial mutation (SBX crossover probability 1.0, $\eta_c=20$; polynomial mutation probability $1/n_{\mathrm{var}}$, $\eta_m=20$).
        This is the most common default operator pipeline in the MOEA literature.
  \item \textbf{Random arm}: At each generation, one of the five operator arms is selected uniformly at random.
        This isolates the effect of portfolio \emph{diversity} (having multiple operators) from \emph{intelligent selection}.
  \item \textbf{AOS}: The same portfolio, but one arm is selected per generation using Thompson Sampling with a sliding window of $W{=}50$ generations, warm-up of $m_{\min}{=}5$ pulls per arm, and an exploration floor of $p_{\mathrm{floor}}{=}0.05$.
        Rewards use a weighted combination of survival rate, non-dominated insertion rate, and a bounded hypervolume proxy with weights $(0.4, 0.4, 0.2)$.
\end{enumerate}
Comparing (1) vs.\ (2) reveals whether operator diversity alone helps; comparing (2) vs.\ (3) reveals whether the bandit policy adds value beyond random selection.

\subsection{Operator portfolio}
The five arms were designed along two axes of diversity:
(i)~\emph{parameter diversity}---three SBX variants with distribution indices $\eta \in \{5, 20, 50\}$ span an exploration--exploitation continuum, from wide offspring spread (low $\eta$) to children near parents (high $\eta$); and
(ii)~\emph{structural diversity}---BLX-$\alpha$ provides a different interpolation geometry, while DE/rand/1/bin introduces a fundamentally different search strategy using differential vectors between population members.
All arms use polynomial mutation, keeping the portfolio small ($K{=}5$) and differences attributable solely to crossover strategy.
This design is intentionally composed of variations and complements of a strong baseline (SBX+PM), reflecting the practical insight that large performance gains are more reliably achieved through calibrated parameter variation than through mixing in highly disruptive operators.

\subsection{Metrics}
We report normalized hypervolume (higher is better) and wall-clock runtime (lower is better).
Hypervolume is computed using approximate reference fronts generated by running multiple seeds of baseline NSGA-II with an extended budget (200{,}000 evaluations, 50 seeds) and merging all non-dominated solutions per problem.
The reference point is the per-objective maximum of the reference front plus a small $\epsilon$ margin.
Normalized hypervolume divides the raw HV of a run by the reference front's HV; values near~1 indicate performance comparable to the reference front (values slightly above~1 can occur when a run discovers solutions beyond the reference approximation).
We additionally report \emph{anytime convergence}: mean normalized HV at evaluation checkpoints (5\,000, 10\,000, 20\,000, 50\,000), which captures convergence \emph{speed} in addition to final quality.

\begin{table}[t]
\centering
\small
\caption{Key experimental settings.}
\label{tab:settings}
\begin{tabular}{l|l}
\toprule
Setting & Value \\
\midrule
Algorithm & NSGA-II \\
Population / offspring & 100 / 100 \\
Budget & 50{,}000 evaluations \\
Seeds & \AOSNSeeds{} per problem \\
Problems & \AOSNProblems{} (ZDT + DTLZ + WFG, continuous) \\
Baseline operators & SBX($p{=}1.0$, $\eta_c{=}20$) + PM($p{=}1/n_{\mathrm{var}}$, $\eta_m{=}20$) \\
Random arm & Uniform selection from 5-arm pool \\
AOS policy & Thompson Sampling ($W{=}50$, $m_{\min}{=}5$, $p_{\mathrm{floor}}{=}0.05$) \\
AOS reward & $0.4\,r_{\mathrm{surv}} + 0.4\,r_{\mathrm{nd}} + 0.2\,r_{\mathrm{hv}}$ \\
Portfolio & 5 arms (Table~\ref{tab:portfolio}) \\
\bottomrule
\end{tabular}
\end{table}
