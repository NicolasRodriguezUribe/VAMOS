\section{Experimental setup}
\label{sec:experiments}

\subsection{Benchmarks and budget}
We evaluate on \AOSNProblems{} continuous multiobjective benchmarks drawn from two families specifically chosen to stress-test operator choice:
\begin{itemize}
  \item \textbf{CEC\,2009 UF}~\cite{zhang2009cec2009}: 10 unconstrained problems (UF1--UF10) with 30 decision variables and 2--3 objectives.
        These problems feature curved, non-linear Pareto sets in decision space, making them difficult for operators that assume coordinate-aligned structure (e.g., standard SBX).
        UF1--UF7 are bi-objective; UF8--UF10 are tri-objective.
  \item \textbf{LSMOP}~\cite{cheng2017lsmop}: 9 large-scale problems (LSMOP1--LSMOP9) with 100 decision variables and 2 objectives.
        The high dimensionality ($\sim$3$\times$ more variables than standard benchmarks) makes operator choice critical: the search space grows exponentially, and a single crossover strategy often cannot balance exploration and exploitation across all variable groups.
        LSMOP problems vary in distance-function difficulty (sphere, Rastrigin, Rosenbrock, Ackley, Griewank) and front shape (linear, convex, disconnected).
\end{itemize}
Together these \AOSNProblems{} problems cover landscapes where standard SBX-based NSGA-II is known to struggle---curved Pareto sets, high dimensionality, multi-modality, and non-separability---making them an ideal testbed for evaluating whether adaptive operator selection provides genuine benefit beyond what a well-tuned fixed operator can achieve.
Each run is terminated after a fixed evaluation budget of 100{,}000 function evaluations, using \AOSNSeeds{} independent seeds per problem.

\subsection{Methods compared}
We compare three methods, all based on NSGA-II with the same 5-arm operator portfolio (Appendix, Table~\ref{tab:portfolio}):

\begin{enumerate}
  \item \textbf{Baseline}: Fixed SBX + polynomial mutation (SBX crossover probability 1.0, $\eta_c=20$; polynomial mutation probability $1/n_{\mathrm{var}}$, $\eta_m=20$).
        This is the most common default operator pipeline in the MOEA literature.
  \item \textbf{Random arm}: At each generation, one of the five operator arms is selected uniformly at random.
        This isolates the effect of portfolio \emph{diversity} (having multiple operators) from \emph{intelligent selection}.
  \item \textbf{AOS}: The same portfolio, but one arm is selected per generation using Thompson Sampling with a sliding window of $W{=}50$ generations, warm-up of $m_{\min}{=}5$ pulls per arm, and an exploration floor of $p_{\mathrm{floor}}{=}0.02$.
        Rewards use a weighted combination of survival rate, non-dominated insertion rate, and a bounded hypervolume proxy with weights $(0.4, 0.4, 0.2)$.
        Additionally, \emph{adaptive arm elimination} prunes statistically underperforming arms after $T_{\mathrm{elim}}{=}300$ generations ($\approx$30{,}000 evaluations) using a $z$-threshold of 2.0, retaining at least $K_{\min}{=}2$ arms.
\end{enumerate}
Comparing (1) vs.\ (2) reveals whether operator diversity alone helps; comparing (2) vs.\ (3) reveals whether the bandit policy with adaptive pruning adds value beyond random selection.

\subsection{Operator portfolio}
The five arms were designed along two axes of diversity:
(i)~\emph{parameter diversity}---three SBX variants with distribution indices $\eta \in \{5, 20, 50\}$ span an exploration--exploitation continuum, from wide offspring spread (low $\eta$) to children near parents (high $\eta$); and
(ii)~\emph{structural diversity}---BLX-$\alpha$ provides a different interpolation geometry, while DE/rand/1/bin introduces a fundamentally different search strategy using differential vectors between population members.
All arms use polynomial mutation, keeping the portfolio small ($K{=}5$) and differences attributable solely to crossover strategy.
This design is intentionally composed of variations and complements of a strong baseline (SBX+PM), reflecting the practical insight that large performance gains are more reliably achieved through calibrated parameter variation than through mixing in highly disruptive operators.

\subsection{Metrics}
We report normalized hypervolume (higher is better) and wall-clock runtime (lower is better).
Hypervolume is computed using analytically derived reference fronts for each problem family (known true Pareto fronts for UF and LSMOP).
The reference point is the per-objective maximum of the reference front plus a small $\epsilon$ margin.
Normalized hypervolume divides the raw HV of a run by the reference front's HV; values near~1 indicate performance comparable to the true Pareto front (values slightly above~1 can occur when a run discovers solutions beyond the reference approximation).
We additionally report \emph{anytime convergence}: mean normalized HV at evaluation checkpoints (5\,000, 10\,000, 20\,000, 50\,000, 100\,000), which captures convergence \emph{speed} in addition to final quality.

\begin{table}[t]
\centering
\small
\caption{Key experimental settings.}
\label{tab:settings}
\begin{tabular}{l|l}
\toprule
Setting & Value \\
\midrule
Algorithm & NSGA-II \\
Population / offspring & 100 / 100 \\
Budget & 100{,}000 evaluations \\
Seeds & \AOSNSeeds{} per problem \\
Problems & \AOSNProblems{} (CEC\,2009 UF + LSMOP, continuous) \\
Baseline operators & SBX($p{=}1.0$, $\eta_c{=}20$) + PM($p{=}1/n_{\mathrm{var}}$, $\eta_m{=}20$) \\
Random arm & Uniform selection from 5-arm pool \\
AOS policy & Thompson Sampling ($W{=}50$, $m_{\min}{=}5$, $p_{\mathrm{floor}}{=}0.02$) \\
Arm elimination & After 300 gen., $z_{\mathrm{thresh}}{=}2.0$, $K_{\min}{=}2$ \\
AOS reward & $0.4\,r_{\mathrm{surv}} + 0.4\,r_{\mathrm{nd}} + 0.2\,r_{\mathrm{hv}}$ \\
Portfolio & 5 arms (Table~\ref{tab:portfolio}) \\
\bottomrule
\end{tabular}
\end{table}
