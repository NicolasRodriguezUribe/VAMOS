\section{Experimental setup}
\label{sec:experiments}

\subsection{Benchmarks and budget}
We evaluate on 21 continuous real-world engineering surrogates drawn from two established benchmark suites:
\begin{itemize}
  \item \textbf{Tanabe--Ishibuchi RE}~\cite{tanabe2020re}: 11 continuous problems (RE21, RE24, RE31--34, RE37, RE41--42, RE61, RE91), spanning 2--9 objectives and 3--7 decision variables.
  \item \textbf{Zapotecas-Mart\'{i}nez RWA}~\cite{zapotecas2023rwa}: 10 continuous problems (RWA1--RWA10), spanning 2--7 objectives and 3--10 decision variables.
\end{itemize}
These problems originate from engineering domains including structural design, vehicle crashworthiness, combustion, machining, antenna design, and chemical processing.
Unlike the synthetic ZDT~\cite{zitzler2000zdt}, DTLZ~\cite{deb2002dtlz}, and WFG~\cite{huband2006wfg} families, these surrogates have unknown landscape geometry, making operator adaptation genuinely data-driven.
Each run is terminated after a fixed evaluation budget of 50{,}000 function evaluations, using \AOSNSeeds{} independent seeds per problem.

\subsection{Methods compared}
We compare three methods, all based on NSGA-II with the same 5-arm operator portfolio (Appendix, Table~\ref{tab:portfolio}):

\begin{enumerate}
  \item \textbf{Baseline}: Fixed SBX + polynomial mutation (SBX crossover probability 1.0, $\eta_c=20$; polynomial mutation probability $1/n_{\mathrm{var}}$, $\eta_m=20$).
        This is the most common default operator pipeline in the MOEA literature.
  \item \textbf{Random arm}: At each generation, one of the five operator arms is selected uniformly at random.
        This isolates the effect of portfolio \emph{diversity} (having multiple operators) from \emph{intelligent selection}.
  \item \textbf{AOS}: The same portfolio, but one arm is selected per generation using an $\varepsilon$-greedy policy with $\varepsilon=0.05$.
        Rewards use a weighted combination of survival rate, non-dominated insertion rate, and a bounded hypervolume proxy with weights $(0.4, 0.4, 0.2)$.
        For problems with 5 or more objectives, the hypervolume component is disabled to avoid prohibitive per-generation computation; the policy then relies on survival and non-dominated insertion signals alone.
\end{enumerate}
Comparing (1) vs.\ (2) reveals whether operator diversity alone helps; comparing (2) vs.\ (3) reveals whether the bandit policy adds value beyond random selection.

\subsection{Metrics}
We report normalized hypervolume (higher is better) and wall-clock runtime (lower is better).
Hypervolume is computed using approximate reference fronts generated by running multiple seeds of baseline NSGA-II with an extended budget (200{,}000 evaluations) and merging all non-dominated solutions per problem.
The reference point is the per-objective maximum of the reference front plus a small $\epsilon$ margin.
Normalized hypervolume divides the raw HV of a run by the reference front's HV; values near~1 indicate performance comparable to the reference front (values slightly above~1 can occur when a run discovers solutions beyond the reference approximation).

\begin{table}[t]
\centering
\small
\caption{Key experimental settings.}
\label{tab:settings}
\begin{tabular}{l|l}
\toprule
Setting & Value \\
\midrule
Algorithm & NSGA-II \\
Population / offspring & 100 / 100 \\
Budget & 50{,}000 evaluations \\
Seeds & \AOSNSeeds{} per problem \\
Problems & \AOSNProblems{} (RE + RWA, continuous) \\
Baseline operators & SBX($p{=}1.0$, $\eta_c{=}20$) + PM($p{=}1/n_{\mathrm{var}}$, $\eta_m{=}20$) \\
Random arm & Uniform selection from 5-arm pool \\
AOS policy & $\varepsilon$-greedy ($\varepsilon{=}0.05$), 1 arm per generation \\
AOS reward & $0.4\,r_{\mathrm{surv}} + 0.4\,r_{\mathrm{nd}} + 0.2\,r_{\mathrm{hv}}$ \\
Portfolio & 5 arms (Table~\ref{tab:portfolio}) \\
\bottomrule
\end{tabular}
\end{table}
