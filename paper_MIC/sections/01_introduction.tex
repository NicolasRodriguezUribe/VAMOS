\section{Introduction}
\label{sec:intro}

Multiobjective evolutionary algorithms (MOEAs) are a default choice for black-box optimization problems with conflicting objectives, including engineering design and automated decision support.
Most MOEAs rely on a small number of variation operators (crossover and mutation) to balance exploration and exploitation.
However, operator choice is a major source of performance variability: operators that work well on one problem can stall on another, and the most effective operator can change as the population approaches the Pareto front.
This motivates \emph{adaptive operator selection} (AOS): selecting operators online based on feedback from the ongoing search, rather than relying on a single hand-tuned pipeline.

Most AOS mechanisms can be seen as two coupled components: (i) credit assignment (how to score an operator based on recent offspring) and (ii) an adaptation rule (how to turn scores into future operator-selection decisions).
Following the bandit view of AOS~\cite{dacosta2008aosdmab}, we model each candidate variation pipeline as an arm in a multi-armed bandit, and update an online policy from per-generation rewards.
Compared to offline tuning, this approach aims to improve robustness across problems and reduce the need for manual operator configuration.

This paper focuses on NSGA-II~\cite{deb2002nsgaii}, a widely used MOEA whose performance depends strongly on variation settings.
We implement a generation-level AOS layer for NSGA-II in a vectorized optimization framework and evaluate it on \AOSNProblems{} standard benchmarks from the ZDT~\cite{zitzler2000zdt}, DTLZ~\cite{deb2002dtlz}, and WFG~\cite{huband2006wfg} families.
Our evaluation reveals that AOS with Thompson Sampling not only improves final solution quality on the majority of problems, but---more importantly---converges significantly faster than the fixed baseline, making it especially attractive in budget-constrained settings.

\paragraph{Contributions.}
This paper makes four contributions:
\begin{itemize}
  \item A simple, reproducible AOS interface for NSGA-II that selects exactly one variation pipeline per generation using Thompson Sampling with a sliding reward window.
  \item A reward design aligned with NSGA-II survivor selection, combining offspring survival and non-dominated insertion rates, with an optional bounded hypervolume-improvement proxy.
  \item A portfolio design based on parameter diversity (SBX $\eta$ variants) and structural diversity (BLX-$\alpha$, DE/rand/1/bin), showing that calibrated variation of a strong baseline is more effective than mixing in exotic operators.
  \item An empirical evaluation on \AOSNProblems{} standard MO benchmarks (ZDT, DTLZ, WFG) demonstrating that AOS achieves \AOSConvergenceAdvPct{}\% faster convergence at 10\% of budget, with \AOSWins{}/\AOSNProblems{} final-quality wins, while incurring measurable runtime overhead.
\end{itemize}

\paragraph{Organization.}
Section~\ref{sec:background} reviews NSGA-II and the bandit perspective on AOS.
Section~\ref{sec:method} presents the method, beginning with a high-level overview of the five-stage AOS pipeline (Figure~\ref{fig:aos_overview}), followed by formal details on portfolios, rewards, and policies.
Section~\ref{sec:implementation} summarizes implementation and reproducibility details.
Sections~\ref{sec:experiments}--\ref{sec:results} describe the experimental protocol and results, followed by discussion and threats to validity.
