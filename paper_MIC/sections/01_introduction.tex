\section{Introduction}
\label{sec:intro}

Multiobjective evolutionary algorithms (MOEAs) are a default choice for black-box optimization problems with conflicting objectives, including engineering design and automated decision support.
Most MOEAs rely on a small number of variation operators (crossover and mutation) to balance exploration and exploitation.
However, operator choice is a major source of performance variability: operators that work well on one problem can stall on another, and the most effective operator can change as the population approaches the Pareto front.
This motivates \emph{adaptive operator selection} (AOS): selecting operators online based on feedback from the ongoing search, rather than relying on a single hand-tuned pipeline.

Most AOS mechanisms can be seen as two coupled components: (i) credit assignment (how to score an operator based on recent offspring) and (ii) an adaptation rule (how to turn scores into future operator-selection decisions).
Following the bandit view of AOS~\cite{dacosta2008aosdmab}, we model each candidate variation pipeline as an arm in a multi-armed bandit, and update an online policy from per-generation rewards.
Compared to offline tuning, this approach aims to improve robustness across problems and reduce the need for manual operator configuration.

This paper focuses on NSGA-II~\cite{deb2002nsgaii}, a widely used MOEA whose performance depends strongly on variation settings.
We implement a generation-level AOS layer for NSGA-II in a vectorized optimization framework and evaluate it on \AOSNProblems{} challenging benchmarks from the CEC\,2009 UF~\cite{zhang2009cec2009} and LSMOP~\cite{cheng2017lsmop} families---problems specifically designed to expose operator-choice sensitivity through curved Pareto sets and high-dimensional decision spaces.
Our evaluation reveals that AOS with Thompson Sampling and adaptive arm elimination dramatically outperforms the fixed baseline on these challenging landscapes, achieving +18\% higher mean HV overall with \emph{zero} statistically significant losses across all \AOSNProblems{} problems.

\paragraph{Contributions.}
This paper makes four contributions:
\begin{itemize}
  \item A simple, reproducible AOS interface for NSGA-II that selects exactly one variation pipeline per generation using Thompson Sampling with a sliding reward window.
  \item A reward design aligned with NSGA-II survivor selection, combining offspring survival and non-dominated insertion rates, with an optional bounded hypervolume-improvement proxy.
  \item An \emph{adaptive arm elimination} mechanism that statistically prunes underperforming operators after an initial learning phase, automatically recovering baseline-level performance on problems where the default operator is already effective.
  \item An empirical evaluation on \AOSNProblems{} challenging benchmarks (CEC\,2009 UF, LSMOP) demonstrating that AOS achieves +18\% higher mean HV than the fixed baseline, with \StatWinsBase{} statistically significant wins and \StatLossesBase{} significant losses---making AOS a strictly safe default in practice.
\end{itemize}

\paragraph{Organization.}
Section~\ref{sec:background} reviews NSGA-II and the bandit perspective on AOS.
Section~\ref{sec:method} presents the method, beginning with a high-level overview of the five-stage AOS pipeline (Figure~\ref{fig:aos_overview}), followed by formal details on portfolios, rewards, and policies.
Section~\ref{sec:implementation} summarizes implementation and reproducibility details.
Sections~\ref{sec:experiments}--\ref{sec:results} describe the experimental protocol and results, followed by discussion and threats to validity.
