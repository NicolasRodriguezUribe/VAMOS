\section{Conclusion}
\label{sec:conclusion}

We presented a bandit-based adaptive operator selection layer for NSGA-II that selects one variation pipeline per generation and updates an online policy from survival and diversity-oriented rewards.
Across \AOSNProblems{} real-world engineering surrogates spanning 2--9 objectives, AOS matches or exceeds the fixed baseline on \AOSWins{} of \AOSNProblems{} problems and significantly outperforms uniform random arm selection (median HV \AOSHVMedianAOS{} vs.\ \AOSHVMedianRandom{}), with a median runtime overhead of \AOSRuntimeOverheadPct{}\%.
The three-way comparison design reveals that portfolio diversity alone can degrade performance (random arm scores below the baseline on several 4-obj and many-obj problems), while the bandit policy recovers this loss and occasionally exceeds the baseline.

Future work includes portfolio construction rules (e.g., automatic arm design from parameter sweeps), scalable reward signals for many-objective problems (e.g., $R_2$-indicator proxies), non-stationary policies with explicit change detection, and cost-aware implementations that reduce controller overhead by reusing selection statistics already computed during NSGA-II's survivor selection.

