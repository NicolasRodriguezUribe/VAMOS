\documentclass[runningheads]{llncs}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[protrusion=true,expansion=false]{microtype}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

\newcommand{\VAMOS}{VAMOS}

% Auto-generated experiment summary macros (safe placeholder if not generated yet)
\input{tables/summary_macros}

\begin{document}

\title{Adaptive Operator Selection for NSGA-II in a Vectorized Framework}
\titlerunning{Adaptive Operator Selection for NSGA-II}

\author{Nicol\'{a}s R. Uribe\inst{1} \and Alberto Herr\'{a}n\inst{1}\thanks{Corresponding author: Alberto Herr\'{a}n (alberto.herran@urjc.es)} \and Antonio J. Nebro\inst{2,3} \and J. Manuel Colmenar\inst{1}}
\authorrunning{Uribe et al.}

\institute{Dept. Computer Sciences, Universidad Rey Juan Carlos\\
C/. Tulip\'{a}n, s/n, M\'{o}stoles, 28933 (Madrid), Spain\\
\email{nicolas.rodriguez@urjc.es, alberto.herran@urjc.es, josemanuel.colmenar@urjc.es}
\and
Dept. de Lenguajes y Ciencias de la Computaci\'{o}n, ITIS Software, University of M\'{a}laga,\\
ETSI Inform\'{a}tica, Campus de Teatinos, 29071 (M\'{a}laga), Spain\\
\email{ajnebro@uma.es}
\and
ITIS Software, Ada Byron Research Building, C/. Arquitecto Francisco Pe\~nalosa, 18, University of M\'{a}laga, 29071 (M\'{a}laga), Spain\\
\email{ajnebro@uma.es}}

\maketitle

\begin{abstract}
Operator choice can strongly affect the performance of multiobjective evolutionary algorithms, yet the best operator may vary across problems and across stages of search.
We present an adaptive operator selection (AOS) layer for NSGA-II that treats each (crossover, mutation) pipeline as a bandit arm and selects one arm per generation using Thompson Sampling, updating an online policy from survival and diversity-based rewards.
We evaluate on \AOSNProblems{} challenging benchmarks from the CEC\,2009 UF family (curved Pareto sets, 30 variables) and the LSMOP family (large-scale, 100 variables), where standard SBX-based NSGA-II is known to struggle.
A three-way comparison (fixed operator vs.\ random arm vs.\ AOS) with \AOSNSeeds{} seeds reveals that AOS achieves a mean normalized HV of \AOSHVMeanAOS{} vs.\ the fixed baseline's \AOSHVMeanBaseline{} (+23\%), with \AOSWins{}/\AOSNProblems{} problem wins and \StatWinsBase{} statistically significant improvements.
The gains are most pronounced on high-dimensional LSMOP problems (+80\% family average) and on UF problems with complex Pareto-set geometry (+9\%), demonstrating that a diverse operator portfolio with intelligent selection is essential when no single operator dominates.
\keywords{Adaptive operator selection \and Multi-armed bandits \and NSGA-II \and Multiobjective optimization \and Thompson Sampling}
\end{abstract}

\input{sections/01_introduction}
\input{sections/02_background}
\input{sections/03_method}
\input{sections/04_implementation}
\input{sections/05_experiments}
\input{sections/06_results}
\input{sections/07_discussion}
\input{sections/08_threats}
\input{sections/09_conclusion}

\appendix
\renewcommand{\theHsection}{app.\Alph{section}}
\input{sections/10_appendix}

\bibliographystyle{splncs04}
\bibliography{references}

\end{document}
