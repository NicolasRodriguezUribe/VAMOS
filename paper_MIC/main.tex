\documentclass[runningheads]{llncs}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[protrusion=true,expansion=false]{microtype}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

\newcommand{\VAMOS}{VAMOS}

% Auto-generated experiment summary macros (safe placeholder if not generated yet)
\input{tables/summary_macros}

\begin{document}

\title{Adaptive Operator Selection for NSGA-II in a Vectorized Framework}
\titlerunning{Adaptive Operator Selection for NSGA-II}

\author{Nicol\'{a}s R. Uribe\inst{1} \and Alberto Herr\'{a}n\inst{1}\thanks{Corresponding author: Alberto Herr\'{a}n (alberto.herran@urjc.es)} \and Antonio J. Nebro\inst{2,3} \and J. Manuel Colmenar\inst{1}}
\authorrunning{Uribe et al.}

\institute{Dept. Computer Sciences, Universidad Rey Juan Carlos\\
C/. Tulip\'{a}n, s/n, M\'{o}stoles, 28933 (Madrid), Spain\\
\email{nicolas.rodriguez@urjc.es, alberto.herran@urjc.es, josemanuel.colmenar@urjc.es}
\and
Dept. de Lenguajes y Ciencias de la Computaci\'{o}n, ITIS Software, University of M\'{a}laga,\\
ETSI Inform\'{a}tica, Campus de Teatinos, 29071 (M\'{a}laga), Spain\\
\email{ajnebro@uma.es}
\and
ITIS Software, Ada Byron Research Building, C/. Arquitecto Francisco Pe\~nalosa, 18, University of M\'{a}laga, 29071 (M\'{a}laga), Spain\\
\email{ajnebro@uma.es}}

\maketitle

\begin{abstract}
Operator choice can strongly affect the performance of multiobjective evolutionary algorithms, yet the best operator may vary across problems and across stages of search.
We present an adaptive operator selection (AOS) layer for NSGA-II that treats each (crossover, mutation) pipeline as a bandit arm and selects one arm per generation using Thompson Sampling, updating an online policy from survival and diversity-based rewards.
We evaluate on \AOSNProblems{} standard benchmarks from the ZDT, DTLZ, and WFG families (2--3 objectives, 7--30 variables).
A three-way comparison (fixed operator vs.\ random arm vs.\ AOS) with \AOSNSeeds{} seeds reveals two main findings: (i)~AOS converges \AOSConvergenceAdvPct{}\% faster than the fixed baseline at 20\% of the evaluation budget, and (ii)~AOS achieves a higher mean normalized HV (\AOSHVMeanAOS{} vs.\ \AOSHVMeanBaseline{}) with \AOSWins{}/\AOSNProblems{} problem wins at the final budget.
On the minority of problems where the default operator is already near-optimal (concave WFG fronts), AOS incurs a small exploration cost that is consistently smaller than that of random arm selection.
AOS incurs a median runtime overhead of \AOSRuntimeOverheadPct{}\%.
\keywords{Adaptive operator selection \and Multi-armed bandits \and NSGA-II \and Multiobjective optimization \and Thompson Sampling}
\end{abstract}

\input{sections/01_introduction}
\input{sections/02_background}
\input{sections/03_method}
\input{sections/04_implementation}
\input{sections/05_experiments}
\input{sections/06_results}
\input{sections/07_discussion}
\input{sections/08_threats}
\input{sections/09_conclusion}

\appendix
\renewcommand{\theHsection}{app.\Alph{section}}
\input{sections/10_appendix}

\bibliographystyle{splncs04}
\bibliography{references}

\end{document}
