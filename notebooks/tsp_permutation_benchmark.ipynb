{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "246e1541",
   "metadata": {},
   "source": [
    "# TSPLIB permutation benchmark\n",
    "Runs NSGA-II across KroA?E100 instances comparing available backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3956e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import types\n",
    "\n",
    "def resolve_project_root(markers=(\"pyproject.toml\", \".git\")):\n",
    "    current = Path.cwd().resolve()\n",
    "    for candidate in (current, *current.parents):\n",
    "        if any((candidate / marker).exists() for marker in markers):\n",
    "            return candidate\n",
    "    raise RuntimeError(\"No se pudo ubicar la raiz del proyecto; ejecuta este notebook dentro del arbol de VAMOS.\")\n",
    "\n",
    "PROJECT_ROOT = resolve_project_root()\n",
    "SRC_DIR = PROJECT_ROOT / 'src'\n",
    "if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "import vamos.runner as runner\n",
    "from vamos.runner import ExperimentConfig, resolve_kernel\n",
    "from vamos.external import run_external\n",
    "from vamos.problem.registry import make_problem_selection\n",
    "from vamos.study.runner import StudyRunner, StudyTask\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f11f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG = ExperimentConfig()\n",
    "TSPLIB_PROBLEMS = ['kroa100', 'krob100', 'kroc100', 'krod100', 'kroe100']\n",
    "ACTIVE_PROBLEMS = list(TSPLIB_PROBLEMS)\n",
    "PROBLEM_GRID = []\n",
    "\n",
    "SEEDS = [DEFAULT_CONFIG.seed]\n",
    "ALG = 'nsgaii'\n",
    "INTERNAL_ENGINES = ['numpy', 'numba', 'moocore']\n",
    "EXTERNAL_ALGOS = ['pymoo_perm_nsga2', 'jmetalpy_perm_nsga2']\n",
    "INCLUDE_EXTERNAL = True\n",
    "ENABLED_ENGINES = []\n",
    "\n",
    "\n",
    "def detect_internal_engines(candidates):\n",
    "    engines = []\n",
    "    for name in candidates:\n",
    "        try:\n",
    "            resolve_kernel(name)\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping engine '{name}': {exc}\")\n",
    "        else:\n",
    "            engines.append(name)\n",
    "    return engines\n",
    "\n",
    "\n",
    "def set_active_problems(names):\n",
    "    normalized = []\n",
    "    for name in names:\n",
    "        key = name.lower()\n",
    "        if key not in TSPLIB_PROBLEMS:\n",
    "            raise ValueError(f\"Unknown TSPLIB benchmark '{name}'.\")\n",
    "        normalized.append(key)\n",
    "    if not normalized:\n",
    "        raise ValueError('At least one benchmark is required.')\n",
    "    specs = [\n",
    "        {\n",
    "            'problem': key,\n",
    "            'label': f\"TSPLIB {key.upper()}\",\n",
    "        }\n",
    "        for key in normalized\n",
    "    ]\n",
    "    txt = ', '.join(spec['label'] for spec in specs)\n",
    "    print(f\"Active problems: {txt}\")\n",
    "    return specs\n",
    "\n",
    "\n",
    "ENABLED_ENGINES = detect_internal_engines(INTERNAL_ENGINES)\n",
    "if not ENABLED_ENGINES:\n",
    "    raise RuntimeError('No compatible engines detected.')\n",
    "PROBLEM_GRID = set_active_problems(ACTIVE_PROBLEMS)\n",
    "print('Engines:', ', '.join(ENABLED_ENGINES))\n",
    "print('External algos:', ', '.join(EXTERNAL_ALGOS) if INCLUDE_EXTERNAL else 'None')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd351e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RUN_HISTORY = []\n",
    "\n",
    "\n",
    "def build_tasks(problem_grid, engines, seeds, include_external=True):\n",
    "    tasks = []\n",
    "    for spec in problem_grid:\n",
    "        for engine in engines:\n",
    "            for seed in seeds:\n",
    "                tasks.append(StudyTask(\n",
    "                    algorithm=ALG,\n",
    "                    engine=engine,\n",
    "                    problem=spec['problem'],\n",
    "                    n_var=spec.get('n_var'),\n",
    "                    seed=seed,\n",
    "                ))\n",
    "        if include_external:\n",
    "            for algo in EXTERNAL_ALGOS:\n",
    "                tasks.append(StudyTask(\n",
    "                    algorithm=algo,\n",
    "                    engine='external',\n",
    "                    problem=spec['problem'],\n",
    "                    n_var=spec.get('n_var'),\n",
    "                    seed=seeds[0],\n",
    "                ))\n",
    "    return tasks\n",
    "\n",
    "\n",
    "def run_tasks(tasks):\n",
    "    global RUN_HISTORY\n",
    "    runner = StudyRunner(verbose=True)\n",
    "    raw_results = runner.run(tasks)\n",
    "    RUN_HISTORY = [types.SimpleNamespace(selection=res.selection, metrics=res.metrics) for res in raw_results]\n",
    "    rows = []\n",
    "    for entry in RUN_HISTORY:\n",
    "        m = entry.metrics\n",
    "        rows.append({\n",
    "            'problem': entry.selection.spec.key,\n",
    "            'problem_label': entry.selection.spec.label,\n",
    "            'engine': m['engine'],\n",
    "            'algorithm': m['algorithm'],\n",
    "            'time_ms': m['time_ms'],\n",
    "            'evaluations': m['evaluations'],\n",
    "            'evals_per_sec': m['evals_per_sec'],\n",
    "            'hv': m.get('hv'),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = build_tasks(PROBLEM_GRID, ENABLED_ENGINES, SEEDS, include_external=False)\n",
    "summary_df = run_tasks(tasks)\n",
    "if INCLUDE_EXTERNAL:\n",
    "    extra_rows = []\n",
    "    ext_cfg = ExperimentConfig(seed=SEEDS[0], population_size=DEFAULT_CONFIG.population_size, max_evaluations=DEFAULT_CONFIG.max_evaluations)\n",
    "    for spec in PROBLEM_GRID:\n",
    "        selection = make_problem_selection(spec['problem'], n_var=spec.get('n_var'))\n",
    "        for algo in EXTERNAL_ALGOS:\n",
    "            metrics = run_external(\n",
    "                algo,\n",
    "                selection,\n",
    "                use_native_problem=False,\n",
    "                config=ext_cfg,\n",
    "                make_metrics=runner._make_metrics,\n",
    "                print_banner=lambda problem, selection, label, backend, cfg=ext_cfg: runner._print_run_banner(\n",
    "                    problem, selection, label, backend, cfg\n",
    "                ),\n",
    "                print_results=runner._print_run_results,\n",
    "            )\n",
    "            if metrics is None:\n",
    "                continue\n",
    "            entry = types.SimpleNamespace(selection=selection, metrics=metrics)\n",
    "            RUN_HISTORY.append(entry)\n",
    "            extra_rows.append({\n",
    "                'problem': selection.spec.key,\n",
    "                'problem_label': selection.spec.label,\n",
    "                'engine': metrics.get('engine'),\n",
    "                'algorithm': metrics.get('algorithm'),\n",
    "                'time_ms': metrics.get('time_ms'),\n",
    "                'evaluations': metrics.get('evaluations'),\n",
    "                'evals_per_sec': metrics.get('evals_per_sec'),\n",
    "                'hv': metrics.get('hv'),\n",
    "            })\n",
    "    if extra_rows:\n",
    "        extra_df = pd.DataFrame(extra_rows)\n",
    "        summary_df = pd.concat([summary_df, extra_df], ignore_index=True)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71982c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if summary_df.empty:\n",
    "    print('No data yet.')\n",
    "else:\n",
    "    for label, group in summary_df.groupby('problem_label', sort=False):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        axes[0].bar(group['algorithm'], group['hv'], color='tab:blue')\n",
    "        axes[0].set_ylabel('Hypervolume')\n",
    "        axes[0].set_title(label)\n",
    "        axes[0].tick_params(axis='x', rotation=30)\n",
    "\n",
    "        axes[1].bar(group['algorithm'], group['time_ms'], color='tab:orange')\n",
    "        axes[1].set_ylabel('Time (ms)')\n",
    "        axes[1].set_title(label)\n",
    "        axes[1].tick_params(axis='x', rotation=30)\n",
    "\n",
    "        fig.suptitle(label)\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ecbb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not RUN_HISTORY:\n",
    "    print('Run the experiment cell first.')\n",
    "else:\n",
    "    runs_by_problem = {}\n",
    "    for res in RUN_HISTORY:\n",
    "        label = res.selection.spec.label\n",
    "        runs_by_problem.setdefault(label, []).append(res)\n",
    "    for label, runs in runs_by_problem.items():\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        for res in runs:\n",
    "            F = res.metrics['F']\n",
    "            legend_entry = f\"{res.metrics['algorithm']}\"\n",
    "            ax.scatter(F[:, 0], F[:, 1], s=30, alpha=0.7, label=legend_entry)\n",
    "        ax.set_xlabel('Tour length')\n",
    "        ax.set_ylabel('Max edge length')\n",
    "        ax.set_title(f'{label} Pareto front')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}