{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tuning Analysis: Visualizing Hyperparameter Search\n",
                "\n",
                "This notebook demonstrates how to inspect the results of a parameter tuning session.\n",
                "Since we don't have a live database of tuning runs yet, we generate **synthetic tuning data** to showcase:\n",
                "1.  **Parallel Coordinates**: How parameters interact to impact performance.\n",
                "2.  **Parameter Importance**: Using Random Forest to quantify which parameters matter most.\n",
                "3.  **Optimization Trace**: How the tuner improved over time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.inspection import PartialDependenceDisplay"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Generate Synthetic Tuning Data\n",
                "We simulate a tuning session for NSGA-II on a TSP problem.\n",
                "Parameters:\n",
                "- `pop_size`: [50, 200]\n",
                "- `crossover_prob`: [0.5, 1.0]\n",
                "- `mutation_prob`: [0.0, 0.2]\n",
                "- `selection_pressure`: [2, 10]\n",
                "\n",
                "Metric: **Hypervolume** (maximize). We'll assume a \"ground truth\" landscape where:\n",
                "- High `pop_size` is good.\n",
                "- `crossover_prob` ~ 0.9 is good.\n",
                "- `mutation_prob` ~ 0.1 is good."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "n_samples = 200\n",
                "\n",
                "# Random Search Sampling\n",
                "data = {\n",
                "    \"pop_size\": np.random.randint(50, 201, n_samples),\n",
                "    \"crossover_prob\": np.random.uniform(0.5, 1.0, n_samples),\n",
                "    \"mutation_prob\": np.random.uniform(0.0, 0.2, n_samples),\n",
                "    \"selection_pressure\": np.random.randint(2, 11, n_samples),\n",
                "}\n",
                "df = pd.DataFrame(data)\n",
                "\n",
                "# Ground Truth Function (Synthetic Efficiency)\n",
                "# HV = Base + contributions - penalties\n",
                "def ground_truth(row):\n",
                "    hv = 0.5\n",
                "    # larger pop is better but diminishing returns\n",
                "    hv += 0.2 * (row[\"pop_size\"] / 200)\n",
                "    # Optimal crossover ~ 0.9\n",
                "    hv -= 2.0 * (row[\"crossover_prob\"] - 0.9)**2\n",
                "    # Optimal mutation ~ 0.1\n",
                "    hv -= 5.0 * (row[\"mutation_prob\"] - 0.1)**2\n",
                "    # Selection pressure doesn't matter much (noise)\n",
                "    hv += 0.01 * np.random.randn()\n",
                "    \n",
                "    # Add some random noise\n",
                "    hv += 0.02 * np.random.randn()\n",
                "    return hv\n",
                "\n",
                "df[\"hypervolume\"] = df.apply(ground_truth, axis=1)\n",
                "df[\"iteration\"] = np.arange(n_samples)\n",
                "\n",
                "# Sort by HV to see best\n",
                "df_sorted = df.sort_values(\"hypervolume\", ascending=False)\n",
                "print(\"Top 5 Configurations:\")\n",
                "print(df_sorted.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Optimization Trace\n",
                "Did we find better solutions over time? (In Random Search, this is just luck, but in active learning it would trend up)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(df[\"iteration\"], df[\"hypervolume\"], 'o', alpha=0.5, label=\"Evaluations\")\n",
                "plt.plot(df[\"iteration\"], df[\"hypervolume\"].cummax(), 'r-', linewidth=2, label=\"Best Found\")\n",
                "plt.xlabel(\"Iteration\")\n",
                "plt.ylabel(\"Hypervolume\")\n",
                "plt.title(\"Optimization Trace\")\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Parallel Coordinates Plot\n",
                "Visualize high-dimensional data. Best solutions are colored brightly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalize inputs for Parallel Coordinates\n",
                "cols = [\"pop_size\", \"crossover_prob\", \"mutation_prob\", \"selection_pressure\", \"hypervolume\"]\n",
                "df_norm = df[cols].copy()\n",
                "scaler = MinMaxScaler()\n",
                "df_norm[cols] = scaler.fit_transform(df_norm[cols])\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "pd.plotting.parallel_coordinates(df_norm, \"hypervolume\", cols=cols, color=plt.cm.viridis(df_norm[\"hypervolume\"]))\n",
                "# Note: standard parallel_coordinates uses a categorical class column, which is messy for continuous HV.\n",
                "# Let's use a custom approach or just plot lines colored by value.\n",
                "plt.close()\n",
                "\n",
                "# Better Line Plot approach\n",
                "plt.figure(figsize=(12, 6))\n",
                "for i, row in df_norm.iterrows():\n",
                "    alpha = max(0.1, row[\"hypervolume\"]**2) # emphasize good ones\n",
                "    color = plt.cm.viridis(row[\"hypervolume\"])\n",
                "    plt.plot(range(len(cols)-1), row[:-1], color=color, alpha=alpha)\n",
                "\n",
                "plt.xticks(range(len(cols)-1), cols[:-1])\n",
                "plt.ylabel(\"Normalized Value\")\n",
                "plt.title(\"Parallel Coordinates (Bighter = Better HV)\")\n",
                "plt.colorbar(plt.cm.ScalarMappable(cmap=\"viridis\"), label=\"Normalized Hypervolume\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Parameter Importance (Random Forest)\n",
                "Which parameters actually drive the performance?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = df[[\"pop_size\", \"crossover_prob\", \"mutation_prob\", \"selection_pressure\"]]\n",
                "y = df[\"hypervolume\"]\n",
                "\n",
                "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
                "rf.fit(X, y)\n",
                "\n",
                "importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
                "\n",
                "plt.figure(figsize=(8, 5))\n",
                "sns.barplot(x=importances.values, y=importances.index, palette=\"viridis\")\n",
                "plt.title(\"Parameter Importance (Random Forest)\")\n",
                "plt.xlabel(\"Importance Score\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Partial Dependence Plots\n",
                "Feature importance tells us *what* matters, but Partial Dependence Plots (PDP) tell us *how* it matters (the functional shape)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "features = [\"pop_size\", \"crossover_prob\", \"mutation_prob\", \"selection_pressure\"]\n",
                "\n",
                "print(\"Computing Partial Dependence Plots...\")\n",
                "fig, ax = plt.subplots(figsize=(14, 4))\n",
                "# Note: grid_resolution controls the smoothness\n",
                "PartialDependenceDisplay.from_estimator(rf, X, features, ax=ax, grid_resolution=20)\n",
                "plt.suptitle(\"Partial Dependence of Hypervolume on Parameters\", fontsize=14, y=1.05)\n",
                "plt.subplots_adjust(top=0.9)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}