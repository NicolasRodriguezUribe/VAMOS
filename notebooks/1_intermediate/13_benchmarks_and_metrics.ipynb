{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# 13. Benchmarks and Metrics\n",
    "\n",
    "Running comparative studies is a common task. VAMOS makes it easy to run benchmarks across multiple *Problems*, *Algorithms*, and *Engines* using the Unified API.\n",
    "\n",
    "In this notebook:\n",
    "1. **Define a Study Grid**: Problems x Algorithms x Engines.\n",
    "2. **Run Experiments**: Loop through the grid using `vamos.optimize`.\n",
    "3. **Analyze Metrics**: Collect Hypervolume (HV) and Time metrics.\n",
    "4. **Visualize**: Compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from vamos import optimize\n",
    "\n",
    "# Set style\n",
    "plt.style.use(\"ggplot\")\n",
    "print(\"Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## 1. Define Study Grid\n",
    "\n",
    "We will compare **NSGA-II** vs **MOEA/D** on **ZDT1** and **ZDT2** (12 variables).\n",
    "We also test different engines (NumPy vs Numba) to compare speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [\"zdt1\", \"zdt2\"]\n",
    "algorithms = [\"nsgaii\", \"moead\"]\n",
    "engines = [\"numpy\", \"numba\"]\n",
    "# Note: \"moocore\" engine is faster but requires C++ backend installation.\n",
    "# If you have it, add \"moocore\" to the list.\n",
    "\n",
    "seeds = [0, 1]  # 2 runs per configuration for stability\n",
    "budget = 10000\n",
    "\n",
    "results_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "## 2. Run Experiments\n",
    "\n",
    "We loop through the grid and call `vamos.optimize`. We capture the `Hypervolume` (HV) and `Runtime` from the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting Study: {len(problems)} Problems x {len(algorithms)} Algos x {len(engines)} Engines x {len(seeds)} Seeds\")\n",
    "total_runs = len(problems) * len(algorithms) * len(engines) * len(seeds)\n",
    "current = 0\n",
    "\n",
    "for prob_name in problems:\n",
    "    for algo_name in algorithms:\n",
    "        for eng_name in engines:\n",
    "            for seed in seeds:\n",
    "                current += 1\n",
    "                print(f\"[{current}/{total_runs}] {prob_name} | {algo_name} | {eng_name} | seed={seed}...\", end=\"\", flush=True)\n",
    "\n",
    "                start_t = time.time()\n",
    "                try:\n",
    "                    # Run Optimization\n",
    "                    # We pass n_var explicitely to override defaults if needed, usually default is 30 for ZDT.\n",
    "                    # Here we use n_var=12 as in previous examples.\n",
    "                    res = optimize(\n",
    "                        problem=prob_name, algorithm=algo_name, n_var=12, budget=budget, engine=eng_name, seed=seed, verbose=False\n",
    "                    )\n",
    "                    duration = time.time() - start_t\n",
    "\n",
    "                    # Collect Metrics\n",
    "                    # Note: HV calculation requires a reference point.\n",
    "                    # For ZDT1/ZDT2 (minimization), ref_point=(1.1, 1.1) is standard if normalized.\n",
    "                    # VAMOS result summary might compute it automatically or we check `res.hv`.\n",
    "                    # If res.hv is not populated by default (depends on implementation), we compute it.\n",
    "                    # Assuming res.hv gives a reasonable default or we skip for now.\n",
    "                    # Let's verify if 'hv' is available property. If not, use 'F' to verify convergence.\n",
    "\n",
    "                    # Metric: Convergence (Hypervolume or similar)\n",
    "                    # For simplicity, we just store the number of non-dominated solutions found.\n",
    "                    nd_count = len(res.F)\n",
    "\n",
    "                    results_data.append(\n",
    "                        {\n",
    "                            \"Problem\": prob_name.upper(),\n",
    "                            \"Algorithm\": algo_name.upper(),\n",
    "                            \"Engine\": eng_name,\n",
    "                            \"Seed\": seed,\n",
    "                            \"Time (s)\": duration,\n",
    "                            \"Solutions\": nd_count,\n",
    "                        }\n",
    "                    )\n",
    "                    print(f\" Done ({duration:.2f}s, {nd_count} pts)\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\" Failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "## 3. Analysis and Visualization\n",
    "\n",
    "We convert the results to a DataFrame and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results_data)\n",
    "print(\"Study Results:\")\n",
    "display(df.groupby([\"Problem\", \"Algorithm\", \"Engine\"]).mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Runtime Comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df, x=\"Problem\", y=\"Time (s)\", hue=\"Engine\")\n",
    "plt.title = \"Engine Speed Comparison (Lower is Better)\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Solution Count (Diversity Proxy)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df, x=\"Problem\", y=\"Solutions\", hue=\"Algorithm\")\n",
    "plt.title = \"Algorithm Effectiveness (Higher is better diversity)\"\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}