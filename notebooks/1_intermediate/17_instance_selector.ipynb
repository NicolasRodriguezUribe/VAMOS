{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Selector for VAMOS\n",
    "\n",
    "This notebook analyzes the TSP instances available in `data/tsplib` to help select a diverse set of benchmark instances. \n",
    "It adapts the methodology from [GRAFO-URJC/DRFLP](https://github.com/GRAFO-URJC/DRFLP/tree/master/instance-selector) (Graph Analysis).\n",
    "\n",
    "## Methodology\n",
    "1. **Load Instances**: Read `.tsp` files from `data/tsplib`.\n",
    "2. **Graph Conversion**: Convert TSP coordinates into NetworkX graphs (Euclidean distance weights).\n",
    "3. **Metric Extraction**: Calculate graph metrics (Density, Degree stats, etc.).\n",
    "4. **Feature Correlation**: Identify redundant metrics.\n",
    "5. **Dimensionality Reduction**: Use PCA to visualize the instance space.\n",
    "6. **Clustering**: Application of K-Means to group similar instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from yellowbrick.cluster import KElbowVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "We read the TSPLIB files and parse the `NODE_COORD_SECTION`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsp_file(filepath):\n",
    "    coords = {}\n",
    "    with open(filepath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    in_coord_section = False\n",
    "    name = os.path.basename(filepath)\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"NODE_COORD_SECTION\"):\n",
    "            in_coord_section = True\n",
    "            continue\n",
    "        if line.startswith(\"EOF\"):\n",
    "            break\n",
    "\n",
    "        if in_coord_section:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 3:\n",
    "                idx = int(parts[0])\n",
    "                x = float(parts[1])\n",
    "                y = float(parts[2])\n",
    "                coords[idx] = (x, y)\n",
    "\n",
    "    return name, coords\n",
    "\n",
    "\n",
    "def create_graph_from_tsp(name, coords):\n",
    "    G = nx.Graph(name=name)\n",
    "    nodes = list(coords.keys())\n",
    "\n",
    "    # Add nodes\n",
    "    for n in nodes:\n",
    "        G.add_node(n, pos=coords[n])\n",
    "\n",
    "    # Add edges (Complete Graph with Euclidean distances)\n",
    "    # Optimization: For very large graphs, we might want to prune this, but for <500 nodes full is fine.\n",
    "    for u, v in itertools.combinations(nodes, 2):\n",
    "        p1 = coords[u]\n",
    "        p2 = coords[v]\n",
    "        dist = math.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n",
    "        G.add_edge(u, v, weight=dist)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "DATA_DIR = \"../data/tsplib\"\n",
    "graphs = []\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(f\"Directory {DATA_DIR} not found. Please ensure you are running this from 'notebooks/' and data exists.\")\n",
    "else:\n",
    "    files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".tsp\")]\n",
    "    print(f\"Found {len(files)} TSP instances.\")\n",
    "\n",
    "    for f in files:\n",
    "        path = os.path.join(DATA_DIR, f)\n",
    "        name, coords = read_tsp_file(path)\n",
    "        G = create_graph_from_tsp(name, coords)\n",
    "        graphs.append(G)\n",
    "        print(f\"Loaded {name}: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Metrics\n",
    "We calculate various graph metrics for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data = []\n",
    "\n",
    "for G in graphs:\n",
    "    # Degree stats\n",
    "    degrees = [val for (node, val) in G.degree()]\n",
    "\n",
    "    # Edge weights\n",
    "    weights = [d[\"weight\"] for u, v, d in G.edges(data=True)]\n",
    "\n",
    "    m = {\n",
    "        \"name\": G.name,\n",
    "        \"num_vertices\": G.number_of_nodes(),\n",
    "        # For complete graphs, edges = n*(n-1)/2, so density is always 1.0\n",
    "        # But we verify anyway or maybe we should use MST or KNN graph for more interesting structure?\n",
    "        # We'll stick to basic stats of the full distance matrix for now.\n",
    "        \"vertex_avg_degree\": np.mean(degrees),\n",
    "        \"edge_mean_weight\": np.mean(weights),\n",
    "        \"edge_std_weight\": np.std(weights),\n",
    "        \"edge_min_weight\": np.min(weights),\n",
    "        \"edge_max_weight\": np.max(weights),\n",
    "        # Coefficient of variation for weights (dispersion)\n",
    "        \"edge_weight_cv\": np.std(weights) / np.mean(weights) if np.mean(weights) > 0 else 0,\n",
    "    }\n",
    "    metrics_data.append(m)\n",
    "\n",
    "df = pd.DataFrame(metrics_data)\n",
    "df.set_index(\"name\", inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature Correlation Analysis\n",
    "Identify redundant features by computing the Pearson correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 1:\n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\", square=True, linewidths=0.5)\n",
    "    plt.title(\"Feature Correlation Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Identify highly correlated pairs (|r| > 0.9)\n",
    "    high_corr = []\n",
    "    for i, col1 in enumerate(corr_matrix.columns):\n",
    "        for col2 in corr_matrix.columns[i + 1 :]:\n",
    "            r = corr_matrix.loc[col1, col2]\n",
    "            if abs(r) > 0.9:\n",
    "                high_corr.append((col1, col2, r))\n",
    "\n",
    "    if high_corr:\n",
    "        print(\"\\nHighly Correlated Feature Pairs (|r| > 0.9):\")\n",
    "        for col1, col2, r in high_corr:\n",
    "            print(f\"  {col1} <-> {col2}: r={r:.3f}\")\n",
    "    else:\n",
    "        print(\"\\nNo highly correlated pairs found.\")\n",
    "else:\n",
    "    print(\"Not enough instances for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA & Clustering\n",
    "Normalize data and apply PCA, then cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 1:\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=0.95)  # Keep 95% variance\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    print(f\"Reduced to {X_pca.shape[1]} components explaining {np.sum(pca.explained_variance_ratio_):.2%} variance.\")\n",
    "\n",
    "    # Elbow Method for K\n",
    "    model = KMeans(random_state=42, n_init=10)\n",
    "    visualizer = KElbowVisualizer(model, k=(2, min(10, len(df) - 1)))\n",
    "    visualizer.fit(X_pca)\n",
    "    visualizer.show()\n",
    "else:\n",
    "    print(\"Not enough instances for clustering (need at least 2).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 1 and visualizer.elbow_value_:\n",
    "    k = visualizer.elbow_value_\n",
    "    print(f\"Selected K={k}\")\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_pca)\n",
    "    df[\"cluster\"] = clusters\n",
    "\n",
    "    # Plot PCA colored by cluster\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=clusters, palette=\"viridis\", s=100)\n",
    "\n",
    "    for i, name in enumerate(df.index):\n",
    "        plt.annotate(name, (X_pca[i, 0], X_pca[i, 1]), fontsize=8, alpha=0.7)\n",
    "\n",
    "    plt.title(\"TSP Instances PCA + Clustering\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nCluster Assignments:\")\n",
    "    print(df[\"cluster\"].sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hardness Proxy Visualization\n",
    "Color the PCA plot by a \"hardness proxy\" (e.g., `num_vertices` or `edge_weight_cv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 1:\n",
    "    # Use num_vertices as a proxy for hardness (larger = harder)\n",
    "    hardness_proxy = df[\"num_vertices\"].values\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=hardness_proxy, cmap=\"plasma\", s=100, edgecolors=\"black\", linewidths=0.5)\n",
    "    plt.colorbar(scatter, label=\"Hardness Proxy (num_vertices)\")\n",
    "\n",
    "    for i, name in enumerate(df.index):\n",
    "        plt.annotate(name, (X_pca[i, 0], X_pca[i, 1]), fontsize=8, alpha=0.7)\n",
    "\n",
    "    plt.title(\"TSP Instances PCA (Colored by Hardness Proxy)\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough instances.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}