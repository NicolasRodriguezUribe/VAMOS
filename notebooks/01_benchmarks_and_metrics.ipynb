{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks and Metrics with StudyRunner\n",
    "\n",
    "Run multi-problem studies, compare algorithms and backends, and inspect hypervolume metrics.\n",
    "\n",
    "This notebook covers:\n",
    "1. **Algorithm comparison** - NSGA-II vs MOEA/D across problems\n",
    "2. **Backend comparison** - numpy vs numba vs moocore engines\n",
    "3. **Pareto front visualization** - overlay fronts from different runs\n",
    "4. **Performance metrics** - HV, runtime, evaluations/sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from vamos import ExperimentConfig\n",
    "from vamos.experiment.study.runner import StudyRunner, StudyTask\n",
    "from vamos.ux.analysis.tuning_viz import study_results_to_dataframe\n",
    "from vamos.foundation.core.runner import resolve_kernel\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# Detect available backends\n",
    "def detect_engines(candidates):\n",
    "    engines = []\n",
    "    for name in candidates:\n",
    "        try:\n",
    "            resolve_kernel(name)\n",
    "            engines.append(name)\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping engine '{name}': {exc}\")\n",
    "    return engines\n",
    "\n",
    "AVAILABLE_ENGINES = detect_engines([\"numpy\", \"numba\", \"moocore\"])\n",
    "print(f\"Available engines: {', '.join(AVAILABLE_ENGINES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a study grid: problems × algorithms × engines × seeds\n",
    "problems = [\n",
    "    {\"problem\": \"zdt1\", \"n_var\": 12},\n",
    "    {\"problem\": \"zdt2\", \"n_var\": 12},\n",
    "]\n",
    "algorithms = [\"nsgaii\", \"moead\"]\n",
    "engines = AVAILABLE_ENGINES[:2]  # Use first 2 available for speed\n",
    "seeds = [0, 1]\n",
    "\n",
    "tasks = []\n",
    "for prob in problems:\n",
    "    for alg in algorithms:\n",
    "        for engine in engines:\n",
    "            for seed in seeds:\n",
    "                tasks.append(\n",
    "                    StudyTask(\n",
    "                        algorithm=alg,\n",
    "                        engine=engine,\n",
    "                        problem=prob[\"problem\"],\n",
    "                        n_var=prob[\"n_var\"],\n",
    "                        seed=seed,\n",
    "                        config_overrides={\n",
    "                            \"population_size\": 30,\n",
    "                            \"offspring_population_size\": 30,\n",
    "                            \"max_evaluations\": 400,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "\n",
    "print(f\"Total tasks: {len(tasks)}\")\n",
    "runner = StudyRunner(verbose=True)\n",
    "results = runner.run(tasks)\n",
    "df = study_results_to_dataframe(results)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize HV per algorithm/problem/engine\n",
    "summary = df.groupby([\"problem\", \"algorithm\", \"engine\"]).agg({\n",
    "    \"hv\": [\"mean\", \"std\"],\n",
    "    \"time_ms\": [\"mean\"],\n",
    "}).round(4)\n",
    "summary.columns = [\"hv_mean\", \"hv_std\", \"time_ms_mean\"]\n",
    "summary = summary.reset_index()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HV comparison by algorithm\n",
    "pivot_algo = summary.groupby([\"problem\", \"algorithm\"])[\"hv_mean\"].mean().unstack()\n",
    "ax = pivot_algo.plot(kind=\"bar\", figsize=(8, 4))\n",
    "ax.set_ylabel(\"Hypervolume (mean)\")\n",
    "ax.set_title(\"HV by Algorithm\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backend Comparison\n",
    "\n",
    "Compare performance across different kernel backends (numpy, numba, moocore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HV by backend engine\n",
    "pivot_engine = summary.groupby([\"problem\", \"engine\"])[\"hv_mean\"].mean().unstack()\n",
    "ax = pivot_engine.plot(kind=\"bar\", figsize=(8, 4))\n",
    "ax.set_ylabel(\"Hypervolume (mean)\")\n",
    "ax.set_title(\"HV by Backend Engine\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime comparison by backend\n",
    "pivot_time = summary.groupby([\"problem\", \"engine\"])[\"time_ms_mean\"].mean().unstack()\n",
    "ax = pivot_time.plot(kind=\"bar\", figsize=(8, 4), color=['tab:blue', 'tab:orange', 'tab:green'][:len(pivot_time.columns)])\n",
    "ax.set_ylabel(\"Time (ms)\")\n",
    "ax.set_title(\"Runtime by Backend Engine\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto Front Visualization\n",
    "\n",
    "Overlay Pareto fronts from different engines to verify consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Pareto fronts by problem, overlaying different engines\n",
    "if not results:\n",
    "    print(\"No results available yet. Run the study cell first.\")\n",
    "else:\n",
    "    # Group results by problem\n",
    "    by_problem = {}\n",
    "    for res in results:\n",
    "        key = res.selection.spec.label\n",
    "        by_problem.setdefault(key, []).append(res)\n",
    "    \n",
    "    for label, entries in by_problem.items():\n",
    "        fig, ax = plt.subplots(figsize=(7, 5))\n",
    "        for res in entries:\n",
    "            F = np.asarray(res.metrics[\"F\"])\n",
    "            engine = res.metrics[\"engine\"]\n",
    "            alg = res.metrics[\"algorithm\"]\n",
    "            ax.scatter(F[:, 0], F[:, 1], s=25, alpha=0.6, label=f\"{alg}/{engine}\")\n",
    "        \n",
    "        ax.set_xlabel(\"Objective 1\")\n",
    "        ax.set_ylabel(\"Objective 2\")\n",
    "        ax.set_title(f\"{label} - Pareto Fronts\")\n",
    "        ax.legend(loc=\"upper right\", fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
