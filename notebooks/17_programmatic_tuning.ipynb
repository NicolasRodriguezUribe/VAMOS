{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Programmatic Tuning with VAMOS\n",
                "\n",
                "While the `vamos-tune` CLI is convenient, you often want to integrate tuning directly into your Python scripts or notebooks.\n",
                "This notebook demonstrates how to use the **RacingTuner** API programmatically to optimize NSGA-II on the **ZDT1** problem.\n",
                "\n",
                "## Steps\n",
                "1.  **Define Parameter Space**: Using `build_nsgaii_config_space`.\n",
                "2.  **Configure Scenario**: Budget, parallel jobs, etc.\n",
                "3.  **Run Tuner**: Execute the race.\n",
                "4.  **Validate**: Run the best configuration found."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# VAMOS Imports\n",
                "from vamos.api import optimize, OptimizeConfig\n",
                "from vamos.engine.algorithm.config import NSGAIIConfig\n",
                "from vamos.foundation.problem.registry import make_problem_selection\n",
                "from vamos.foundation.metrics.hypervolume import compute_hypervolume\n",
                "from vamos.foundation.core.experiment_config import ExperimentConfig\n",
                "from vamos.foundation.core.runner import run_single\n",
                "\n",
                "# Tuning Imports\n",
                "from vamos.engine.tuning.racing.core import RacingTuner\n",
                "from vamos.engine.tuning.racing.scenario import Scenario\n",
                "from vamos.engine.tuning.racing.tuning_task import TuningTask, Instance, EvalContext\n",
                "from vamos.engine.tuning.racing.bridge import build_nsgaii_config_space, config_from_assignment"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Tuning Task\n",
                "We utilize `build_nsgaii_config_space` to get the standard search space for NSGA-II (pop_size, crossover/mutation probs, etc)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Parameter Space\n",
                "param_space = build_nsgaii_config_space()\n",
                "\n",
                "# 2. Instances & Seeds\n",
                "# We tune on ZDT1 (30 vars)\n",
                "instances = [Instance(name=\"zdt1\", n_var=30, kwargs={})]\n",
                "seeds = [100, 101, 102, 103, 104] # 5 training seeds\n",
                "\n",
                "# 3. Task Definition\n",
                "task = TuningTask(\n",
                "    name=\"tune_nsgaii_zdt1\",\n",
                "    param_space=param_space,\n",
                "    instances=instances,\n",
                "    seeds=seeds,\n",
                "    aggregator=lambda scores: np.mean(scores),\n",
                "    budget_per_run=2000, # Evaluations per run during tuning\n",
                "    maximize=True # We maximize Hypervolume\n",
                ")\n",
                "\n",
                "# 4. Scenario (The Racing Strategy)\n",
                "scenario = Scenario(\n",
                "    max_experiments=5000,   # Total tuning budget (small for demo)\n",
                "    n_jobs=2,               # Parallel jobs\n",
                "    verbose=True,\n",
                "    elimination_fraction=0.3\n",
                ")\n",
                "\n",
                "print(\"Tuning Task Set Up:\")\n",
                "print(f\"  Param Space Size: ~{len(param_space.params)} params\")\n",
                "print(f\"  Budget: {scenario.max_experiments} evals\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Evaluation Function\n",
                "The tuner calls this function. It receives a `config_dict` and must return a scalar score (Hypervolume)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def eval_zdt1(config_dict, ctx: EvalContext) -> float:\n",
                "    try:\n",
                "        # Convert dict to AlgorithmConfig object\n",
                "        # Note: pop_size might be missing if fixed, inject if needed\n",
                "        if \"pop_size\" not in config_dict:\n",
                "            config_dict[\"pop_size\"] = 100\n",
                "            \n",
                "        cfg = config_from_assignment(\"nsgaii\", config_dict)\n",
                "        \n",
                "        # Setup Experiment\n",
                "        selection = make_problem_selection(\"zdt1\", n_var=ctx.instance.n_var)\n",
                "        exp_cfg = ExperimentConfig(\n",
                "            population_size=cfg.population_size, \n",
                "            max_evaluations=ctx.budget, \n",
                "            seed=ctx.seed\n",
                "        )\n",
                "        \n",
                "        # Run Single Evaluation\n",
                "        result = run_single(\n",
                "            engine=\"numpy\",\n",
                "            algorithm=\"nsgaii\",\n",
                "            problem_selection=selection,\n",
                "            experiment_config=exp_cfg,\n",
                "            algorithm_config=cfg,\n",
                "            verbose=False\n",
                "        )\n",
                "        \n",
                "        # Calculate HV\n",
                "        if result.F is None or len(result.F) == 0:\n",
                "            return 0.0\n",
                "        \n",
                "        ref_point = np.array([1.1, 1.1])\n",
                "        return compute_hypervolume(result.F, ref_point)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Eval failed: {e}\")\n",
                "        return 0.0"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run Racing Tuner\n",
                "This starts the race!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tuner = RacingTuner(task=task, scenario=scenario, seed=42)\n",
                "\n",
                "print(\"Starting Race...\")\n",
                "best_config, history = tuner.run(eval_zdt1)\n",
                "\n",
                "print(\"\\n--- Tuning Complete ---\")\n",
                "print(\"Best Configuration found:\")\n",
                "for k, v in best_config.items():\n",
                "    print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Validate Best Configuration\n",
                "We compare the tuned configuration against a default configuration on a fresh seed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert best dict to Config object\n",
                "if \"pop_size\" not in best_config: best_config[\"pop_size\"] = 100\n",
                "tuned_cfg = config_from_assignment(\"nsgaii\", best_config)\n",
                "\n",
                "# Default Config\n",
                "default_cfg = NSGAIIConfig().pop_size(100).crossover(\"sbx\", prob=0.9).mutation(\"pm\", prob=0.1).selection(\"tournament\").fixed()\n",
                "\n",
                "def evaluate_final(cfg, name):\n",
                "    p = make_problem_selection(\"zdt1\").instantiate()\n",
                "    res = optimize(\n",
                "        OptimizeConfig(\n",
                "            problem=p,\n",
                "            algorithm=\"nsgaii\",\n",
                "            algorithm_config=cfg,\n",
                "            termination=(\"n_eval\", 5000),\n",
                "            seed=999 # Test seed\n",
                "        )\n",
                "    )\n",
                "    hv = compute_hypervolume(res.F, np.array([1.1, 1.1]))\n",
                "    print(f\"{name} HV: {hv:.4f}\")\n",
                "    return hv\n",
                "\n",
                "print(\"\\nValidation Result:\")\n",
                "hv_tuned = evaluate_final(tuned_cfg, \"Tuned\")\n",
                "hv_default = evaluate_final(default_cfg, \"Default\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}