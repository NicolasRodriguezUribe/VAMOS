{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Operator Selection (AOS)\n",
    "\n",
    "Adaptive Operator Selection (AOS) allows the algorithm to dynamically select variation operators (crossover, mutation) from a pool based on their performance during the optimization process. This can improve performance on difficult problems where the best operator is unknown or changes over different stages of search.\n",
    "\n",
    "In this notebook, we will:\n",
    "1.  Configure NSGA-II with an **Operator Pool** containing different crossover/mutation strategies.\n",
    "2.  Enable AOS with an **Epsilon-Greedy** policy to balance exploration and exploitation.\n",
    "3.  Run the optimization and visualize the dynamic operator probabilities over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from vamos import optimize\n",
    "from vamos.algorithms import NSGAIIConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We will define a pool of two distinct operator combinations:\n",
    "1.  **SBX + Polynomial Mutation** (Standard NSGA-II)\n",
    "2.  **BLX-Alpha + Gaussian Mutation** (Alternative strategy)\n",
    "\n",
    "The AOS method is set to `epsilon_greedy` with `epsilon=0.2`, meaning 20% of the time it chooses a random operator, and 80% of the time it chooses the best-performing one based on recent rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_name = \"zdt4\"  # ZDT4 is multimodal and sensitive to operator choice\n",
    "pop_size = 200\n",
    "max_evals = 20000\n",
    "seed = 42\n",
    "\n",
    "aos_config = {\n",
    "    \"enabled\": True,\n",
    "    \"method\": \"epsilon_greedy\",\n",
    "    \"reward_scope\": \"survival\",  # Reward based on survival of offspring\n",
    "    \"epsilon\": 0.2,\n",
    "    \"min_usage\": 1,\n",
    "    \"window_size\": 50,  # Moving window for reward calculation\n",
    "    \"operator_pool\": [\n",
    "        # Operator 0: Standard SBX + PM\n",
    "        {\n",
    "            \"crossover\": [\"sbx\", {\"prob\": 0.9, \"eta\": 20}],\n",
    "            \"mutation\": [\"pm\", {\"prob\": \"1/n\", \"eta\": 20, \"var_type\": \"real\"}]\n",
    "        },\n",
    "        # Operator 1: BLX-Alpha + Gaussian\n",
    "        {\n",
    "            \"crossover\": [\"blx_alpha\", {\"prob\": 0.9, \"alpha\": 0.5}],\n",
    "            \"mutation\": [\"gaussian\", {\"prob\": \"1/n\", \"sigma\": 0.1, \"var_type\": \"real\"}]\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "algo_cfg = (\n",
    "    NSGAIIConfig.builder()\n",
    "    .pop_size(pop_size)\n",
    "    .offspring_size(pop_size)\n",
    "    .crossover(\"sbx\", prob=0.9, eta=20)\n",
    "    .mutation(\"pm\", prob=\"1/n\", eta=20, var_type=\"real\")\n",
    "    .selection(\"tournament\", pressure=2)\n",
    "    .adaptive_operator_selection(aos_config)\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "We run the optimization with the unified `optimize(...)` API. AOS traces are\n",
    "returned in `result.data[\"aos\"][\"trace_rows\"]`, so no filesystem output is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running optimization on {problem_name}...\")\n",
    "result = optimize(\n",
    "    problem_name,\n",
    "    algorithm=\"nsgaii\",\n",
    "    algorithm_config=algo_cfg,\n",
    "    max_evaluations=max_evals,\n",
    "    seed=seed,\n",
    "    engine=\"numpy\",\n",
    ")\n",
    "print(\"Optimization complete.\")\n",
    "print(f\"Solutions found: {len(result)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_rows = (result.data.get(\"aos\") or {}).get(\"trace_rows\", [])\n",
    "if trace_rows:\n",
    "    df_trace = pd.DataFrame(trace_rows)\n",
    "    print(\"Trace data loaded.\")\n",
    "    print(df_trace.head())\n",
    "else:\n",
    "    df_trace = pd.DataFrame()\n",
    "    print(\"Warning: No AOS trace rows found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_trace.empty:\n",
    "    # Calculate cumulative usage over time\n",
    "    df_trace[\"op_label\"] = df_trace[\"op_name\"]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(data=df_trace, x=\"step\", hue=\"op_label\", multiple=\"fill\", bins=50, palette=\"viridis\")\n",
    "    plt.title(f\"Operator Selection Probability over Time ({problem_name})\")\n",
    "    plt.xlabel(\"Generation / Step\")\n",
    "    plt.ylabel(\"Selection Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    # Also show rewards\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df_trace, x=\"step\", y=\"reward\", hue=\"op_label\", alpha=0.3)\n",
    "    plt.title(\"Operator Rewards over Time\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Reward (Survival contribution)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No trace data to plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The plots above demonstrate how the algorithm adaptively switches between operators. If one operator consistently yields higher rewards (e.g., generates offspring that survive), its selection probability increases. This mechanism allows the algorithm to \"learn\" the best search strategy for the specific problem landscape."
   ]
  }
 ]
}