{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Operator Selection (AOS)\n",
    "\n",
    "Adaptive Operator Selection (AOS) allows the algorithm to dynamically select variation operators (crossover, mutation) from a pool based on their performance during the optimization process. This can improve performance on difficult problems where the best operator is unknown or changes over different stages of search.\n",
    "\n",
    "In this notebook, we will:\n",
    "1.  Configure NSGA-II with an **Operator Pool** containing different crossover/mutation strategies.\n",
    "2.  Enable AOS with an **Epsilon-Greedy** policy to balance exploration and exploitation.\n",
    "3.  Run the optimization and visualize the dynamic operator probabilities over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from vamos.optimize import optimize, OptimizeConfig, NSGAIIConfig\n",
    "\n",
    "# Create a directory for outputs\n",
    "output_dir = Path(\"results/aos_demo\")\n",
    "if output_dir.exists():\n",
    "    shutil.rmtree(output_dir)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We will define a pool of two distinct operator combinations:\n",
    "1.  **SBX + Polynomial Mutation** (Standard NSGA-II)\n",
    "2.  **BLX-Alpha + Gaussian Mutation** (Alternative strategy)\n",
    "\n",
    "The AOS method is set to `epsilon_greedy` with `epsilon=0.2`, meaning 20% of the time it chooses a random operator, and 80% of the time it chooses the best-performing one based on recent rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_name = \"zdt4\"  # ZDT4 is multimodal and sensitive to operator choice\n",
    "pop_size = 200\n",
    "max_evals = 20000\n",
    "\n",
    "aos_config = {\n",
    "    \"enabled\": True,\n",
    "    \"method\": \"epsilon_greedy\",\n",
    "    \"reward_scope\": \"survival\",  # Reward based on survival of offspring\n",
    "    \"epsilon\": 0.2,\n",
    "    \"min_usage\": 1,\n",
    "    \"window_size\": 50,  # Moving window for reward calculation\n",
    "    \"operator_pool\": [\n",
    "        # Operator 0: Standard SBX + PM\n",
    "        {\n",
    "            \"crossover\": [\"sbx\", {\"prob\": 0.9, \"eta\": 20}],\n",
    "            \"mutation\": [\"pm\", {\"prob\": \"1/n\", \"eta\": 20, \"var_type\": \"real\"}]\n",
    "        },\n",
    "        # Operator 1: BLX-Alpha + Gaussian\n",
    "        {\n",
    "            \"crossover\": [\"blx_alpha\", {\"prob\": 0.9, \"alpha\": 0.5}],\n",
    "            \"mutation\": [\"gaussian\", {\"prob\": \"1/n\", \"sigma\": 0.1, \"var_type\": \"real\"}]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "config = OptimizeConfig(\n",
    "    problem=problem_name,\n",
    "    algorithm=NSGAIIConfig()\n",
    "        .pop_size(pop_size)\n",
    "        .adaptive_operator_selection(aos_config)\n",
    "        .result_mode(\"non_dominated\"),\n",
    "    max_evaluations=max_evals,\n",
    "    seed=42,\n",
    "    output_root=str(output_dir)  # Important: AOS trace is currently file-based\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "We run the optimization. Note that `output_root` is specified, so `vamos` will write `aos_trace.csv` to the results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running optimization on {problem_name}...\")\n",
    "result = optimize(config)\n",
    "print(\"Optimization complete.\")\n",
    "print(f\"Solutions found: {len(result.F)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Operator Dynamics\n",
    "\n",
    "We load the `aos_trace.csv` file to analyze which operators were chosen throughout the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate correct seed directory\n",
    "run_dir = list(output_dir.glob(f\"*/nsgaii/numpy/seed_{config.seed}\"))[0]\n",
    "trace_path = run_dir / \"aos_trace.csv\"\n",
    "\n",
    "if trace_path.exists():\n",
    "    df_trace = pd.read_csv(trace_path)\n",
    "    print(\"Trace data loaded.\")\n",
    "    print(df_trace.head())\n",
    "else:\n",
    "    print(\"Warning: aos_trace.csv not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trace_path.exists():\n",
    "    # Calculate cumulative usage over time\n",
    "    df_trace['op_label'] = df_trace['op_name']\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(data=df_trace, x='step', hue='op_label', multiple=\"fill\", bins=50, palette=\"viridis\")\n",
    "    plt.title(f\"Operator Selection Probability over Time ({problem_name})\")\n",
    "    plt.xlabel(\"Generation / Step\")\n",
    "    plt.ylabel(\"Selection Frequency\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Also show rewards\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df_trace, x='step', y='reward', hue='op_label', alpha=0.3)\n",
    "    plt.title(\"Operator Rewards over Time\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Reward (Survival contribution)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The plots above demonstrate how the algorithm adaptively switches between operators. If one operator consistently yields higher rewards (e.g., generates offspring that survive), its selection probability increases. This mechanism allows the algorithm to \"learn\" the best search strategy for the specific problem landscape."
   ]
  }
 ]
}
